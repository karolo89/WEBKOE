analysis %>%
ggplot(aes(x = score)) +
geom_histogram(binwidth = 1, fill = "#2171b5") +
labs(x = "Sentiment Score", y = "Frequency") +
ggtitle("Distribution of Sentiment Scores of the Tweets") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.title = element_text(size = 12),
axis.text = element_text(size = 10),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()) +
easy_center_title()
ggsave("plot.png", width = 8, height = 6, dpi = 300)  # Save the plot as an image file
neutral <- length(which(analysis$score == 0))
positive <- length(which(analysis$score > 0))
negative <- length(which(analysis$score < 0))
Sentiment <- c("Positive", "Neutral", "Negative")
Count <- c(positive, neutral, negative)
output <- data.frame(Sentiment, Count)
output$Sentiment <- factor(output$Sentiment, levels = Sentiment)
# Define custom color palette for business report
business_colors <- c("#0072B2", "#009E73", "#D55E00")
ggplot(output, aes(x = Sentiment, y = Count, fill = Sentiment)) +
geom_bar(stat = "identity") +
labs(x = "Sentiment", y = "Count", fill = "Sentiment") +
ggtitle("Distribution of Sentiment") +
scale_fill_manual(values = business_colors) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.title = element_text(size = 12),
axis.text = element_text(size = 10),
legend.title = element_text(size = 12),
legend.text = element_text(size = 10),
legend.position = "none",
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
)
library(RColorBrewer)
# Create a text corpus
text_corpus <- Corpus(VectorSource(cleanText))
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
text_corpus <- tm_map(text_corpus, removeWords, stopwords("english"))
text_corpus <- tm_map(text_corpus, removeWords, c("global", "globalwarming"))
# Create a term-document matrix
tdm <- TermDocumentMatrix(text_corpus)
tdm <- as.matrix(tdm)
tdm <- sort(rowSums(tdm), decreasing = TRUE)
tdm <- data.frame(word = names(tdm), freq = tdm)
set.seed(123)
wordcloud(text_corpus, min.freq = 1, max.words = 100, scale = c(2.2,1),
colors=brewer.pal(8, "Dark2"), random.color = T, random.order = F)
ggplot(tdm[1:20,], aes(x = reorder(word, freq), y = freq, fill = freq)) +
geom_bar(stat = "identity") +
xlab("Terms") +
ylab("Count") +
coord_flip() +
theme(axis.text = element_text(size = 7)) +
ggtitle("Most Common Word Frequency Plot") +
easy_center_title() +
theme_minimal() +
scale_fill_gradient(low = "#E5F5E0", high = "#31A354")
#bigram
bi.gram.words <- tweets.df %>%
unnest_tokens(
input = text,
output = bigram,
token = 'ngrams',
n = 2
) %>%
filter(! is.na(bigram))
bi.gram.words %>%
select(bigram) %>%
head(10)
extra.stop.words <- c('https')
stopwords.df <- tibble(
word = c(stopwords(kind = 'es'),
stopwords(kind = 'en'),
extra.stop.words)
)
bi.gram.count <- bi.gram.words %>%
dplyr::count(word1, word2, sort = TRUE) %>%
dplyr::rename(weight = n)
bi.gram.words %<>%
separate(col = bigram, into = c('word1', 'word2'), sep = ' ') %>%
filter(! word1 %in% stopwords.df$word) %>%
filter(! word2 %in% stopwords.df$word) %>%
filter(! is.na(word1)) %>%
filter(! is.na(word2))
bi.gram.count <- bi.gram.words %>%
dplyr::count(word1, word2, sort = TRUE) %>%
dplyr::rename(weight = n)
bi.gram.count %>% head()
## Note that it is very skewed, for visualization purposes it might be a good idea to perform a transformation, eg log transform:
bi.gram.count %>%
mutate(weight = log(weight + 1)) %>%
ggplot(mapping = aes(x = weight)) +
theme_light() +
geom_histogram(fill = "#2171b5") +
labs(title = "Bigram log-Weight Distribution")+
theme_minimal()
threshold <- 50
# For visualization purposes we scale by a global factor.
ScaleWeight <- function(x, lambda) {
x / lambda
}
network <-  bi.gram.count %>%
filter(weight > threshold) %>%
mutate(weight = ScaleWeight(x = weight, lambda = 2E3)) %>%
graph_from_data_frame(directed = FALSE)
plot(
network,
vertex.size = 1,
vertex.label.color = 'black',
vertex.label.cex = 0.7,
vertex.label.dist = 1,
edge.color = 'gray',
main = 'Bigram Count Network',
sub = glue('Weight Threshold: {threshold}'),
alpha = 50
)
threshold <- 50
network <-  bi.gram.count %>%
filter(weight > threshold) %>%
graph_from_data_frame(directed = FALSE)
# Store the degree.
V(network)$degree <- strength(graph = network)
# Compute the weight shares.
E(network)$width <- E(network)$weight/max(E(network)$weight)
# Create networkD3 object.
network.D3 <- igraph_to_networkD3(g = network)
# Define node size.
network.D3$nodes %<>% mutate(Degree = (1E-2)*V(network)$degree)
# Define color group
network.D3$nodes %<>% mutate(Group = 1)
# Define edges width.
network.D3$links$Width <- 10*E(network)$width
forceNetwork(
Links = network.D3$links,
Nodes = network.D3$nodes,
Source = 'source',
Target = 'target',
NodeID = 'name',
Group = 'Group',
opacity = 0.9,
Value = 'Width',
Nodesize = 'Degree',
# We input a JavaScript function.
linkWidth = JS("function(d) { return Math.sqrt(d.value); }"),
fontSize = 12,
zoom = TRUE,
opacityNoHover = 1
)
install.packages("tidytext")
install.packages("tidytext")
library(tidytext)
# Extract the positive and negative word lists
words <- get_sentiments("bing")
positive <- words %>% filter(sentiment == "positive")
library(hms)
library(twitteR)
library(lubridate)
library(tidytext)
library(tm)
library(wordcloud)
library(igraph)
library(glue)
library(networkD3)
library(rtweet)
library(plyr)
library(stringr)
library(ggplot2)
library(ggeasy)
library(plotly)
library(dplyr)
library(hms)
library(lubridate)
library(magrittr)
library(tidyverse)
library(janeaustenr)
library(widyr)
library(RColorBrewer)
library(ggeasy)
tweets <- read.csv("https://raw.githubusercontent.com/karolo89/WEBKOE/main/SatisfactionAnalysis/Tweets.csv")%>%
select(-negativereason_confidence)
head(tweets)
str(tweets)
library(ggplot2)
library(ggthemes)
library(dplyr)
tweets <- tweets %>%
mutate(airline = factor(airline, levels = names(sort(table(airline), decreasing = TRUE))))
ggplot(tweets) +
aes(airline, fill = airline) +
geom_bar() +
geom_text(stat = 'count', aes(label = paste0(round((..count..)/sum(..count..) * 100), "%")),   vjust = 1.6, color = "white") +
labs(title = 'Number of Tweets by Airlines', x = "", y = "") +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 16),
axis.title = element_text(size = 12),
axis.text = element_text(size = 10),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank(),
legend.position = "none",
legend.title = element_blank(),
legend.text = element_text(size = 10),
axis.text.y = element_text(size = 10),
legend.key.size = unit(0.5, "cm"),
legend.key.height = unit(0.5, "cm"),
legend.key.width = unit(0.5, "cm")
) +
scale_fill_tableau()
tableau_colors <- c("#4E79A7", "#F28E2B", "#E15759", "#76B7B2", "#59A14F", "#EDC948")
tweets %>%
group_by(airline, airline_sentiment) %>%
summarise(count = n()) %>%
ggplot(aes(airline, count, fill = airline)) +
geom_bar(stat = "identity") +
facet_wrap(~ airline_sentiment, ncol = 1) +
theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 9)) +
labs(x = "Airlines", y = "Count of Tweet Sentiments") +
theme_minimal() +
theme(
axis.title = element_text(size = 12),
axis.text = element_text(size = 10),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank(),
legend.position = "none",
legend.title = element_blank(),
legend.text = element_text(size = 10),
axis.text.y = element_text(size = 10),
legend.key.size = unit(0.5, "cm"),
legend.key.height = unit(0.5, "cm"),
legend.key.width = unit(0.5, "cm")
) +
scale_fill_manual(values = tableau_colors)
tweets %>%
group_by(negativereason) %>%
summarise(count = n(), na.rm = TRUE) %>%
arrange(desc(count)) %>%
ggplot(aes(reorder(negativereason, count), count)) +
geom_bar(fill = "#4E79A7", color = "black", stat = "identity") +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank(),
legend.position = "none",
plot.title = element_text(hjust = 0.5, size = 16),
axis.title = element_text(size = 12),
axis.text = element_text(size = 10)
) +
labs(x = "", y = "Count of Negative Reasons") +
scale_fill_manual(values = c("#4E79A7"))  # Using Tableau palette color
tweets.df <- tweets %>%
mutate(text = str_replace_all(text, "[^[:graph:]]", " ") %>%
gsub("https://", "", .) %>%
gsub("rt", "", .) %>%
gsub("@\\w+", "", .) %>%
gsub("http://", "", .) %>%
gsub("[[:digit:]]", "", .) %>%
gsub("[[:punct:]]", "", .) %>%
gsub("[ |\t]{2,}", "", .) %>%
gsub("[^[:graph:]]", " ", .) %>%
gsub("[[:punct:]]", "", .) %>%
gsub("[[:cntrl:]]", "", .) %>%
gsub(" $", "", .) %>%
gsub("\\d+", "", .) %>%
str_replace_all(., "[^[:graph:]]", " ") %>%
tolower())
cleanText <- tweets.df$text
# remove empty results (if any)
cleanText <- cleanText[cleanText != " "]
head(tweets.df)
# Apply the mutate function to update 'created' column
tweets.df <- tweets.df %>%
mutate(
created = tweet_created %>%
# Remove zeros.
str_remove_all(pattern = '\\+0000') %>%
# Parse date.
parse_date_time(orders = '%m/%d/%Y %H:%M')
)
# Create a new column 'Created_At_Round' by rounding 'created' to hours
tweets.df <- tweets.df %>%
mutate(Created_At_Round = round_date(created, unit = "hours") %>% as.POSIXct())
# Extract the minimum value of 'created' column
min_created <- min(tweets.df$created)
# Print the minimum value
print(min_created)
tweets.df %>% pull(created) %>% max()
plt <- tweets.df %>%
dplyr::count(Created_At_Round) %>%
ggplot(mapping = aes(x = Created_At_Round, y = n)) +
geom_line(color = "blue") +
labs(x = "Date", y = "Number of Tweets", title = "Number of Tweets per Hour") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5))
ggplotly(plt)
library(tidytext)
# Extract the positive and negative word lists
words <- get_sentiments("bing")
positive <- words %>% filter(sentiment == "positive")
negative <- words %>% filter(sentiment == "negative")
pos.words <- c(positive$word, 'upgrade', 'Congrats', 'prizes', 'prize', 'thanks', 'thnx', 'Grt', 'gr8', 'plz', 'trending', 'recovering', 'brainstorm', 'leader')
neg.words <- c(negative$word, 'wtf', 'wait', 'waiting', 'epicfail', 'Fight', 'fighting', 'arrest', 'no', 'not')
score.sentiment = function(sentences, pos.words, neg.words, airline, .progress='none')
{
require(plyr)
require(stringr)
# we are giving vector of sentences as input.
# plyr will handle a list or a vector as an "l" for us
# we want a simple array of scores back, so we use "l" + "a" + "ply" = laply:
scores = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub() function:
sentence = gsub('https://','',sentence)
sentence = gsub('http://','',sentence)
sentence = gsub('[^[:graph:]]', ' ',sentence)
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
sentence = str_replace_all(sentence,"[^[:graph:]]", " ")
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
analysis <- score.sentiment(cleanText, pos.words, neg.words)
# Sentiment score frequency table
score_table <- as.data.frame(table(analysis$score))
colnames(score_table) <- c("Sentiment Score", "Frequency")
score_table
analysis %>%
ggplot(aes(x = score)) +
geom_histogram(binwidth = 1, fill = "#2171b5") +
labs(x = "Sentiment Score", y = "Frequency") +
ggtitle("Distribution of Sentiment Scores of the Tweets") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.title = element_text(size = 12),
axis.text = element_text(size = 10),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()) +
easy_center_title()
ggsave("plot.png", width = 8, height = 6, dpi = 300)  # Save the plot as an image file
neutral <- length(which(analysis$score == 0))
positive <- length(which(analysis$score > 0))
negative <- length(which(analysis$score < 0))
Sentiment <- c("Positive", "Neutral", "Negative")
Count <- c(positive, neutral, negative)
output <- data.frame(Sentiment, Count)
output$Sentiment <- factor(output$Sentiment, levels = Sentiment)
# Define custom color palette for business report
business_colors <- c("#0072B2", "#009E73", "#D55E00")
ggplot(output, aes(x = Sentiment, y = Count, fill = Sentiment)) +
geom_bar(stat = "identity") +
labs(x = "Sentiment", y = "Count", fill = "Sentiment") +
ggtitle("Distribution of Sentiment") +
scale_fill_manual(values = business_colors) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
axis.title = element_text(size = 12),
axis.text = element_text(size = 10),
legend.title = element_text(size = 12),
legend.text = element_text(size = 10),
legend.position = "none",
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
)
library(RColorBrewer)
# Create a text corpus
text_corpus <- Corpus(VectorSource(cleanText))
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
text_corpus <- tm_map(text_corpus, removeWords, stopwords("english"))
text_corpus <- tm_map(text_corpus, removeWords, c("global", "globalwarming"))
# Create a term-document matrix
tdm <- TermDocumentMatrix(text_corpus)
tdm <- as.matrix(tdm)
tdm <- sort(rowSums(tdm), decreasing = TRUE)
tdm <- data.frame(word = names(tdm), freq = tdm)
set.seed(123)
wordcloud(text_corpus, min.freq = 1, max.words = 100, scale = c(2.2,1),
colors=brewer.pal(8, "Dark2"), random.color = T, random.order = F)
ggplot(tdm[1:20,], aes(x = reorder(word, freq), y = freq, fill = freq)) +
geom_bar(stat = "identity") +
xlab("Terms") +
ylab("Count") +
coord_flip() +
theme(axis.text = element_text(size = 7)) +
ggtitle("Most Common Word Frequency Plot") +
easy_center_title() +
theme_minimal() +
scale_fill_gradient(low = "#E5F5E0", high = "#31A354")
#bigram
bi.gram.words <- tweets.df %>%
unnest_tokens(
input = text,
output = bigram,
token = 'ngrams',
n = 2
) %>%
filter(! is.na(bigram))
bi.gram.words %>%
select(bigram) %>%
head(10)
extra.stop.words <- c('https')
stopwords.df <- tibble(
word = c(stopwords(kind = 'es'),
stopwords(kind = 'en'),
extra.stop.words)
)
bi.gram.words %<>%
separate(col = bigram, into = c('word1', 'word2'), sep = ' ') %>%
filter(! word1 %in% stopwords.df$word) %>%
filter(! word2 %in% stopwords.df$word) %>%
filter(! is.na(word1)) %>%
filter(! is.na(word2))
bi.gram.count <- bi.gram.words %>%
dplyr::count(word1, word2, sort = TRUE) %>%
dplyr::rename(weight = n)
bi.gram.count %>% head()
## Note that it is very skewed, for visualization purposes it might be a good idea to perform a transformation, eg log transform:
bi.gram.count %>%
mutate(weight = log(weight + 1)) %>%
ggplot(mapping = aes(x = weight)) +
theme_light() +
geom_histogram(fill = "#2171b5") +
labs(title = "Bigram log-Weight Distribution")+
theme_minimal()
threshold <- 50
# For visualization purposes we scale by a global factor.
ScaleWeight <- function(x, lambda) {
x / lambda
}
network <-  bi.gram.count %>%
filter(weight > threshold) %>%
mutate(weight = ScaleWeight(x = weight, lambda = 2E3)) %>%
graph_from_data_frame(directed = FALSE)
plot(
network,
vertex.size = 1,
vertex.label.color = 'black',
vertex.label.cex = 0.7,
vertex.label.dist = 1,
edge.color = 'gray',
main = 'Bigram Count Network',
sub = glue('Weight Threshold: {threshold}'),
alpha = 50
)
threshold <- 50
network <-  bi.gram.count %>%
filter(weight > threshold) %>%
graph_from_data_frame(directed = FALSE)
# Store the degree.
V(network)$degree <- strength(graph = network)
# Compute the weight shares.
E(network)$width <- E(network)$weight/max(E(network)$weight)
# Create networkD3 object.
network.D3 <- igraph_to_networkD3(g = network)
# Define node size.
network.D3$nodes %<>% mutate(Degree = (1E-2)*V(network)$degree)
# Define color group
network.D3$nodes %<>% mutate(Group = 1)
# Define edges width.
network.D3$links$Width <- 10*E(network)$width
forceNetwork(
Links = network.D3$links,
Nodes = network.D3$nodes,
Source = 'source',
Target = 'target',
NodeID = 'name',
Group = 'Group',
opacity = 0.9,
Value = 'Width',
Nodesize = 'Degree',
# We input a JavaScript function.
linkWidth = JS("function(d) { return Math.sqrt(d.value); }"),
fontSize = 12,
zoom = TRUE,
opacityNoHover = 1
)
