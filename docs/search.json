[
  {
    "objectID": "blog/facebook/facebook.html",
    "href": "blog/facebook/facebook.html",
    "title": "Facebook’s Algorithmic Pandora’s Box: Unmasking the Amplification of Fake News and Political Polarization",
    "section": "",
    "text": "As we integrate social media into our daily routines, we must consider the potential impact on the spread of information. Facebook’s algorithm is intended to personalize users’ News Feeds using engagement, relevance, relationships, post type, and recency, but this approach may unintentionally create filter bubbles that propagate false narratives and reinforce confirmation bias. This article delves into the impact of Facebook’s algorithm on the amplification of incorrect information and political polarization and explores the mechanisms behind this concerning phenomenon.\n\nUnderstanding Facebook’s Algorithm\nFacebook’s algorithm heavily relies on user engagement to determine the popularity and relevance of a post. All forms of engagement, including likes, comments, shares, and reactions, are considered to ensure that the content reaches a broader audience, prompting users to interact with posts of interest and increasing the visibility of similar content in the future. The algorithm aims to deliver content that aligns with users’ interests by analyzing past interactions, preferences, and relationships prioritizing engaging formats such as videos or visually captivating posts in the user’s News Feeds. Moreover, the algorithm considers advertising and promoted content, targeting users based on their demographics, interests, and behaviors to present them with relevant ads.\n\n\nThe Evolution of the Algorithm\nFacebook, established by Mark Zuckerberg in 2004 from a Harvard dormitory, quickly gained popularity and became a global sensation with its intuitive design, innovative features, and emphasis on digital connectivity. Throughout its history, Facebook’s algorithm has evolved significantly to meet its user base’s changing needs and challenges. From the introduction of the News Feed in 2006 to the ongoing refinements, the algorithm has aimed to balance personalization, user engagement, and responsible content curation. However, the platform has faced ethical and societal challenges, including privacy and transparency concerns, data security issues, and misinformation.\nThe News Feed’s introduction replaced users’ need to browse friends’ profiles manually. This infinite scroll of content tailored to each user’s interests and connections revolutionized how people consumed information on the platform. The algorithm gained further power in 2009 with the introduction of the Like button, allowing users to express approval or support for posts and photos.\nIn 2011, Facebook shifted its algorithm to prioritize content based on user engagement metrics like likes, comments, and shares. This update aimed to deliver more relevant and engaging content to users, enhancing their overall experience on the platform. By leveraging these metrics, the algorithm became a powerful tool for advertisers to target their desired audience precisely.\nThe algorithm continued to evolve in 2013 with the introduction of Graph Search. This feature enabled users to search for specific content, posts, photos, and people based on their connections and interests. This update improved the search experience and further personalized content recommendations, making it easier for users to discover relevant information.\nIn 2015, Facebook announced an algorithm update that prioritized posts from friends and family over content from brands, publishers, and pages. With growing concerns about the spread of clickbait headlines and fake news, Facebook took action to reduce the visibility of fake news, updating the system to favor posts from trustworthy sources and promoting more accurate and reliable information. This move aimed to address challenges associated with misinformation and enhance the platform’s content quality.\nIn 2018, the company made significant changes to de-emphasize posts created by publishers and brands and prioritize content shared by friends and families. In 2019, the company introduced the “Why am I seeing this post?” feature, allowing users to understand the factors influencing their News Feed content and promoting transparency. Users gained more control over the types of content they see through improved customization options, empowering them to tailor their Facebook experience.\nIn 2020, the algorithm was adjusted to combat false information dissemination, especially during significant events like elections. The platform prioritized authoritative sources, implemented measures to prevent the amplification of misinformation, and worked with third-party fact-checkers to minimize the impact on elections and provide users with reliable and more accurate information.\n\n\nSpread of Misinformation and Political Polarization on Facebook\nThe prevalence of fake news on Facebook is a significant concern encompassing a wide range of problematic content, including imposter news, conspiracy theories, hyperpartisan websites, and “junk news.” The insidious use of clickbait to attract readers and the dissemination of dubious information through bots and trolls, known as “computational propaganda,” only exacerbates the issue. This problem extends beyond news, surrounding cultural commentary and satire, and raises crucial questions about how user engagement and algorithmic prioritization contribute to the spread of misinformation.\nThe influence of user engagement on Facebook’s dissemination of fake news cannot be disregarded. Those who unintentionally like, comment, or share false information only increase their visibility and reach. Facebook’s algorithm can magnify fake news with high engagement, making it visible to a broader audience. Additionally, personalized content based on user preferences and behavior can create echo chambers and filter bubbles, exposing users to misinformation that aligns with their beliefs and interests. Acknowledging that maximizing engagement can lead to polarization is crucial, particularly within networks of like-minded users.\nWhile Facebook has tried to combat the spread of fake news, the issue remains complex. The platform has partnered with third-party organizations for fact-checking, flagging disputed content, and reducing false information’s visibility. However, balancing freedom of expression, user engagement, and the responsibility to combat misinformation is an ongoing challenge.\nRegarding political polarization, experts challenge Facebook CEO Mark Zuckerberg’s denial that the platform fuels divisiveness. Zuckerberg’s assertions have been disputed despite testifying before a U.S. House of Representatives subcommittee in March 2021 and attributing division to external factors, such as the political and media environment. Facebook’s VP for global affairs and communication, Nick Clegg, has similarly claimed there is no evidence to support the notion that social media is a clear driver of polarization. However, the scholarly consensus is that social media platforms like Facebook and Twitter intensify political sectarianism, contrary to the company’s claims. In articles published in Science in October 2020 and Trends in Cognitive Sciences in August 2021, researchers have concluded that social media is a significant facilitator of polarization.\nFacebook’s algorithm limits exposure to cross-cutting links, causing users to connect with others who share the same political beliefs and reducing the likelihood of encountering diverse opinions. While this algorithm aims to enhance user experience, it inadvertently reduces politically diverse content by around 5% for conservatives and 8% for liberals. Typically, individuals have five politically like-minded friends on Facebook for every friend from the opposing side.\nAlthough Silicon Valley is not solely responsible for these issues, a study conducted in March 2020 showed that taking a break from Facebook for a month significantly reduced polarization of views on policy issues among participants. The study published by the American Economic Review suggested that exposure to political content on social media tends to provoke heightened emotions, anger toward the opposing side, and more robust views on specific issues, which contradicts Facebook’s narrative.\n\n\nAlgorithmic Failure’s High Costs\nThe spread of false information through social media is a grave issue that can trigger severe short, and long-lasting consequences for individuals, society, and organizations. Acknowledging that misinformation can lead to social division, offer misleading medical advice, and promote fraudulent schemes that erode trust and pose serious health risks is paramount. The fact that fake news purporting that pure alcohol (methanol) could cure COVID-19 led to approximately 800 deaths in Iran, with an additional 5,876 people hospitalized due to methanol poisoning, is unacceptable. This tragic example underscores the precarious nature of misinformation for individuals and society.\nMoreover, the Cambridge Analytica and Facebook case explicitly demonstrates the cost of false information for businesses. Cambridge Analytica was accused of manipulating political outcomes through Facebook data, with some executives proposing unethical practices such as bribery and fake news. These actions ultimately led to the closure of the company in 2018 and Facebook paying a record $5 billion settlement to the Federal Trade Commission (FTC) and another $100 million to settle allegations of misuse of user data with the U.S. Securities and Exchange Commission.\nSuch actions have far-reaching consequences beyond these companies, as non-profit organizations and advocacy groups may struggle to communicate their messages effectively when fake news dominates the conversation. This effect in non-profits was demonstrated in 2014 when anti-abortion activists secretly recorded meetings and staff lunches at Planned Parenthood, a non-profit organization that provides reproductive healthcare options. The activists then edited and disseminated the footage through social media, creating a viral sensation. The hashtag #defundPP was shared over 1.3 million times within a few months, subjecting Planned Parenthood to intense scrutiny and calls for defunding.\nAnother example of the algorithm failure cost is the aftermath of the January 6th attack on the U.S. Capitol, which has been financially and politically disastrous. It’s been widely reported that Facebook groups played a significant role in fueling political polarization and spreading false narratives that contributed to the siege. These groups saw a massive surge in posts aimed at undermining the legitimacy of Joe Biden’s victory, totaling over 650,000 between Election Day and the attack. The costs of the damage caused by this event have already exceeded $30 million and are expected to continue rising. These expenses include implementing enhanced security measures, repairing damaged infrastructure, and deploying additional law enforcement resources. However, the financial implications are only a tiny part of the more significant problem. The attack on the Capitol represents a severe assault on American democratic principles and substantially threatens the peaceful transfer of power.\n\n\nData Ethics and Facebook’s Responsibility\nFacebook’s handling of user data has sparked significant concerns, mainly due to the infamous Cambridge Analytica scandal in 2018. The scandal highlighted Facebook’s unethical practices, including weak consent mechanisms, inadequate data protection, user profiling, targeted advertising, and a lack of transparency. The scandal revealed that personal data from around 87 million Facebook users had been collected and exploited through a third-party app called “This Is Your Digital Life.” This breach violated users’ privacy rights and exposed their data to misuse. Cambridge Analytica used this unauthorized data to create psychographic profiles and targeted political advertising during the 2016 U.S. presidential election and other campaigns worldwide.\nAs of today, misleading, and extremist content disguised as journalism and facts continue to dominate the most shared posts on the platform, making Facebook a prominent source of misinformation. Likewise, this criticism is further accentuated by the legal and regulatory landscape, allowing the company to prioritize profit maximization and shareholder value over proactive ethical considerations.\nRegarding algorithm responsibility, Article 230 of the Communications Decency Act of 1996 is a significant factor. Originally intended to safeguard freedom of expression online, this legislation has shielded platforms like Facebook from active duty in regulating their hosted content. However, it has been exploited by social media startups and big tech companies, enabling harmful content to thrive. The article stipulates that interactive computer service providers or users should not be held accountable for the information provided by content creators.\nFacebook promotes a “shared responsibility” approach and emphasizes community cooperation. The company argues that it cannot be the sole arbiter of truth and instead believes in empowering individuals to have a voice. They encourage users to report false posts, avoid sharing or posting misleading articles, and flag spammy content, placing some responsibility on the user community.\nIn the face of these concerns, it is evident that Facebook needs to address its data ethics practices more effectively and take substantial steps to combat misinformation while prioritizing transparency, user privacy, and responsible decision-making.\n\n\nNext Steps: Collaborative Solutions, Section 230 Reform, Proactive Measures by Social Media Companies, and Digital Citizenship\nSeveral next steps are crucial to address the challenges in the online environment: Section 230 Reform: Reforming Section 230 of the Communications Decency Act of 1996 is necessary to encourage platforms to exercise their duty of care responsibly without unnecessary regulatory burdens. This reform could motivate media outlets to address potential risks proactively and create a safer and more responsible online environment. The protections outlined in Section 230 were crafted over 25 years ago, a time of limited technological capabilities and naive technological optimism. In light of the significant changes since then, these protections are outdated and need reconsideration and updating by the legislators.\nCollaboration for Comprehensive Solutions: Governments, tech companies, civil society organizations, and individuals must join forces to develop comprehensive solutions. Collaborative efforts can include sharing expertise, resources, and best practices to effectively address the spread of fake news. This collective approach ensures a more holistic and impactful response.\nStrengthening Media Literacy and Promoting Digital Citizenship: Investing in media literacy programs is crucial to empower individuals with the critical skills to navigate the digital landscape. Education initiatives should focus on fostering media literacy, promoting digital citizenship, and cultivating necessary thinking skills. By equipping people with the tools to discern reliable information from misinformation, they become active participants in combating the spread of fake news.\nLong-term Algorithmic Adjustments: Social media companies, such as Facebook, must acknowledge their contribution to political polarization and take proactive measures to reduce the prevalence of divisive content. Making lasting changes to their algorithms is a practical step in curbing heated debates and reducing the amplification of polarizing narratives. Continuously refining automated systems and moderation policies is necessary to balance fostering free expression and mitigating the unintended suppression of legitimate political discourse.\nEmbrace transparency: Social media platforms must prioritize transparency by disclosing how their algorithms work, such as how content is rated, suggested, and removed. In addition, these platforms should continuously enhance their data protection measures, participate in independent audits, and foster a responsible environment for sharing information.\n\n\nConclusion\nThe potential contribution of Facebook’s algorithm to spreading fake news and political polarization is a significant concern. Despite its original purpose to personalize users’ News Feeds, the algorithm could create filter bubbles reinforcing false narratives and confirmation bias. Imposter news, conspiracy theories, hyperpartisan websites, and “junk news” are just a few examples of inappropriate content in the fake news sphere on Facebook. It’s worth noting that user engagement is a significant factor in disseminating phony information, and personalized content based on user preferences and behavior can create echo chambers and filter bubbles that expose users to misinformation that aligns with their beliefs and interests.\nIt’s essential to recognize that maximizing engagement can lead to polarization, particularly within networks of like-minded users. Facebook has tried to counter the spread of fake news, but balancing freedom of expression, user engagement, and the responsibility to combat misinformation is challenging. To develop comprehensive solutions, we must collaborate, promote media literacy, adjust algorithms, and embrace transparency.\n\n\nReferences\nAllcott, H., Braghieri, L., Eichmeyer, S., & Gentzkow, M. (2020). [The Welfare Effects of Social Media](https://doi.org/10.1257/aer.20190468). American Economic Review, 110(3), 629-676.\nBarrett, P., et al. (2021, September 27). [How Tech Platforms Fuel U.S. Political Polarization and What Government Can Do about It](https://www.brookings.edu/blog/techtank/2021/09/27/how-tech-platforms-fuel-u-s-political-polarization-and-what-government-can-do-about-it/). Brookings.\nBleiberg, J., & West, D. M. (2015, May 13). [Political Polarization on Facebook](https://www.brookings.edu/blog/techtank/2015/05/13/political-polarization-on-facebook/). Brookings.\nCochrane, E., & Broadwater, L. (2021, February 24). [Capitol Riot Costs Will Exceed $30 Million, Official Tells Congress](https://www.nytimes.com/2021/02/24/us/politics/capitol-riot-damage.html). The New York Times.\nDepartment of Justice. (2020, June 3). [Department of Justice’s Review of Section 230 of the Communications Decency Act of 1996](https://www.justice.gov/archives/ag/department-justice-s-review-section-230-communications-decency-act-1996).\nFinkel, E. J., et al. (2020). [Political Sectarianism in America](https://doi.org/10.1126/science.abe1715). Science, 370(6516), 533-536.\nHarvard Kennedy School. (n.d.). [Research Note: The Scale of Facebook’s Problem Depends upon How ‘Fake News’ Is Classified](https://misinforeview.hks.harvard.edu/article/research-note-the-scale-of-facebooks-problem-depends-upon-how-fake-news-is-classified/).\nHarvard Law Today. (n.d.). [The Algorithm Has Primacy over Media … over Each of Us, and It Controls What We Do](https://hls.harvard.edu/today/the-algorithm-has-primacy-over-media-over-each-of-us-and-it-controls-what-we-do/).\nMadrigal, A. C. (2017, October 12). [What Facebook Did to American Democracy](https://www.theatlantic.com/technology/archive/2017/10/what-facebook-did/542502/). The Atlantic.\nMIT Technology Review. (2021, March 11). [How Facebook Got Addicted to Spreading Misinformation](https://www.technologyreview.com/2021/03/11/1020600/facebook-responsible-ai-misinformation/).\nMIT Technology Review. (2021, November 20). [How Facebook and Google Fund Global Misinformation](https://www.technologyreview.com/2021/11/20/1039076/facebook-google-disinformation-clickbait/).\nMosseri, A. (2017, April 7). [Working to Stop Misinformation and False News](https://www.facebook.com/formedia/blog/working-to-stop-misinformation-and-false-news). Facebook Media.\nRocha, Y. M., et al. (2021). [The Impact of Fake News on Social Media and Its Influence on Health during the COVID-19 Pandemic: A Systematic Review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8502082/). Journal of Public Health, 1(10).\nSilverman, C. (2016, November 16). [This Analysis Shows How Viral Fake Election News Stories Outperformed Real News on Facebook](https://www.buzzfeednews.com/article/craigsilverman/viral-fake-election-news-outperformed-real-news-on-facebook). BuzzFeed News.\nSmith, M. D., & Van Alstyne, M. (2021, August 12). [It’s Time to Update Section 230](https://hbr.org/2021/08/its-time-to-update-section-230). Harvard Business Review.\nStars, S., Stortstrom, R., Lagrace, L., & King, N. (2018). [Overview of Fake News: For Non-Profit Organizations](https://stars.library.ucf.edu/cgi/viewcontent.cgi?article=1001&context=publicsectormedialiteracy).\nThe Guardian. (2016, November 10). [Facebook’s Failure: Did Fake News and Polarized Politics Get Trump Elected?](https://www.theguardian.com/technology/2016/nov/10/facebook-fake-news-election-conspiracy-theories).\nThe New York Times. (2015, May 8). [Facebook Use Polarizing? Site Begs to Differ](https://www.nytimes.com/2015/05/08/technology/facebook-study-disputes-theory-of-political-polarization-among-users.html).\nThe New York Times. (2020, May 28). [Facebook and Its Secret Policies](https://www.nytimes.com/2020/05/28/technology/facebook-polarization.html).\nVan Bavel, J. J., et al. (2021). [How Social Media Shapes Polarization](https://doi.org/10.1016/j.tics.2021.07.013). Trends in Cognitive Sciences, 25(11).\nFacebook AI. (n.d.). [How We’re Using Fairness Flow to Help Build AI That Works Better for Everyone](https://ai.facebook.com/blog/how-were-using-fairness-flow-to-help-build-ai-that-works-better-for-everyone)."
  },
  {
    "objectID": "blog/Theranos/theranos.html",
    "href": "blog/Theranos/theranos.html",
    "title": "Case Analysis: Theranos: The Unicorn That Wasn’t, By Joseph B. Fuller and John Masko",
    "section": "",
    "text": "Situation Analysis\nTheranos was a privately-owned biotech start-up based in Palo Alto, California, United States. It was founded in 2005 by Elizabeth Holmes, a 19-year-old chemical engineering student who dropped out of Stanford University with aspirations of revolutionizing the healthcare industry through technological innovations in blood testing and becoming a billionaire. At its peak, the company was valued at $9 billion, but it eventually dissolved in September 2018 with a value below zero.\nIn 2013, Theranos established its first blood collection sites in Palo Alto and two locations in Phoenix, Arizona. The company employed approximately 700-900 staff, including Sunny Balwani as the president and COO. Its board comprised high-profile individuals with strong connections to political figures in the United States. Quest Diagnostics and LabCorp, which held significant market share, posed as major competitors for Theranos. Nevertheless, the start-up managed to form powerful alliances with renowned pharmaceutical entities such as Walgreens, CVS, Safeway, and the U.S. Department of Defense (DOD).\nTheranos aimed to develop a prototype that would revolutionize blood testing by eliminating the need for needles, utilizing smaller blood samples, providing rapid results, and enabling testing at home. However, the company encountered various setbacks in realizing this vision. The initial design, a medical patch for diagnosing and treating medical conditions, proved unsuccessful, leading to the development of other failed inventions such as the cartridge-and-reader blood analyzer called the Edison, and later, the MiniLab.\nTheranos operated within an industry where most companies exhibited characteristics of traditional organizations and heavily relied on government support, technological advancements, and socio-economic conditions. According to the World Health Organization (WHO), the healthcare industry in the U.S. is one of the largest and most complex. In 2015, the country spent over $3.2 trillion on healthcare, primarily funded through public funds and various insurance coverage options, including individual and private employer-based plans.\nMoreover, the healthcare industry faced stringent regulations, making it the most highly regulated sector in the U.S. The Department of Health and Human Services (DHHS) and agencies like the Centers for Medicare & Medicaid Services and the Food and Drug Administration (FDA) played crucial roles in monitoring and regulating the industry. Failure to comply with government regulations and agency requirements could result in civil and criminal cases, financial penalties, exclusion from healthcare programs, and potential prohibitions or restrictions on services, all of which would significantly harm the company’s reputation and potentially lead to its dissolution.\nThe healthcare industry was subject to technological advancements and the introduction of new products. Strong competition existed, making intellectual property protection and partnerships crucial for the success of the innovation business model. The ability to develop, acquire, or license new and improved technologies would be key determinants of a company’s success. In the absence of such advancements or in the event of adverse effects, the company’s testing methods could become obsolete. Therefore, validation of the scientific base, expertise in clinical testing, manufacturing, risk reduction, and acquisition of necessary resources and approvals were essential for developing and advancing their science.\nThese organizations were also affected by socio-economic factors that influenced health disparities and people’s spending behavior. Factors such as inflation, unemployment, interest rates, salary inequality, age, marital status, food and housing security, and poverty rates played significant roles. Individuals facing unemployment were less likely to afford medical costs, including emergency visits or routine lab tests. Consequently, the price of services or products often served as a critical factor when selecting a healthcare provider.\n\n\nAnalysis\nTo analyze Theranos, I will employ the Nadler-Tushman Congruence Model, which offers a comprehensive framework for evaluating the organization’s performance. This model considers essential elements including strategy, critical tasks, structure and processes, people, and culture. By examining these interconnected aspects, we can gain a holistic understanding of the company’s operations and their alignment.\n\n\n\nNadler-Tushman Congruence Model\n\n\n\nStrategy:\nThe article does not explicitly mention Theranos’ operational objectives, strategy, values, and mission. However, it can be inferred that the organization aimed to prioritize innovation and maximize shareholder value. Theranos Inc utilized the blue ocean strategy to explore untapped market spaces, where competition was limited or non-existent. Instead of engaging in head-to-head battles in existing competitive markets, the company aimed to create new demand and establish itself as a pioneer in innovative healthcare solutions.\n\n\nStructure and Processes:\nAccording to the Fuller and Maski article, Theranos had a hierarchical structure with departments organized based on their respective functions, resulting in high work specialization. The organization comprised a Board of Directors, a CEO, a COO, a human resources department, a laboratory, and an engineering unit. These units operated as silos, and in some cases, there was a sense of rivalry among them. The article lacks information regarding the size of each department and the span of control.\nThe authors also suggest the existence of weak linking mechanisms within the organization due to Holmes’ obsession with maintaining tight control over information flow and the company’s narrative. Decision-making was centralized and top-down, with Holmes being the primary decision-maker. Communication channels like instant messaging were prohibited, and emails and computer programs were compulsively monitored, leading to reports containing misleading information.\nRegarding alignment mechanisms at Theranos, the article does not provide details about the systems the company used to measure performance, manage talent, or provide rewards and incentives at the individual and group levels. However, based on the article, there seemed to be a policy of dismissal or marginalization if employees failed to meet Holmes’ expectations, which included following orders, working long hours, and demonstrating critical abilities and discretion. Additionally, the selection process seemed to rely on the leader’s social and family connections, the reputation of employees, and their susceptibility to manipulation.\n\n\nCulture and Leadership:\nThe culture at Theranos was characterized by secrecy, fear, and control. Psychological safety was absent, and employees tended to work in isolation, avoiding eye contact and distrusting others. They feared retaliation from Holmes and Balwani, who were also involved in a secret romantic relationship. The leadership styles of both individuals were authoritarian, abusive, and micromanaging. Holmes used a baritone voice and maintained an unwavering gaze that conveyed a sense of intensity.\nEmployees faced humiliation and threats of legal action if they disobeyed orders or expressed disagreement with the company’s philosophy. The culture also exhibited hypersensitivity towards intellectual property and a paranoia regarding information leakage, necessitating the signing of confidentiality and nondisclosure agreements.\nIt is worth mentioning that Theranos was situated in Silicon Valley, a place known for slogans like “Fake it until you make it” and “Fail Fast.” The culture at Theranos was influenced by Apple and its CEO, with Holmes being particularly obsessed with Steve Jobs and his illusory effect.\n\n\nCritical Task:\nWhile the article does not explicitly outline Theranos’ workflow, critical tasks within the organization were identified in the engineering and laboratory departments. The engineering department was responsible for developing a compact product and ensuring the interaction of all elements as expected. The laboratory department focused on creating the necessary chemical reactions for analyzing blood samples. These tasks were complex, and each unit operated independently.\n\n\nPeople:\nThe leaders and members of the Board of Directors lacked formal scientific training and had no prior experience in biotechnology or healthcare, with the exception of two physician Board members. Most Board members were older men with backgrounds in politics and the military, and they had close connections to the CEO. The workforce consisted of renowned scientists and engineers, interns, recent graduates, and immigrants holding H1-B visas.\nAfter analyzing the elements of the Congruence model, it can be concluded that there was a lack of congruence among them. If Theranos’ products had worked and been certified, the strategy could have aligned with a blue ocean approach, revolutionizing the market through innovation.\n\n\n\nRecommendations\nTerminate Elizabeth Holmes and Sunny Balwani: The Board of Directors should promptly develop a comprehensive termination plan that includes clear justifications for this decision, performance evaluations, and the necessary skills and knowledge required in new leadership. A communication plan should be established to inform all stakeholders impacted by this change, ensuring clarity and authoritative communication. The Board should also identify an interim CEO for the transition period and define the legal framework for the dismissal process. Following this transition, a robust hiring process should be implemented to select a new leader, with clearly communicated expectations and a formal performance review process.\nStrengthen Linking Mechanisms: The new leadership team should prioritize the establishment of effective linking mechanisms within the organization. This can be achieved by creating liaison roles and forming cross-functional teams consisting of at least two members from each department. Regular meetings of these teams should be conducted to address specific problems and generate innovative ideas for new technologies. Utilizing technology such as enterprise resource planning (ERP), chat platforms, and other collaboration tools will facilitate information sharing and improve interdepartmental visibility.\nFoster a Culture of Transparency and Inclusion: Leaders must actively work to earn the trust of employees and create a culture of transparency and inclusion. This can be accomplished by providing consistent and honest communication, delegating responsibilities, and promoting a participatory and inclusive decision-making process. Creating spaces for listening and understanding within the organization is essential. Initiatives such as retreats, focus groups, Q&A sessions, employee surveys, and incentivizing creativity and teamwork can contribute to fostering an environment of trust and open communication.\nHalt Product Commercialization: Theranos should immediately cease the marketing of its products, recall those already on the market, and develop a comprehensive marketing plan. Establishing a dedicated control design unit with cross-functional teams will ensure strict oversight of facilities, equipment, design, and manufacturing processes. Prior to relaunching the product, obtaining approvals from relevant regulatory entities is crucial. A soft launch strategy should be adopted, accompanied by continuous monitoring to ensure compliance with industry requirements. Analytical and assessment tools, such as The Congruence Model, Process Analytical Technology (PAT), Design of Experiments, Risk Assessments, and Corrective and Preventive Actions (CAPA), should be utilized to enhance product quality and regulatory compliance.\nBy implementing these recommendations, Theranos can begin to address its performance and reputation challenges, foster a more transparent and inclusive culture, and establish a solid foundation for future growth and compliance with industry standards.\n\n\nReference\n\nCarreyrou, J. (Year). Bad Blood (p.183).\nFuller, T., & Masko, J. (Year). Theranos: The Unicorn that Wasn’t (The Unicorn, p.10).\nFuller, T., & Masko, J. (Year). Theranos: The Unicorn that Wasn’t (Exhibit 5 “Timeline of Theranos Publicity and Exposure” Industry Flash Report – Laboratory Services November 2016). Retrieved from link\nCountry Report: United States of America. (Year). Health Expenditures (p.2). Retrieved from link\nCountry Report: United States of America. (Year). Social Determinants of Health (p.1). Retrieved from link\nAuthor(s) or Organization. (Year). A Guide to Healthcare Compliance Regulations [Description]. Retrieved from link\nCountry Report: United States of America. (Year). Monitoring the Health System’s Organization, Provision of Care, and Performance (p.2). Retrieved from link\nAuthor(s) or Organization. (Year). Pharmaceutical product development: A quality by design approach. Retrieved from link\nCountry Report: United States of America. (Year). Overall Context of the Healthcare Industry. Social Determinants of Health. Retrieved from link\n\nPlease note that the provided links are active and can be accessed directly for further reference."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Facebook’s Algorithmic Pandora’s Box: Unmasking the Amplification of Fake News and Political Polarization\n\n\n\n\n\n\n\nData Journalism\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOrozco Karol M.\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\nCase Analysis: Theranos: The Unicorn That Wasn’t, By Joseph B. Fuller and John Masko\n\n\n\n\n\n\n\nData Journalism\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOrozco Karol M.\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello! It’s nice to meet you",
    "section": "",
    "text": "About Me\nI help to create and sustain communities, programs, and solutions that enhance the use of responsible data science and business practices for equitable social good.\nMy name is Karol Orozco; I live in Portland, OR, with my husband, two kids, and my dog Lucy. I am originally from Barranquilla, Colombia, a city located in the northern region of Colombia on the Caribbean coast. It is the fourth-largest city in the country and an important economic and cultural center in the Caribbean region. Barranquilla is known for its vibrant culture and festive atmosphere, exemplified by its famous Carnival of Barranquilla.\n\nlibrary(leaflet)\nleaflet() %>%\n  addTiles() %>%  # Add default OpenStreetMap map tiles\n  addMarkers(lng=-74.77028, lat=10.98611, popup=\"The birthplace of R\")\n\n\n\n\n\nAs a graduated engineer, I have acquired extensive experience and problem–solving expertise in data wrangling, statistical analysis, and model development. Since effective communication of data is of paramount importance, I have subsequently developed strong skills in data visualization and business knowledge. I hold an MBA from Willamette University and currently pursuing a second master’s in Data Science.\nI have experience working in the public and private sectors helping organizations to embrace shared value by positively impacting society and the environment: The sweet spot where profit meets purpose."
  },
  {
    "objectID": "next/CLVAnalysis/CustomerLifetimeValue.html",
    "href": "next/CLVAnalysis/CustomerLifetimeValue.html",
    "title": "Customer Lifetime Value (CLV) Analysis",
    "section": "",
    "text": "I conducted a CLV analysis for a subscription-based business to estimate the long-term value of customers. The analysis involved data preparation, calculation of customer lifetime value using different models (e.g., RFM analysis, cohort analysis), and visualization of results to identify high-value customer segments."
  },
  {
    "objectID": "next/CLVAnalysis/CustomerLifetimeValue.html#approach-and-methodology",
    "href": "next/CLVAnalysis/CustomerLifetimeValue.html#approach-and-methodology",
    "title": "Customer Lifetime Value (CLV) Analysis",
    "section": "Approach and Methodology",
    "text": "Approach and Methodology\n\nLoad the required packages"
  },
  {
    "objectID": "next/CLVAnalysis/CustomerLifetimeValue.html#data-summary",
    "href": "next/CLVAnalysis/CustomerLifetimeValue.html#data-summary",
    "title": "Customer Lifetime Value (CLV) Analysis",
    "section": "Data Summary:",
    "text": "Data Summary:"
  },
  {
    "objectID": "next/CLVAnalysis/CustomerLifetimeValue.html#data-exploration",
    "href": "next/CLVAnalysis/CustomerLifetimeValue.html#data-exploration",
    "title": "Customer Lifetime Value (CLV) Analysis",
    "section": "Data Exploration:",
    "text": "Data Exploration:"
  },
  {
    "objectID": "next/CLVAnalysis/CustomerLifetimeValue.html#feature-engineering",
    "href": "next/CLVAnalysis/CustomerLifetimeValue.html#feature-engineering",
    "title": "Customer Lifetime Value (CLV) Analysis",
    "section": "Feature Engineering",
    "text": "Feature Engineering"
  },
  {
    "objectID": "next/CLVAnalysis/CustomerLifetimeValue.html#conclusion",
    "href": "next/CLVAnalysis/CustomerLifetimeValue.html#conclusion",
    "title": "Customer Lifetime Value (CLV) Analysis",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "next/SatisfactionAnalysis/satisfaction.html",
    "href": "next/SatisfactionAnalysis/satisfaction.html",
    "title": "Customer Satisfaction Analysis",
    "section": "",
    "text": "I conducted a customer satisfaction analysis for a service-based organization to assess customer satisfaction levels and identify areas for improvement. The analysis involved data cleaning, calculation of satisfaction scores, statistical analysis (e.g., t-tests, ANOVA) to identify significant differences, and visualization of results using charts and graphs."
  },
  {
    "objectID": "next/SatisfactionAnalysis/satisfaction.html#approach-and-methodology",
    "href": "next/SatisfactionAnalysis/satisfaction.html#approach-and-methodology",
    "title": "Customer Satisfaction Analysis",
    "section": "Approach and Methodology",
    "text": "Approach and Methodology\n\nLoad the required packages"
  },
  {
    "objectID": "next/SatisfactionAnalysis/satisfaction.html#data-summary",
    "href": "next/SatisfactionAnalysis/satisfaction.html#data-summary",
    "title": "Customer Satisfaction Analysis",
    "section": "Data Summary:",
    "text": "Data Summary:"
  },
  {
    "objectID": "next/SatisfactionAnalysis/satisfaction.html#data-exploration",
    "href": "next/SatisfactionAnalysis/satisfaction.html#data-exploration",
    "title": "Customer Satisfaction Analysis",
    "section": "Data Exploration:",
    "text": "Data Exploration:"
  },
  {
    "objectID": "next/SatisfactionAnalysis/satisfaction.html#feature-engineering",
    "href": "next/SatisfactionAnalysis/satisfaction.html#feature-engineering",
    "title": "Customer Satisfaction Analysis",
    "section": "Feature Engineering",
    "text": "Feature Engineering"
  },
  {
    "objectID": "next/SatisfactionAnalysis/satisfaction.html#conclusion",
    "href": "next/SatisfactionAnalysis/satisfaction.html#conclusion",
    "title": "Customer Satisfaction Analysis",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "projects/Business/BrandPerception/brand.html",
    "href": "projects/Business/BrandPerception/brand.html",
    "title": "The Sky’s the Limit: Unveiling the Untold Stories of US Airline Woes on Twitter!",
    "section": "",
    "text": "Welcome to this sentiment analysis project in R, where I delve into the realm of major U.S. airlines and uncover the sentiments expressed by Twitter users regarding their experiences. By analyzing data scraped from February 2015, I aim to gain insights into the problems faced by these airlines through the eyes of their customers.\nThe dataset used in this analysis originated from Crowdflower’s Data for Everyone library. It is available in both CSV file and SQLite database formats, providing flexibility for your preferred data exploration methods. The transformation code used to format the data can be found on GitHub, ensuring transparency and reproducibility.\nWithin this dataset, you will find a comprehensive collection of tweets categorized based on their sentiment: positive, negative, or neutral. Additionally, negative tweets have been further classified based on the reasons provided, ranging from “late flight” to “rude service.” This granular categorization enables us to pinpoint specific problem areas faced by these airlines.\nJoin me on this journey as we uncover the untold stories behind each major U.S. airline, unraveling the sentiments expressed by Twitter users and shedding light on the challenges faced by the industry."
  },
  {
    "objectID": "projects/Business/BrandPerception/brand.html#approach-and-methodology",
    "href": "projects/Business/BrandPerception/brand.html#approach-and-methodology",
    "title": "The Sky’s the Limit: Unveiling the Untold Stories of US Airline Woes on Twitter!",
    "section": "Approach and Methodology",
    "text": "Approach and Methodology\nThe first step is to gather relevant Twitter data. This typically involves utilizing the Twitter API or third-party tools to retrieve tweets based on specific keywords, hashtags, or user profiles. The collected data forms the basis for further analysis.\nOnce the data is collected, it undergoes preprocessing to clean and prepare it for sentiment analysis. This involves removing irrelevant information such as URLs, usernames, and special characters, as well as standardizing the text by converting it to lowercase and removing stopwords.\n\nLoad the required packages and data\n\nlibrary(hms)\nlibrary(twitteR)\nlibrary(lubridate) \nlibrary(tidytext)\nlibrary(tm)\nlibrary(wordcloud)\nlibrary(igraph)\nlibrary(glue)\nlibrary(networkD3)\nlibrary(rtweet)\nlibrary(plyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggeasy)\nlibrary(plotly)\nlibrary(dplyr)  \nlibrary(hms)\nlibrary(lubridate) \nlibrary(magrittr)\nlibrary(tidyverse)\nlibrary(janeaustenr)\nlibrary(widyr)\nlibrary(RColorBrewer)\nlibrary(ggeasy)\n\n\ntweets <- read.csv(\"https://raw.githubusercontent.com/karolo89/WEBKOE/main/projects/Business/BrandPerception/Tweets.csv\")%>%\n  select(-negativereason_confidence)\n\nhead(tweets)\n\n     tweet_id airline_sentiment airline_sentiment_confidence negativereason\n1 5.70306e+17           neutral                       1.0000               \n2 5.70301e+17          positive                       0.3486               \n3 5.70301e+17           neutral                       0.6837               \n4 5.70301e+17          negative                       1.0000     Bad Flight\n5 5.70301e+17          negative                       1.0000     Can't Tell\n6 5.70301e+17          negative                       1.0000     Can't Tell\n         airline airline_sentiment_gold       name negativereason_gold\n1 Virgin America                           cairdin                    \n2 Virgin America                          jnardino                    \n3 Virgin America                        yvonnalynn                    \n4 Virgin America                          jnardino                    \n5 Virgin America                          jnardino                    \n6 Virgin America                          jnardino                    \n  retweet_count\n1             0\n2             0\n3             0\n4             0\n5             0\n6             0\n                                                                                                                                      text\n1                                                                                                      @VirginAmerica What @dhepburn said.\n2                                                                 @VirginAmerica plus you've added commercials to the experience... tacky.\n3                                                                  @VirginAmerica I didn't today... Must mean I need to take another trip!\n4           @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n5                                                                                  @VirginAmerica and it's a really big bad thing about it\n6 @VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA\n  tweet_coord   tweet_created tweet_location              user_timezone\n1             2/24/2015 11:35                Eastern Time (US & Canada)\n2             2/24/2015 11:15                Pacific Time (US & Canada)\n3             2/24/2015 11:15      Lets Play Central Time (US & Canada)\n4             2/24/2015 11:15                Pacific Time (US & Canada)\n5             2/24/2015 11:14                Pacific Time (US & Canada)\n6             2/24/2015 11:14                Pacific Time (US & Canada)\n\nstr(tweets)\n\n'data.frame':   14640 obs. of  14 variables:\n $ tweet_id                    : num  5.7e+17 5.7e+17 5.7e+17 5.7e+17 5.7e+17 ...\n $ airline_sentiment           : chr  \"neutral\" \"positive\" \"neutral\" \"negative\" ...\n $ airline_sentiment_confidence: num  1 0.349 0.684 1 1 ...\n $ negativereason              : chr  \"\" \"\" \"\" \"Bad Flight\" ...\n $ airline                     : chr  \"Virgin America\" \"Virgin America\" \"Virgin America\" \"Virgin America\" ...\n $ airline_sentiment_gold      : chr  \"\" \"\" \"\" \"\" ...\n $ name                        : chr  \"cairdin\" \"jnardino\" \"yvonnalynn\" \"jnardino\" ...\n $ negativereason_gold         : chr  \"\" \"\" \"\" \"\" ...\n $ retweet_count               : int  0 0 0 0 0 0 0 0 0 0 ...\n $ text                        : chr  \"@VirginAmerica What @dhepburn said.\" \"@VirginAmerica plus you've added commercials to the experience... tacky.\" \"@VirginAmerica I didn't today... Must mean I need to take another trip!\" \"@VirginAmerica it's really aggressive to blast obnoxious \\\"entertainment\\\" in your guests' faces &amp; they hav\"| __truncated__ ...\n $ tweet_coord                 : chr  \"\" \"\" \"\" \"\" ...\n $ tweet_created               : chr  \"2/24/2015 11:35\" \"2/24/2015 11:15\" \"2/24/2015 11:15\" \"2/24/2015 11:15\" ...\n $ tweet_location              : chr  \"\" \"\" \"Lets Play\" \"\" ...\n $ user_timezone               : chr  \"Eastern Time (US & Canada)\" \"Pacific Time (US & Canada)\" \"Central Time (US & Canada)\" \"Pacific Time (US & Canada)\" ..."
  },
  {
    "objectID": "projects/Business/BrandPerception/brand.html#data-analysis",
    "href": "projects/Business/BrandPerception/brand.html#data-analysis",
    "title": "The Sky’s the Limit: Unveiling the Untold Stories of US Airline Woes on Twitter!",
    "section": "Data Analysis",
    "text": "Data Analysis\n\nData Overview\n\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(dplyr)\n\ntweets <- tweets %>%\n  mutate(airline = factor(airline, levels = names(sort(table(airline), decreasing = TRUE))))\n\nggplot(tweets) +\n  aes(airline, fill = airline) +\n  geom_bar() +\n  geom_text(stat = 'count', aes(label = paste0(round((..count..)/sum(..count..) * 100), \"%\")),   vjust = 1.6, color = \"white\") +\n  labs(title = 'Number of Tweets by Airlines', x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.border = element_blank(),\n    panel.background = element_blank(),\n    legend.position = \"none\",\n    legend.title = element_blank(),\n    legend.text = element_text(size = 10),\n    axis.text.y = element_text(size = 10),\n    legend.key.size = unit(0.5, \"cm\"),\n    legend.key.height = unit(0.5, \"cm\"),\n    legend.key.width = unit(0.5, \"cm\")\n  ) +\n  scale_fill_tableau()\n\n\n\n\n\ntableau_colors <- c(\"#4E79A7\", \"#F28E2B\", \"#E15759\", \"#76B7B2\", \"#59A14F\", \"#EDC948\")\n\ntweets %>%\n  group_by(airline, airline_sentiment) %>%\n  summarise(count = n()) %>%\n  ggplot(aes(airline, count, fill = airline)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ airline_sentiment, ncol = 1) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 9)) +\n  labs(x = \"Airlines\", y = \"Count of Tweet Sentiments\") +\n  theme_minimal() +\n  theme(\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.border = element_blank(),\n    panel.background = element_blank(),\n    legend.position = \"none\",\n    legend.title = element_blank(),\n    legend.text = element_text(size = 10),\n    axis.text.y = element_text(size = 10),\n    legend.key.size = unit(0.5, \"cm\"),\n    legend.key.height = unit(0.5, \"cm\"),\n    legend.key.width = unit(0.5, \"cm\")\n  ) +\n  scale_fill_manual(values = tableau_colors)\n\n\n\n\n\ntweets %>%\n  group_by(negativereason) %>%\n  summarise(count = n(), na.rm = TRUE) %>%\n  arrange(desc(count)) %>%\n  ggplot(aes(reorder(negativereason, count), count)) +\n  geom_bar(fill = \"#4E79A7\", color = \"black\", stat = \"identity\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 9),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.border = element_blank(),\n    panel.background = element_blank(),\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0.5, size = 16),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  labs(x = \"\", y = \"Count of Negative Reasons\") +\n  scale_fill_manual(values = c(\"#4E79A7\"))  # Using Tableau palette color\n\n\n\n\n\n\nExtracting Tweets\n\ntweets.df <- tweets %>%\n  mutate(text = str_replace_all(text, \"[^[:graph:]]\", \" \") %>%\n           gsub(\"https://\", \"\", .) %>% \n           gsub(\"rt\", \"\", .) %>%\n           gsub(\"@\\\\w+\", \"\", .) %>%\n           gsub(\"http://\", \"\", .) %>%\n           gsub(\"[[:digit:]]\", \"\", .) %>%\n           gsub(\"[[:punct:]]\", \"\", .) %>%\n           gsub(\"[ |\\t]{2,}\", \"\", .) %>%\n           gsub(\"[^[:graph:]]\", \" \", .) %>%\n           gsub(\"[[:punct:]]\", \"\", .) %>%\n           gsub(\"[[:cntrl:]]\", \"\", .) %>%\n           gsub(\" $\", \"\", .) %>%\n           gsub(\"\\\\d+\", \"\", .) %>%\n           str_replace_all(., \"[^[:graph:]]\", \" \") %>%\n           tolower())\n\ncleanText <- tweets.df$text\n# remove empty results (if any)\ncleanText <- cleanText[cleanText != \" \"]\n\nhead(tweets.df)\n\n     tweet_id airline_sentiment airline_sentiment_confidence negativereason\n1 5.70306e+17           neutral                       1.0000               \n2 5.70301e+17          positive                       0.3486               \n3 5.70301e+17           neutral                       0.6837               \n4 5.70301e+17          negative                       1.0000     Bad Flight\n5 5.70301e+17          negative                       1.0000     Can't Tell\n6 5.70301e+17          negative                       1.0000     Can't Tell\n         airline airline_sentiment_gold       name negativereason_gold\n1 Virgin America                           cairdin                    \n2 Virgin America                          jnardino                    \n3 Virgin America                        yvonnalynn                    \n4 Virgin America                          jnardino                    \n5 Virgin America                          jnardino                    \n6 Virgin America                          jnardino                    \n  retweet_count\n1             0\n2             0\n3             0\n4             0\n5             0\n6             0\n                                                                                                               text\n1                                                                                                          whatsaid\n2                                                              plus youve added commercials to the experience tacky\n3                                                               i didnt today must mean i need to take another trip\n4           its really aggressive to blast obnoxious enteainment in your guests faces amp they have little recourse\n5                                                                           and its a really big bad thing about it\n6  seriously would paya flight for seats that didnt have this playing its really the only bad thing about flying va\n  tweet_coord   tweet_created tweet_location              user_timezone\n1             2/24/2015 11:35                Eastern Time (US & Canada)\n2             2/24/2015 11:15                Pacific Time (US & Canada)\n3             2/24/2015 11:15      Lets Play Central Time (US & Canada)\n4             2/24/2015 11:15                Pacific Time (US & Canada)\n5             2/24/2015 11:14                Pacific Time (US & Canada)\n6             2/24/2015 11:14                Pacific Time (US & Canada)\n\n\n\n\nFrequency of Tweets\n\n# Apply the mutate function to update 'created' column\ntweets.df <- tweets.df %>% \n  mutate(\n    created = tweet_created %>% \n      # Remove zeros.\n      str_remove_all(pattern = '\\\\+0000') %>%\n      # Parse date.\n      parse_date_time(orders = '%m/%d/%Y %H:%M')\n  )\n\n# Create a new column 'Created_At_Round' by rounding 'created' to hours\ntweets.df <- tweets.df %>% \n  mutate(Created_At_Round = round_date(created, unit = \"hours\") %>% as.POSIXct())\n\n# Extract the minimum value of 'created' column\nmin_created <- min(tweets.df$created)\n\n# Print the minimum value\nprint(min_created)\n\n[1] \"2015-02-16 23:36:00 UTC\"\n\ntweets.df %>% pull(created) %>% max()\n\n[1] \"2015-02-24 11:53:00 UTC\"\n\n\n\nplt <- tweets.df %>% \n  dplyr::count(Created_At_Round) %>% \n  ggplot(mapping = aes(x = Created_At_Round, y = n)) +\n  geom_line(color = \"blue\") +\n  labs(x = \"Date\", y = \"Number of Tweets\", title = \"Number of Tweets per Hour\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\nggplotly(plt)\n\n\n\n\n\n\n\nLoading sentiment word lists\n\nlibrary(tidytext)\n\n# Extract the positive and negative word lists\nwords <- get_sentiments(\"bing\")\n\npositive <- words %>% filter(sentiment == \"positive\")\nnegative <- words %>% filter(sentiment == \"negative\")\n\npos.words <- c(positive$word, 'upgrade', 'Congrats', 'prizes', 'prize', 'thanks', 'thnx', 'Grt', 'gr8', 'plz', 'trending', 'recovering', 'brainstorm', 'leader')\nneg.words <- c(negative$word, 'wtf', 'wait', 'waiting', 'epicfail', 'Fight', 'fighting', 'arrest', 'no', 'not')\n\n\n\nSentiment scoring function\n\nscore.sentiment = function(sentences, pos.words, neg.words, airline, .progress='none')\n{\n  require(plyr)\n  require(stringr)\n  \n  # we are giving vector of sentences as input. \n  # plyr will handle a list or a vector as an \"l\" for us\n  # we want a simple array of scores back, so we use \"l\" + \"a\" + \"ply\" = laply:\n  scores = laply(sentences, function(sentence, pos.words, neg.words) {\n    \n    # clean up sentences with R's regex-driven global substitute, gsub() function:\n    sentence = gsub('https://','',sentence)\n    sentence = gsub('http://','',sentence)\n    sentence = gsub('[^[:graph:]]', ' ',sentence)\n    sentence = gsub('[[:punct:]]', '', sentence)\n    sentence = gsub('[[:cntrl:]]', '', sentence)\n    sentence = gsub('\\\\d+', '', sentence)\n    sentence = str_replace_all(sentence,\"[^[:graph:]]\", \" \")\n    # and convert to lower case:\n    sentence = tolower(sentence)\n    \n    # split into words. str_split is in the stringr package\n    word.list = str_split(sentence, '\\\\s+')\n    # sometimes a list() is one level of hierarchy too much\n    words = unlist(word.list)\n    \n    # compare our words to the dictionaries of positive & negative terms\n    pos.matches = match(words, pos.words)\n    neg.matches = match(words, neg.words)\n    \n    # match() returns the position of the matched term or NA\n    # we just want a TRUE/FALSE:\n    pos.matches = !is.na(pos.matches)\n    neg.matches = !is.na(neg.matches)\n    \n    # TRUE/FALSE will be treated as 1/0 by sum():\n    score = sum(pos.matches) - sum(neg.matches)\n    \n    return(score)\n  }, pos.words, neg.words, .progress=.progress )\n  \n  scores.df = data.frame(score=scores, text=sentences)\n  return(scores.df)\n}\n\n\n\nCalculating the sentiment score\n\nanalysis <- score.sentiment(cleanText, pos.words, neg.words)\n# Sentiment score frequency table\nscore_table <- as.data.frame(table(analysis$score))\ncolnames(score_table) <- c(\"Sentiment Score\", \"Frequency\")\n\nscore_table\n\n   Sentiment Score Frequency\n1               -8         1\n2               -7         1\n3               -6         6\n4               -5        28\n5               -4       142\n6               -3       503\n7               -2      1442\n8               -1      3358\n9                0      5507\n10               1      2540\n11               2       870\n12               3       179\n13               4        50\n14               5        11\n15               6         2\n\n\n\n\nHistogram of sentiment scores\n\nanalysis %>%\n  ggplot(aes(x = score)) +\n geom_histogram(binwidth = 1, fill = \"#2171b5\") +\n  labs(x = \"Sentiment Score\", y = \"Frequency\") +\n  ggtitle(\"Distribution of Sentiment Scores of the Tweets\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n        axis.title = element_text(size = 12),\n        axis.text = element_text(size = 10),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        panel.background = element_blank()) +\n  easy_center_title()\n\n\n\nggsave(\"plot.png\", width = 8, height = 6, dpi = 300)  # Save the plot as an image file\n\n\n\nWordcloud\n\nlibrary(RColorBrewer)\n\n# Create a text corpus\ntext_corpus <- Corpus(VectorSource(cleanText))\ntext_corpus <- tm_map(text_corpus, content_transformer(tolower))\ntext_corpus <- tm_map(text_corpus, removeWords, stopwords(\"english\"))\ntext_corpus <- tm_map(text_corpus, removeWords, c(\"global\", \"globalwarming\"))\n\n# Create a term-document matrix\ntdm <- TermDocumentMatrix(text_corpus)\ntdm <- as.matrix(tdm)\ntdm <- sort(rowSums(tdm), decreasing = TRUE)\ntdm <- data.frame(word = names(tdm), freq = tdm)\n\n\nset.seed(123)\n\nwordcloud(text_corpus, min.freq = 1, max.words = 100, scale = c(2.2,1),\n          colors=brewer.pal(8, \"Dark2\"), random.color = T, random.order = F)\n\n\n\n\n\n\nWord Frequency plot\n\nggplot(tdm[1:20,], aes(x = reorder(word, freq), y = freq, fill = freq)) +\n  geom_bar(stat = \"identity\") +\n  xlab(\"Terms\") +\n  ylab(\"Count\") +\n  coord_flip() +\n  theme(axis.text = element_text(size = 7)) +\n  ggtitle(\"Most Common Word Frequency Plot\") +\n  easy_center_title() +\n  theme_minimal() +\n  scale_fill_gradient(low = \"#E5F5E0\", high = \"#31A354\")"
  },
  {
    "objectID": "projects/Business/BrandPerception/brand.html#network-analysis",
    "href": "projects/Business/BrandPerception/brand.html#network-analysis",
    "title": "The Sky’s the Limit: Unveiling the Untold Stories of US Airline Woes on Twitter!",
    "section": "Network Analysis",
    "text": "Network Analysis\n\nBigram analysis and Network definition\nBigram counts pairwise occurrences of words which appear together in the text.\n\n#bigram\nbi.gram.words <- tweets.df %>% \n  unnest_tokens(\n    input = text, \n    output = bigram, \n    token = 'ngrams', \n    n = 2\n  ) %>% \n  filter(! is.na(bigram))\n\nbi.gram.words %>% \n  select(bigram) %>% \n  head(10)\n\n              bigram\n1         plus youve\n2        youve added\n3  added commercials\n4     commercials to\n5             to the\n6     the experience\n7   experience tacky\n8            i didnt\n9        didnt today\n10        today must\n\n\n\nextra.stop.words <- c('https')\nstopwords.df <- tibble(\n  word = c(stopwords(kind = 'es'),\n           stopwords(kind = 'en'),\n           extra.stop.words)\n)\n\nNext, we filter for stop words and remove white spaces.\n\nbi.gram.words %<>% \n  separate(col = bigram, into = c('word1', 'word2'), sep = ' ') %>% \n  filter(! word1 %in% stopwords.df$word) %>% \n  filter(! word2 %in% stopwords.df$word) %>% \n  filter(! is.na(word1)) %>% \n  filter(! is.na(word2)) \n\nFinally, we group and count by bigram.\n\nbi.gram.count <- bi.gram.words %>% \n  dplyr::count(word1, word2, sort = TRUE) %>% \n  dplyr::rename(weight = n)\n\nbi.gram.count %>% head()\n\n      word1     word2 weight\n1  customer   service    526\n2 cancelled flightled    451\n3      late    flight    232\n4 cancelled  flighted    202\n5   booking  problems    142\n6      late   flightr    142\n\n\nLet us plot the distribution of the weightvalues:\n\n## Note that it is very skewed, for visualization purposes it might be a good idea to perform a transformation, eg log transform:\n\nbi.gram.count %>% \n  mutate(weight = log(weight + 1)) %>% \n  ggplot(mapping = aes(x = weight)) +\n  theme_light() +\n  geom_histogram(fill = \"#2171b5\") +\n  labs(title = \"Bigram log-Weight Distribution\")+\n  theme_minimal()\n\n\n\n\n\n\nNetwork visualization\n\nthreshold <- 50\n\n# For visualization purposes we scale by a global factor. \nScaleWeight <- function(x, lambda) {\n  x / lambda\n}\n\nnetwork <-  bi.gram.count %>%\n  filter(weight > threshold) %>%\n  mutate(weight = ScaleWeight(x = weight, lambda = 2E3)) %>% \n  graph_from_data_frame(directed = FALSE)\n\nplot(\n  network, \n  vertex.size = 1,\n  vertex.label.color = 'black', \n  vertex.label.cex = 0.7, \n  vertex.label.dist = 1,\n  edge.color = 'gray', \n  main = 'Bigram Count Network', \n  sub = glue('Weight Threshold: {threshold}'), \n  alpha = 50\n)\n\n\n\n\nWe can go a step further and make our visualization more dynamic using the networkD3 library.\n\nthreshold <- 50\n\nnetwork <-  bi.gram.count %>%\n  filter(weight > threshold) %>%\n  graph_from_data_frame(directed = FALSE)\n\n# Store the degree.\nV(network)$degree <- strength(graph = network)\n# Compute the weight shares.\nE(network)$width <- E(network)$weight/max(E(network)$weight)\n\n# Create networkD3 object.\nnetwork.D3 <- igraph_to_networkD3(g = network)\n# Define node size.\nnetwork.D3$nodes %<>% mutate(Degree = (1E-2)*V(network)$degree)\n# Define color group\nnetwork.D3$nodes %<>% mutate(Group = 1)\n# Define edges width. \nnetwork.D3$links$Width <- 10*E(network)$width\n\nforceNetwork(\n  Links = network.D3$links, \n  Nodes = network.D3$nodes, \n  Source = 'source', \n  Target = 'target',\n  NodeID = 'name',\n  Group = 'Group', \n  opacity = 0.9,\n  Value = 'Width',\n  Nodesize = 'Degree', \n  # We input a JavaScript function.\n  linkWidth = JS(\"function(d) { return Math.sqrt(d.value); }\"), \n  fontSize = 12,\n  zoom = TRUE, \n  opacityNoHover = 1\n)\n\n\n\n\n\n\n\nReferences:\nBing Liu, Minqing Hu and Junsheng Cheng. “Opinion Observer: Analyzing and Comparing Opinions on the Web.” Proceedings of the 14th International World Wide Web conference (WWW-2005), May 10-14, 2005, Chiba, Japan."
  },
  {
    "objectID": "projects/Business/Churners/churners.html",
    "href": "projects/Business/Churners/churners.html",
    "title": "Customer Churn Prediction",
    "section": "",
    "text": "This report presents an analysis of bank customer churn using a dataset containing various attributes of bank customers. The goal is to predict whether a customer is likely to churn or not. The dataset consists of 10,127 rows and 20 columns, including both numeric and character variables. The objective is to create a model that predicts customer churn using only five features."
  },
  {
    "objectID": "projects/Business/Churners/churners.html#approach-and-methodology",
    "href": "projects/Business/Churners/churners.html#approach-and-methodology",
    "title": "Customer Churn Prediction",
    "section": "Approach and Methodology",
    "text": "Approach and Methodology\nThe methodology employed in predicting bank churners consisted of several steps. Initially, the necessary R packages were loaded, including tidyverse, caret, pROC, MLmetrics, fastDummies, and skimr, to facilitate the analysis. The bank dataset, obtained from an external source, was imported and assigned to the “bank” variable.\nAn Exploratory Data Analysis (EDA) was conducted using the skim function, which provided summary statistics and distributions of the data. Visualizations, including histograms and scatter plots, were created to explore the relationships between variables and the churn outcome.\nPrincipal Component Analysis (PCA) was employed using the prcomp function after preprocessing the data through scaling and centering. The summary function offered insights into the results of PCA, and the screeplot visually displayed the variance explained by each principal component. The top five principal components were extracted, and their loadings were examined.\nTo determine variable importance, a Random Forest model was trained on the entire dataset using the train function from the caret package. The varImp function was used to calculate the importance of variables, and a plot was generated to visualize their significance.\nThe top two principal components, “young_spenders” and “old_spenders,” were selected, along with the top five variables from the dataset. These features were combined to create a new dataset called “banksy.”\nA K-Nearest Neighbors (KNN) model was trained on the banksy dataset using the train function, with the tuning parameter “k” determined through cross-validation. The model’s performance was evaluated using a confusion matrix and the receiver operating characteristic (ROC) curve.\nTo address the class imbalance between churn and non-churn instances, downsampling was performed on the training dataset. The downSample function from the caret package was utilized for this purpose.\nAnother Random Forest model was then trained on the downsampled training dataset, and its performance was evaluated using a confusion matrix and the ROC curve.\nTwo gradient boosted models were trained: one using the original dataset with PCA variables, and the other using only the top five variables. Confusion matrices and ROC curves were utilized to evaluate the performance of both models.\nFinally, the best-performing model, fit_gbm2, was selected based on evaluation metrics and its optimal hyperparameters were printed.\nIn conclusion, the methodology involved various stages, including data preprocessing, exploratory analysis, dimensionality reduction using PCA, variable importance analysis with Random Forest, training and evaluating models such as KNN, Random Forest, and Gradient Boosted models, and ultimately selecting the best-performing model based on evaluation metrics.\n\nLoad the required packages\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(pROC)\nlibrary(MLmetrics)\nlibrary(fastDummies)\nlibrary(skimr)"
  },
  {
    "objectID": "projects/Business/Churners/churners.html#data-summary",
    "href": "projects/Business/Churners/churners.html#data-summary",
    "title": "Customer Churn Prediction",
    "section": "Data Summary:",
    "text": "Data Summary:\nThe dataset contains six character variables and 14 numeric variables. The character variables include gender, education level, marital status, income category, card category, and churn (the target variable). The numeric variables include customer age, dependent count, months on book, total relationship count, months inactive in the last 12 months, contacts count in the last 12 months, credit limit, total revolving balance, average open-to-buy, total amount change from Q4 to Q1, total transaction amount, total transaction count, total count change from Q4 to Q1, and average utilization ratio."
  },
  {
    "objectID": "projects/Business/Churners/churners.html#data-exploration",
    "href": "projects/Business/Churners/churners.html#data-exploration",
    "title": "Customer Churn Prediction",
    "section": "Data Exploration:",
    "text": "Data Exploration:\nThe numeric variables have been analyzed using summary statistics. The mean age of bank customers is 46.33 years, with a standard deviation of 8.02. The age ranges from 26 to 73 years. The dependent count ranges from 0 to 5, with an average of 2.35. The months on book range from 13 to 56, with an average of 35.93. The total relationship count ranges from 1 to 6, with an average of 3.81. The months inactive in the last 12 months range from 0 to 6, with an average of 2.34. The contacts count in the last 12 months ranges from 0 to 6, with an average of 2.46. The credit limit ranges from 1,438.3 to 34,516, with an average of 8,631.95. The total revolving balance ranges from 0 to 2,517, with an average of 1,162.81. The average open-to-buy ranges from 3 to 34,516, with an average of 7,469.14. The total amount change from Q4 to Q1 ranges from 0 to 3.4, with an average of 0.76. The total transaction amount ranges from 510 to 18,484, with an average of 4,404.09. The total transaction count ranges from 10 to 139, with an average of 64.86. The total count change from Q4 to Q1 ranges from 0 to 3.71, with an average of 0.71. The average utilization ratio ranges from 0 to 1, with an average of 0.27.\n\nbank <-readRDS(gzcon(url(\"https://github.com/karolo89/Raw_Data/raw/main/BankChurners.rds\")))  \nbank = bank %>% rename_all(funs(tolower(.))) \n\nskim(bank)\n\n\nData summary\n\n\nName\nbank\n\n\nNumber of rows\n10127\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngender\n0\n1\n1\n1\n0\n2\n0\n\n\neducation_level\n0\n1\n7\n13\n0\n7\n0\n\n\nmarital_status\n0\n1\n6\n8\n0\n4\n0\n\n\nincome_category\n0\n1\n7\n14\n0\n6\n0\n\n\ncard_category\n0\n1\n4\n8\n0\n4\n0\n\n\nchurn\n0\n1\n2\n3\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncustomer_age\n0\n1\n46.33\n8.02\n26.0\n41.00\n46.00\n52.00\n73.00\n▂▆▇▃▁\n\n\ndependent_count\n0\n1\n2.35\n1.30\n0.0\n1.00\n2.00\n3.00\n5.00\n▇▇▇▅▁\n\n\nmonths_on_book\n0\n1\n35.93\n7.99\n13.0\n31.00\n36.00\n40.00\n56.00\n▁▃▇▃▂\n\n\ntotal_relationship_count\n0\n1\n3.81\n1.55\n1.0\n3.00\n4.00\n5.00\n6.00\n▇▇▆▆▆\n\n\nmonths_inactive_12_mon\n0\n1\n2.34\n1.01\n0.0\n2.00\n2.00\n3.00\n6.00\n▅▇▇▁▁\n\n\ncontacts_count_12_mon\n0\n1\n2.46\n1.11\n0.0\n2.00\n2.00\n3.00\n6.00\n▅▇▇▃▁\n\n\ncredit_limit\n0\n1\n8631.95\n9088.78\n1438.3\n2555.00\n4549.00\n11067.50\n34516.00\n▇▂▁▁▁\n\n\ntotal_revolving_bal\n0\n1\n1162.81\n814.99\n0.0\n359.00\n1276.00\n1784.00\n2517.00\n▇▅▇▇▅\n\n\navg_open_to_buy\n0\n1\n7469.14\n9090.69\n3.0\n1324.50\n3474.00\n9859.00\n34516.00\n▇▂▁▁▁\n\n\ntotal_amt_chng_q4_q1\n0\n1\n0.76\n0.22\n0.0\n0.63\n0.74\n0.86\n3.40\n▅▇▁▁▁\n\n\ntotal_trans_amt\n0\n1\n4404.09\n3397.13\n510.0\n2155.50\n3899.00\n4741.00\n18484.00\n▇▅▁▁▁\n\n\ntotal_trans_ct\n0\n1\n64.86\n23.47\n10.0\n45.00\n67.00\n81.00\n139.00\n▂▅▇▂▁\n\n\ntotal_ct_chng_q4_q1\n0\n1\n0.71\n0.24\n0.0\n0.58\n0.70\n0.82\n3.71\n▇▆▁▁▁\n\n\navg_utilization_ratio\n0\n1\n0.27\n0.28\n0.0\n0.02\n0.18\n0.50\n1.00\n▇▂▂▂▁\n\n\n\n\nbank = bank %>% mutate(churn = as.factor(churn))\n\nvariables = bank %>% select(-churn) %>% colnames()"
  },
  {
    "objectID": "projects/Business/Churners/churners.html#principal-component-analysis-pca",
    "href": "projects/Business/Churners/churners.html#principal-component-analysis-pca",
    "title": "Customer Churn Prediction",
    "section": "Principal Component Analysis (PCA):",
    "text": "Principal Component Analysis (PCA):\nPrincipal Component Analysis (PCA) was performed on the dataset to identify the most influential components. The scree plot revealed that the first [number] principal components explain a significant portion of the variance in the data. Based on the component loadings, the “young spenders” and “old spenders” components appear to be the most predictive of churn.\n\nbank = bank %>% mutate(churn = as.factor(churn))\nbank2 = bank %>% select(-churn) %>% dummy_cols(remove_selected_columns = T)\n\nbank3 = cbind(bank2, select(bank,churn))\n\npr_bank = prcomp(x = select(bank3,-churn), scale = T, center = T)\nsummary(pr_bank)\n\nImportance of components:\n                          PC1     PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.0935 1.59707 1.39557 1.33895 1.32059 1.23115 1.17914\nProportion of Variance 0.1184 0.06894 0.05264 0.04845 0.04713 0.04097 0.03758\nCumulative Proportion  0.1184 0.18739 0.24003 0.28848 0.33562 0.37658 0.41416\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     1.16072 1.14016 1.12050 1.10705 1.09341 1.08695 1.07232\nProportion of Variance 0.03641 0.03513 0.03393 0.03312 0.03231 0.03193 0.03108\nCumulative Proportion  0.45057 0.48571 0.51964 0.55276 0.58507 0.61700 0.64808\n                          PC15    PC16    PC17    PC18    PC19    PC20    PC21\nStandard deviation     1.06071 1.04983 1.03504 1.03468 1.01815 1.00334 1.00200\nProportion of Variance 0.03041 0.02979 0.02895 0.02893 0.02802 0.02721 0.02714\nCumulative Proportion  0.67849 0.70828 0.73723 0.76617 0.79418 0.82139 0.84853\n                          PC22    PC23    PC24    PC25    PC26    PC27    PC28\nStandard deviation     0.98911 0.98698 0.94978 0.89307 0.77324 0.73409 0.48917\nProportion of Variance 0.02644 0.02633 0.02438 0.02156 0.01616 0.01456 0.00647\nCumulative Proportion  0.87497 0.90130 0.92568 0.94723 0.96339 0.97796 0.98442\n                          PC29    PC30    PC31      PC32      PC33      PC34\nStandard deviation     0.45819 0.45069 0.40402 3.319e-15 2.797e-15 1.658e-15\nProportion of Variance 0.00567 0.00549 0.00441 0.000e+00 0.000e+00 0.000e+00\nCumulative Proportion  0.99010 0.99559 1.00000 1.000e+00 1.000e+00 1.000e+00\n                            PC35      PC36      PC37\nStandard deviation     8.306e-16 7.947e-16 5.318e-16\nProportion of Variance 0.000e+00 0.000e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.000e+00 1.000e+00\n\nscreeplot(pr_bank, type = \"lines\")\n\n\n\nhead(pr_bank$rotation)\n\n                                  PC1         PC2        PC3         PC4\ncustomer_age             -0.010813997  0.07575479 -0.4330194  0.44349923\ndependent_count           0.036907819 -0.03773176  0.1127192 -0.08119344\nmonths_on_book           -0.005699747  0.07134628 -0.4278530  0.43907287\ntotal_relationship_count -0.049689093  0.20014156 -0.1023431 -0.14783665\nmonths_inactive_12_mon   -0.012256624  0.02657769 -0.1145865 -0.00192171\ncontacts_count_12_mon     0.016040264  0.09090432 -0.1233009 -0.16313733\n                                 PC5          PC6         PC7         PC8\ncustomer_age             -0.26684222  0.003345576 -0.03336354  0.03636211\ndependent_count           0.10910052  0.097514325  0.04081514 -0.04468165\nmonths_on_book           -0.27473438  0.006509941 -0.02940410  0.04106787\ntotal_relationship_count  0.11507528 -0.293862484 -0.10860198  0.11936190\nmonths_inactive_12_mon   -0.05928810  0.039294366  0.04268363 -0.01071041\ncontacts_count_12_mon     0.03555331 -0.047800261  0.05699464 -0.06272773\n                                  PC9        PC10         PC11        PC12\ncustomer_age              0.034871555 -0.02388748 -0.037534809 -0.03807232\ndependent_count          -0.076070481  0.12190979 -0.008339845  0.02454218\nmonths_on_book            0.044670014 -0.02899957 -0.039330876 -0.04986042\ntotal_relationship_count -0.002946349 -0.01856168 -0.023561561 -0.04594087\nmonths_inactive_12_mon   -0.042706255 -0.01254024 -0.001370611 -0.10113015\ncontacts_count_12_mon     0.018184995  0.00377209  0.002264876  0.01355260\n                                PC13        PC14         PC15         PC16\ncustomer_age             -0.06116398  0.06317606  0.006494843  0.009436493\ndependent_count          -0.02638936  0.11642017  0.001673607  0.055360500\nmonths_on_book           -0.07896600  0.06289540  0.002830306  0.003837683\ntotal_relationship_count -0.02254961  0.04741229  0.048781401 -0.041637320\nmonths_inactive_12_mon   -0.01108998 -0.02635780 -0.049552509  0.051869876\ncontacts_count_12_mon     0.07469369 -0.10088119  0.072474077 -0.005302043\n                                  PC17        PC18        PC19         PC20\ncustomer_age              0.0046260131  0.01474486  0.01783912 -0.007356427\ndependent_count           0.1146010390  0.15478813 -0.05897042  0.074815354\nmonths_on_book            0.0001919797  0.01262245  0.02195394  0.015122347\ntotal_relationship_count -0.0408216142  0.01686286  0.01348213 -0.018687486\nmonths_inactive_12_mon    0.0824836888 -0.01448339  0.11059555  0.277638094\ncontacts_count_12_mon     0.0407699668 -0.02726225  0.06562620  0.146390167\n                                PC21        PC22        PC23       PC24\ncustomer_age             -0.06558833 -0.06351209 -0.01064648  0.1024421\ndependent_count          -0.50796833  0.02001298 -0.50564694  0.5719210\nmonths_on_book           -0.06845297 -0.04275912 -0.02657220  0.1239862\ntotal_relationship_count -0.10580938 -0.03651157  0.03068239 -0.1217489\nmonths_inactive_12_mon    0.21584825  0.78096916 -0.40165046 -0.2026135\ncontacts_count_12_mon     0.52037905  0.09210553  0.27449467  0.7165355\n                                PC25        PC26         PC27         PC28\ncustomer_age             -0.02043515  0.02336751 -0.018369091  0.144540965\ndependent_count          -0.06697885  0.10364753 -0.073862975 -0.064763592\nmonths_on_book           -0.03663324  0.05514194 -0.015721295 -0.165681805\ntotal_relationship_count -0.85955423  0.09837121  0.012247228 -0.029775424\nmonths_inactive_12_mon   -0.06269537 -0.01000118  0.032628165  0.003588938\ncontacts_count_12_mon    -0.11331729 -0.08398211 -0.008385573  0.024734695\n                                 PC29         PC30        PC31          PC32\ncustomer_age             -0.635515600  0.261539522 -0.09877505 -1.887639e-15\ndependent_count          -0.026086053  0.032269460  0.01517943  1.925078e-16\nmonths_on_book            0.636461211 -0.234306089  0.09139278  2.426439e-17\ntotal_relationship_count -0.039147775 -0.025969167  0.08419164  1.291153e-16\nmonths_inactive_12_mon   -0.019657762 -0.005034916 -0.01066676 -5.772458e-18\ncontacts_count_12_mon     0.001569711  0.003087043 -0.02410749 -5.374320e-17\n                                  PC33          PC34          PC35\ncustomer_age              0.000000e+00  0.000000e+00  0.000000e+00\ndependent_count          -7.416891e-16  3.532298e-16 -1.361726e-17\nmonths_on_book           -8.967587e-19  3.684053e-17 -6.963161e-17\ntotal_relationship_count -1.599655e-16 -8.128213e-17  1.130684e-16\nmonths_inactive_12_mon   -3.464590e-16  8.140217e-17 -1.148155e-16\ncontacts_count_12_mon    -6.811342e-17 -7.923795e-18  4.187115e-17\n                                  PC36          PC37\ncustomer_age              0.000000e+00  0.000000e+00\ndependent_count          -2.819017e-17  6.676781e-17\nmonths_on_book            1.695052e-16 -3.853568e-17\ntotal_relationship_count -1.974333e-16 -1.370081e-16\nmonths_inactive_12_mon    8.703000e-17 -5.534544e-18\ncontacts_count_12_mon    -1.440254e-17 -1.028874e-16\n\n\n\nrownames_to_column(as.data.frame(pr_bank$rotation)) %>% \n  select(1:5) %>% \n    filter(abs(PC1) >= 0.3 | abs(PC2) >= 0.3 | abs(PC3) >= 0.3 | abs(PC4) >= 0.3)\n\n                  rowname          PC1         PC2         PC3          PC4\n1            customer_age -0.010813997  0.07575479 -0.43301936  0.443499234\n2          months_on_book -0.005699747  0.07134628 -0.42785303  0.439072869\n3            credit_limit  0.413014059 -0.11846478 -0.11563523 -0.003755843\n4     total_revolving_bal -0.034971577 -0.00101243  0.21650964  0.334555958\n5         avg_open_to_buy  0.416062573 -0.11834914 -0.13502121 -0.033748268\n6         total_trans_amt  0.100869534 -0.36992793  0.28988380  0.262444253\n7          total_trans_ct  0.044317988 -0.38057169  0.30325940  0.225273171\n8                gender_F -0.355263483 -0.32165703 -0.19033683 -0.067697739\n9                gender_M  0.355263483  0.32165703  0.19033683  0.067697739\n10 marital_status_Married -0.030806901  0.17389456 -0.04988192  0.320106969\n11     card_category_Blue -0.257857151  0.35138834  0.15913494 -0.022247279\n12   card_category_Silver  0.223199987 -0.30357645 -0.15383454  0.005039861\n\nprc = bind_cols(select(bank3, churn), as.data.frame(pr_bank$x)) %>%\n  select(1:5) %>%\n    rename(\"rich_men\" = PC1, \"cheap_men\" = PC2, \"young_spenders\" = PC3, \"old_spenders\"= PC4)\n\n#based on the graph below, \"young spenders\" and \"old spenders\" seem to be the most predictive of whether the customer will churn. \n\n# Pivot the data to long format\ndf_long <- prc %>%\n  pivot_longer(cols = -churn, names_to = \"component\", values_to = \"loading\")\n\n# Convert churn to a factor variable\ndf_long$churn <- as.factor(df_long$churn)\n\n# Define custom colors\ncustom_colors <- c(\"#E69F00\", \"#56B4E9\") # Replace with your preferred colors\n\n# Plot the density distribution with improved theme\nggplot(df_long, aes(loading, fill = churn)) +\n  geom_density(alpha = 0.5) +\n  facet_grid(. ~ component) +\n  theme_minimal() +  # Set the overall theme to minimal\n  labs(title = \"Density Distribution of Loading\",\n       x = \"Loading\", y = \"Density\") +  # Modify axis labels and title\n  scale_fill_manual(values = custom_colors) +  # Use custom colors\n  theme(plot.title = element_text(size = 16, face = \"bold\"),  # Adjust title appearance\n        axis.text = element_text(size = 12),  # Adjust axis text size\n        legend.title = element_blank(),  # Hide legend title\n        legend.position = \"bottom\")  # Position legend at the bottom"
  },
  {
    "objectID": "projects/Business/Churners/churners.html#random-forest-model",
    "href": "projects/Business/Churners/churners.html#random-forest-model",
    "title": "Customer Churn Prediction",
    "section": "Random Forest Model",
    "text": "Random Forest Model\n\nctrl <- trainControl(method = \"cv\", number = 3, classProbs=TRUE, summaryFunction = twoClassSummary)\n\nbank_index <- createDataPartition(bank$churn, p = 0.80, list = FALSE)\ntrain <- bank[ bank_index, ]\ntest <- bank[-bank_index, ]\n\nbig_model =train(churn ~ .,\n             data = train, \n             method = \"rf\",\n             tunelength = 4,\n             metric = \"ROC\",\n             trControl = ctrl)\n\n# Compute variable importance\nimportance <- varImp(big_model)\n\n# Plot variable importance with customized theme\nggplot(importance, aes(x = reorder(Variables, Importance), y = Importance)) +\n  geom_bar(stat = \"identity\", fill = \"#1f77b4\") +\n  labs(x = \"Variables\", y = \"Importance\") +\n  ggtitle(\"Variable Importance\") +\n  theme_minimal()\n\n\n\n#most important variables are total_trans_ct, total_trans_amt, total_revolving_bal, total_ct_chng_q4_41, total_relationship_count"
  },
  {
    "objectID": "projects/Business/Churners/churners.html#combining-prc-variables-with-top-columns",
    "href": "projects/Business/Churners/churners.html#combining-prc-variables-with-top-columns",
    "title": "Customer Churn Prediction",
    "section": "Combining PRC variables with top columns",
    "text": "Combining PRC variables with top columns\n\n#choosing \"old_spenders\" and \"young_spenders\" to be 2 of the 5 total features in the model: \nprc2 = prc%>% select(young_spenders,old_spenders) \n\n#combining these features with rest of bank ds, then grabbing best variables: \nbanksy = cbind(prc2, bank3) %>% \n            select(young_spenders, old_spenders,total_trans_ct,total_trans_amt,total_revolving_bal, churn)"
  },
  {
    "objectID": "projects/Business/Churners/churners.html#knn-model",
    "href": "projects/Business/Churners/churners.html#knn-model",
    "title": "Customer Churn Prediction",
    "section": "KNN Model",
    "text": "KNN Model\n\n# specify the model to be used (i.e. KNN, Naive Bayes, decision tree, random forest, bagged trees) and the tuning parameters used\n\n\n\nset.seed(504) \n\nbank_index <- createDataPartition(banksy$churn, p = 0.80, list = FALSE)\ntrain <- banksy[ bank_index, ]\ntest <- banksy[-bank_index, ]\n\n# example spec for rf\nfit <- train(churn ~ .,\n             data = train, \n             method = \"knn\",\n             preProcess = c(\"center\",\"scale\"),\n             tuneGrid = expand.grid(k = seq(31,41,2)), # best K between 31 and 41 \n             metric = \"ROC\",\n             trControl = ctrl)\n\nfit\n\nk-Nearest Neighbors \n\n8102 samples\n   5 predictor\n   2 classes: 'no', 'yes' \n\nPre-processing: centered (5), scaled (5) \nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 5401, 5402, 5401 \nResampling results across tuning parameters:\n\n  k   ROC        Sens       Spec     \n  31  0.9388497  0.9677940  0.5814132\n  33  0.9389534  0.9673527  0.5806452\n  35  0.9388799  0.9677940  0.5760369\n  37  0.9390690  0.9676468  0.5691244\n  39  0.9382645  0.9676467  0.5714286\n  41  0.9378081  0.9672054  0.5675883\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was k = 37.\n\nconfusionMatrix(predict(fit, test),factor(test$churn))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   no  yes\n       no  1640  117\n       yes   60  208\n                                          \n               Accuracy : 0.9126          \n                 95% CI : (0.8994, 0.9245)\n    No Information Rate : 0.8395          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.6509          \n                                          \n Mcnemar's Test P-Value : 2.563e-05       \n                                          \n            Sensitivity : 0.9647          \n            Specificity : 0.6400          \n         Pos Pred Value : 0.9334          \n         Neg Pred Value : 0.7761          \n             Prevalence : 0.8395          \n         Detection Rate : 0.8099          \n   Detection Prevalence : 0.8677          \n      Balanced Accuracy : 0.8024          \n                                          \n       'Positive' Class : no              \n                                          \n\nmyRoc <- roc(test$churn, predict(fit, test, type=\"prob\")[,2])\n\nplot(myRoc)\n\n\n\nauc(myRoc)\n\nArea under the curve: 0.9518\n\n#.95 AUC"
  },
  {
    "objectID": "projects/Business/Churners/churners.html#downsampling-bank-data-to-remove-imbalance-of-yesno-churn",
    "href": "projects/Business/Churners/churners.html#downsampling-bank-data-to-remove-imbalance-of-yesno-churn",
    "title": "Customer Churn Prediction",
    "section": "Downsampling bank data to remove imbalance of yes/no churn:",
    "text": "Downsampling bank data to remove imbalance of yes/no churn:\n\ntraindown = downSample(x = train[,-6], y= train$churn) %>% mutate(churn = Class) %>% select(-Class)\ntraindown %>% group_by(churn) %>% count()\n\n# A tibble: 2 × 2\n# Groups:   churn [2]\n  churn     n\n  <fct> <int>\n1 no     1302\n2 yes    1302"
  },
  {
    "objectID": "projects/Business/Churners/churners.html#random-forest-model-with-downsampling",
    "href": "projects/Business/Churners/churners.html#random-forest-model-with-downsampling",
    "title": "Customer Churn Prediction",
    "section": "Random Forest Model with downsampling",
    "text": "Random Forest Model with downsampling\n\nfit <- train(churn ~ .,\n             data = traindown, \n             method = \"rf\",\n             tuneLength = 4, \n             metric = \"ROC\",\n             trControl = ctrl)\n\nconfusionMatrix(predict(fit, test),factor(test$churn))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   no  yes\n       no  1534   26\n       yes  166  299\n                                          \n               Accuracy : 0.9052          \n                 95% CI : (0.8916, 0.9176)\n    No Information Rate : 0.8395          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.7003          \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.9024          \n            Specificity : 0.9200          \n         Pos Pred Value : 0.9833          \n         Neg Pred Value : 0.6430          \n             Prevalence : 0.8395          \n         Detection Rate : 0.7575          \n   Detection Prevalence : 0.7704          \n      Balanced Accuracy : 0.9112          \n                                          \n       'Positive' Class : no              \n                                          \n\nmyRoc <- roc(test$churn, predict(fit, test, type=\"prob\")[,2])\n\nplot(myRoc)\n\n\n\nauc(myRoc) \n\nArea under the curve: 0.9708\n\n# AUC .97"
  },
  {
    "objectID": "projects/Business/Churners/churners.html#gradient-boosted-model-with-pca-vs-only-top-5-variables",
    "href": "projects/Business/Churners/churners.html#gradient-boosted-model-with-pca-vs-only-top-5-variables",
    "title": "Customer Churn Prediction",
    "section": "Gradient boosted model with PCA vs only top 5 variables:",
    "text": "Gradient boosted model with PCA vs only top 5 variables:\n\n#with PCAs \"young spenders\" and \"old spenders\" \n\nfit_gbm1 <- train(churn ~ .,\n             data = train, \n             method = \"gbm\",\n             tuneLength = 4, \n             preProcess = c(\"center\",\"scale\"),\n             metric = \"ROC\",\n             trControl = ctrl)\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8493             nan     0.1000    0.0167\n     2        0.8208             nan     0.1000    0.0132\n     3        0.7961             nan     0.1000    0.0128\n     4        0.7749             nan     0.1000    0.0096\n     5        0.7547             nan     0.1000    0.0106\n     6        0.7382             nan     0.1000    0.0081\n     7        0.7231             nan     0.1000    0.0073\n     8        0.7089             nan     0.1000    0.0063\n     9        0.6957             nan     0.1000    0.0058\n    10        0.6842             nan     0.1000    0.0057\n    20        0.6184             nan     0.1000    0.0024\n    40        0.5604             nan     0.1000    0.0007\n    60        0.5160             nan     0.1000    0.0011\n    80        0.4835             nan     0.1000    0.0005\n   100        0.4548             nan     0.1000    0.0009\n   120        0.4325             nan     0.1000    0.0006\n   140        0.4143             nan     0.1000    0.0002\n   160        0.3980             nan     0.1000    0.0001\n   180        0.3844             nan     0.1000    0.0001\n   200        0.3717             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8127             nan     0.1000    0.0359\n     2        0.7726             nan     0.1000    0.0206\n     3        0.7400             nan     0.1000    0.0164\n     4        0.7122             nan     0.1000    0.0134\n     5        0.6906             nan     0.1000    0.0113\n     6        0.6676             nan     0.1000    0.0108\n     7        0.6520             nan     0.1000    0.0076\n     8        0.6413             nan     0.1000    0.0048\n     9        0.6313             nan     0.1000    0.0045\n    10        0.6185             nan     0.1000    0.0068\n    20        0.5407             nan     0.1000    0.0036\n    40        0.4341             nan     0.1000    0.0016\n    60        0.3817             nan     0.1000    0.0004\n    80        0.3468             nan     0.1000    0.0006\n   100        0.3256             nan     0.1000    0.0002\n   120        0.3109             nan     0.1000    0.0002\n   140        0.2993             nan     0.1000    0.0001\n   160        0.2890             nan     0.1000   -0.0001\n   180        0.2802             nan     0.1000   -0.0002\n   200        0.2725             nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8047             nan     0.1000    0.0366\n     2        0.7552             nan     0.1000    0.0238\n     3        0.7185             nan     0.1000    0.0169\n     4        0.6894             nan     0.1000    0.0137\n     5        0.6623             nan     0.1000    0.0123\n     6        0.6401             nan     0.1000    0.0112\n     7        0.6201             nan     0.1000    0.0099\n     8        0.6033             nan     0.1000    0.0078\n     9        0.5892             nan     0.1000    0.0069\n    10        0.5770             nan     0.1000    0.0057\n    20        0.4799             nan     0.1000    0.0033\n    40        0.3833             nan     0.1000    0.0003\n    60        0.3342             nan     0.1000    0.0003\n    80        0.3068             nan     0.1000    0.0003\n   100        0.2861             nan     0.1000   -0.0001\n   120        0.2729             nan     0.1000    0.0000\n   140        0.2631             nan     0.1000    0.0000\n   160        0.2537             nan     0.1000   -0.0003\n   180        0.2467             nan     0.1000   -0.0002\n   200        0.2394             nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8027             nan     0.1000    0.0384\n     2        0.7519             nan     0.1000    0.0245\n     3        0.7044             nan     0.1000    0.0214\n     4        0.6684             nan     0.1000    0.0175\n     5        0.6409             nan     0.1000    0.0132\n     6        0.6176             nan     0.1000    0.0095\n     7        0.5961             nan     0.1000    0.0098\n     8        0.5756             nan     0.1000    0.0103\n     9        0.5593             nan     0.1000    0.0075\n    10        0.5426             nan     0.1000    0.0076\n    20        0.4453             nan     0.1000    0.0024\n    40        0.3489             nan     0.1000    0.0011\n    60        0.3010             nan     0.1000    0.0009\n    80        0.2785             nan     0.1000   -0.0002\n   100        0.2637             nan     0.1000    0.0001\n   120        0.2525             nan     0.1000   -0.0001\n   140        0.2419             nan     0.1000    0.0001\n   160        0.2317             nan     0.1000   -0.0001\n   180        0.2240             nan     0.1000   -0.0000\n   200        0.2160             nan     0.1000    0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8476             nan     0.1000    0.0157\n     2        0.8221             nan     0.1000    0.0119\n     3        0.7976             nan     0.1000    0.0115\n     4        0.7786             nan     0.1000    0.0091\n     5        0.7599             nan     0.1000    0.0085\n     6        0.7454             nan     0.1000    0.0071\n     7        0.7319             nan     0.1000    0.0068\n     8        0.7194             nan     0.1000    0.0056\n     9        0.7081             nan     0.1000    0.0050\n    10        0.6986             nan     0.1000    0.0045\n    20        0.6374             nan     0.1000    0.0025\n    40        0.5746             nan     0.1000    0.0009\n    60        0.5292             nan     0.1000    0.0006\n    80        0.4919             nan     0.1000    0.0006\n   100        0.4637             nan     0.1000    0.0005\n   120        0.4381             nan     0.1000    0.0008\n   140        0.4184             nan     0.1000    0.0003\n   160        0.4011             nan     0.1000    0.0001\n   180        0.3874             nan     0.1000   -0.0001\n   200        0.3748             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8208             nan     0.1000    0.0302\n     2        0.7800             nan     0.1000    0.0202\n     3        0.7498             nan     0.1000    0.0150\n     4        0.7285             nan     0.1000    0.0106\n     5        0.7024             nan     0.1000    0.0126\n     6        0.6869             nan     0.1000    0.0075\n     7        0.6686             nan     0.1000    0.0091\n     8        0.6535             nan     0.1000    0.0075\n     9        0.6413             nan     0.1000    0.0063\n    10        0.6301             nan     0.1000    0.0054\n    20        0.5467             nan     0.1000    0.0028\n    40        0.4457             nan     0.1000    0.0015\n    60        0.3907             nan     0.1000    0.0008\n    80        0.3626             nan     0.1000    0.0004\n   100        0.3333             nan     0.1000    0.0011\n   120        0.3148             nan     0.1000    0.0006\n   140        0.3003             nan     0.1000   -0.0001\n   160        0.2913             nan     0.1000   -0.0001\n   180        0.2827             nan     0.1000    0.0001\n   200        0.2769             nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8113             nan     0.1000    0.0331\n     2        0.7683             nan     0.1000    0.0204\n     3        0.7329             nan     0.1000    0.0175\n     4        0.7034             nan     0.1000    0.0143\n     5        0.6737             nan     0.1000    0.0142\n     6        0.6503             nan     0.1000    0.0100\n     7        0.6311             nan     0.1000    0.0085\n     8        0.6151             nan     0.1000    0.0067\n     9        0.6007             nan     0.1000    0.0067\n    10        0.5893             nan     0.1000    0.0046\n    20        0.4784             nan     0.1000    0.0064\n    40        0.3864             nan     0.1000    0.0027\n    60        0.3359             nan     0.1000    0.0001\n    80        0.3104             nan     0.1000    0.0001\n   100        0.2926             nan     0.1000    0.0001\n   120        0.2805             nan     0.1000    0.0005\n   140        0.2683             nan     0.1000    0.0003\n   160        0.2582             nan     0.1000   -0.0001\n   180        0.2509             nan     0.1000   -0.0001\n   200        0.2451             nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8061             nan     0.1000    0.0355\n     2        0.7575             nan     0.1000    0.0232\n     3        0.7210             nan     0.1000    0.0180\n     4        0.6890             nan     0.1000    0.0149\n     5        0.6653             nan     0.1000    0.0106\n     6        0.6376             nan     0.1000    0.0124\n     7        0.6114             nan     0.1000    0.0115\n     8        0.5918             nan     0.1000    0.0092\n     9        0.5783             nan     0.1000    0.0058\n    10        0.5601             nan     0.1000    0.0091\n    20        0.4529             nan     0.1000    0.0025\n    40        0.3576             nan     0.1000    0.0010\n    60        0.3134             nan     0.1000    0.0003\n    80        0.2861             nan     0.1000   -0.0001\n   100        0.2729             nan     0.1000    0.0001\n   120        0.2605             nan     0.1000    0.0003\n   140        0.2486             nan     0.1000   -0.0003\n   160        0.2396             nan     0.1000   -0.0001\n   180        0.2303             nan     0.1000   -0.0001\n   200        0.2233             nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8480             nan     0.1000    0.0165\n     2        0.8224             nan     0.1000    0.0112\n     3        0.7985             nan     0.1000    0.0123\n     4        0.7789             nan     0.1000    0.0093\n     5        0.7618             nan     0.1000    0.0076\n     6        0.7453             nan     0.1000    0.0081\n     7        0.7329             nan     0.1000    0.0063\n     8        0.7212             nan     0.1000    0.0057\n     9        0.7100             nan     0.1000    0.0053\n    10        0.7003             nan     0.1000    0.0050\n    20        0.6402             nan     0.1000    0.0027\n    40        0.5782             nan     0.1000    0.0007\n    60        0.5286             nan     0.1000    0.0007\n    80        0.4941             nan     0.1000    0.0007\n   100        0.4644             nan     0.1000    0.0003\n   120        0.4399             nan     0.1000    0.0003\n   140        0.4187             nan     0.1000    0.0003\n   160        0.4020             nan     0.1000   -0.0001\n   180        0.3886             nan     0.1000    0.0002\n   200        0.3748             nan     0.1000    0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8233             nan     0.1000    0.0290\n     2        0.7803             nan     0.1000    0.0202\n     3        0.7457             nan     0.1000    0.0174\n     4        0.7191             nan     0.1000    0.0121\n     5        0.6960             nan     0.1000    0.0110\n     6        0.6797             nan     0.1000    0.0076\n     7        0.6664             nan     0.1000    0.0072\n     8        0.6502             nan     0.1000    0.0077\n     9        0.6363             nan     0.1000    0.0061\n    10        0.6253             nan     0.1000    0.0049\n    20        0.5390             nan     0.1000    0.0017\n    40        0.4288             nan     0.1000    0.0015\n    60        0.3760             nan     0.1000    0.0015\n    80        0.3429             nan     0.1000    0.0009\n   100        0.3205             nan     0.1000    0.0001\n   120        0.3079             nan     0.1000   -0.0002\n   140        0.2934             nan     0.1000    0.0002\n   160        0.2833             nan     0.1000    0.0000\n   180        0.2748             nan     0.1000   -0.0000\n   200        0.2669             nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8097             nan     0.1000    0.0358\n     2        0.7629             nan     0.1000    0.0227\n     3        0.7273             nan     0.1000    0.0164\n     4        0.6969             nan     0.1000    0.0150\n     5        0.6708             nan     0.1000    0.0127\n     6        0.6492             nan     0.1000    0.0096\n     7        0.6323             nan     0.1000    0.0078\n     8        0.6165             nan     0.1000    0.0076\n     9        0.6015             nan     0.1000    0.0071\n    10        0.5813             nan     0.1000    0.0097\n    20        0.4813             nan     0.1000    0.0024\n    40        0.3772             nan     0.1000    0.0013\n    60        0.3301             nan     0.1000    0.0001\n    80        0.2986             nan     0.1000    0.0005\n   100        0.2810             nan     0.1000   -0.0000\n   120        0.2657             nan     0.1000   -0.0000\n   140        0.2554             nan     0.1000   -0.0002\n   160        0.2457             nan     0.1000   -0.0002\n   180        0.2386             nan     0.1000   -0.0002\n   200        0.2326             nan     0.1000   -0.0003\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8074             nan     0.1000    0.0368\n     2        0.7563             nan     0.1000    0.0249\n     3        0.7169             nan     0.1000    0.0202\n     4        0.6832             nan     0.1000    0.0160\n     5        0.6460             nan     0.1000    0.0177\n     6        0.6212             nan     0.1000    0.0119\n     7        0.6041             nan     0.1000    0.0080\n     8        0.5839             nan     0.1000    0.0090\n     9        0.5657             nan     0.1000    0.0086\n    10        0.5475             nan     0.1000    0.0087\n    20        0.4472             nan     0.1000    0.0033\n    40        0.3552             nan     0.1000    0.0005\n    60        0.3057             nan     0.1000    0.0008\n    80        0.2813             nan     0.1000   -0.0001\n   100        0.2632             nan     0.1000   -0.0000\n   120        0.2459             nan     0.1000   -0.0002\n   140        0.2368             nan     0.1000   -0.0000\n   160        0.2282             nan     0.1000   -0.0001\n   180        0.2209             nan     0.1000   -0.0002\n   200        0.2135             nan     0.1000   -0.0005\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8089             nan     0.1000    0.0368\n     2        0.7568             nan     0.1000    0.0246\n     3        0.7171             nan     0.1000    0.0190\n     4        0.6877             nan     0.1000    0.0135\n     5        0.6635             nan     0.1000    0.0116\n     6        0.6365             nan     0.1000    0.0127\n     7        0.6137             nan     0.1000    0.0105\n     8        0.5942             nan     0.1000    0.0097\n     9        0.5764             nan     0.1000    0.0084\n    10        0.5572             nan     0.1000    0.0097\n    20        0.4548             nan     0.1000    0.0030\n    40        0.3516             nan     0.1000    0.0010\n    60        0.3127             nan     0.1000    0.0002\n    80        0.2920             nan     0.1000    0.0002\n   100        0.2746             nan     0.1000   -0.0001\n   120        0.2632             nan     0.1000    0.0002\n   140        0.2528             nan     0.1000   -0.0002\n   160        0.2454             nan     0.1000   -0.0001\n   180        0.2390             nan     0.1000   -0.0003\n   200        0.2334             nan     0.1000   -0.0002\n\nconfusionMatrix(predict(fit_gbm1, test),factor(test$churn))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   no  yes\n       no  1654   80\n       yes   46  245\n                                          \n               Accuracy : 0.9378          \n                 95% CI : (0.9264, 0.9479)\n    No Information Rate : 0.8395          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.7589          \n                                          \n Mcnemar's Test P-Value : 0.003283        \n                                          \n            Sensitivity : 0.9729          \n            Specificity : 0.7538          \n         Pos Pred Value : 0.9539          \n         Neg Pred Value : 0.8419          \n             Prevalence : 0.8395          \n         Detection Rate : 0.8168          \n   Detection Prevalence : 0.8563          \n      Balanced Accuracy : 0.8634          \n                                          \n       'Positive' Class : no              \n                                          \n\nmyRoc <- roc(test$churn, predict(fit_gbm1, test, type=\"prob\")[,2])\n\nplot(myRoc)\n\n\n\n## auc(myRoc)\n#kappa = .76, AUC  = .97\n\n\n#with only top 5 variables \n\nbanksy2 = bank %>% select(total_amt_chng_q4_q1, total_trans_ct, total_trans_amt,total_revolving_bal, total_relationship_count,churn)\n\nbank_index2 <- createDataPartition(banksy2$churn, p = 0.80, list = FALSE)\ntrain2 <- banksy2[ bank_index2, ]\ntest2 <- banksy2[-bank_index2, ]\n\nfit_gbm2 <- train(churn ~ .,\n             data = train2, \n             method = \"gbm\",\n             tuneLength = 4, \n             preProcess = c(\"center\",\"scale\"),\n             metric = \"ROC\",\n             trControl = ctrl)\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8467             nan     0.1000    0.0166\n     2        0.8216             nan     0.1000    0.0132\n     3        0.7986             nan     0.1000    0.0115\n     4        0.7787             nan     0.1000    0.0100\n     5        0.7626             nan     0.1000    0.0085\n     6        0.7480             nan     0.1000    0.0067\n     7        0.7319             nan     0.1000    0.0081\n     8        0.7184             nan     0.1000    0.0067\n     9        0.7067             nan     0.1000    0.0052\n    10        0.6977             nan     0.1000    0.0040\n    20        0.6350             nan     0.1000    0.0026\n    40        0.5637             nan     0.1000    0.0010\n    60        0.5192             nan     0.1000    0.0006\n    80        0.4802             nan     0.1000    0.0005\n   100        0.4473             nan     0.1000    0.0004\n   120        0.4183             nan     0.1000    0.0002\n   140        0.3961             nan     0.1000    0.0002\n   160        0.3782             nan     0.1000    0.0000\n   180        0.3617             nan     0.1000    0.0002\n   200        0.3494             nan     0.1000    0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8192             nan     0.1000    0.0323\n     2        0.7780             nan     0.1000    0.0218\n     3        0.7442             nan     0.1000    0.0169\n     4        0.7174             nan     0.1000    0.0137\n     5        0.6973             nan     0.1000    0.0093\n     6        0.6748             nan     0.1000    0.0108\n     7        0.6594             nan     0.1000    0.0070\n     8        0.6393             nan     0.1000    0.0100\n     9        0.6277             nan     0.1000    0.0051\n    10        0.6140             nan     0.1000    0.0066\n    20        0.5315             nan     0.1000    0.0027\n    40        0.4101             nan     0.1000    0.0015\n    60        0.3475             nan     0.1000    0.0010\n    80        0.3141             nan     0.1000    0.0002\n   100        0.2889             nan     0.1000    0.0003\n   120        0.2668             nan     0.1000    0.0002\n   140        0.2503             nan     0.1000    0.0001\n   160        0.2352             nan     0.1000    0.0002\n   180        0.2258             nan     0.1000    0.0001\n   200        0.2177             nan     0.1000    0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8072             nan     0.1000    0.0364\n     2        0.7560             nan     0.1000    0.0233\n     3        0.7168             nan     0.1000    0.0173\n     4        0.6820             nan     0.1000    0.0168\n     5        0.6552             nan     0.1000    0.0124\n     6        0.6320             nan     0.1000    0.0112\n     7        0.6104             nan     0.1000    0.0104\n     8        0.5934             nan     0.1000    0.0078\n     9        0.5789             nan     0.1000    0.0072\n    10        0.5640             nan     0.1000    0.0065\n    20        0.4660             nan     0.1000    0.0086\n    40        0.3430             nan     0.1000    0.0012\n    60        0.2922             nan     0.1000    0.0002\n    80        0.2613             nan     0.1000    0.0001\n   100        0.2371             nan     0.1000   -0.0000\n   120        0.2192             nan     0.1000   -0.0000\n   140        0.2084             nan     0.1000   -0.0001\n   160        0.1975             nan     0.1000    0.0001\n   180        0.1881             nan     0.1000   -0.0000\n   200        0.1799             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8054             nan     0.1000    0.0395\n     2        0.7540             nan     0.1000    0.0258\n     3        0.7163             nan     0.1000    0.0182\n     4        0.6838             nan     0.1000    0.0164\n     5        0.6522             nan     0.1000    0.0140\n     6        0.6284             nan     0.1000    0.0113\n     7        0.6030             nan     0.1000    0.0119\n     8        0.5818             nan     0.1000    0.0101\n     9        0.5634             nan     0.1000    0.0089\n    10        0.5433             nan     0.1000    0.0086\n    20        0.4268             nan     0.1000    0.0029\n    40        0.3126             nan     0.1000    0.0014\n    60        0.2585             nan     0.1000    0.0005\n    80        0.2292             nan     0.1000    0.0000\n   100        0.2079             nan     0.1000    0.0000\n   120        0.1940             nan     0.1000    0.0000\n   140        0.1829             nan     0.1000   -0.0001\n   160        0.1746             nan     0.1000   -0.0001\n   180        0.1662             nan     0.1000   -0.0001\n   200        0.1589             nan     0.1000    0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8467             nan     0.1000    0.0168\n     2        0.8212             nan     0.1000    0.0128\n     3        0.7963             nan     0.1000    0.0125\n     4        0.7757             nan     0.1000    0.0092\n     5        0.7585             nan     0.1000    0.0092\n     6        0.7434             nan     0.1000    0.0071\n     7        0.7277             nan     0.1000    0.0076\n     8        0.7158             nan     0.1000    0.0057\n     9        0.7022             nan     0.1000    0.0063\n    10        0.6910             nan     0.1000    0.0054\n    20        0.6232             nan     0.1000    0.0029\n    40        0.5513             nan     0.1000    0.0013\n    60        0.5094             nan     0.1000    0.0006\n    80        0.4722             nan     0.1000    0.0005\n   100        0.4448             nan     0.1000    0.0004\n   120        0.4192             nan     0.1000    0.0005\n   140        0.3965             nan     0.1000    0.0003\n   160        0.3783             nan     0.1000    0.0006\n   180        0.3624             nan     0.1000    0.0006\n   200        0.3487             nan     0.1000    0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8154             nan     0.1000    0.0316\n     2        0.7752             nan     0.1000    0.0203\n     3        0.7389             nan     0.1000    0.0173\n     4        0.7128             nan     0.1000    0.0129\n     5        0.6923             nan     0.1000    0.0094\n     6        0.6697             nan     0.1000    0.0109\n     7        0.6564             nan     0.1000    0.0066\n     8        0.6391             nan     0.1000    0.0083\n     9        0.6289             nan     0.1000    0.0048\n    10        0.6149             nan     0.1000    0.0066\n    20        0.5307             nan     0.1000    0.0015\n    40        0.4174             nan     0.1000    0.0011\n    60        0.3652             nan     0.1000    0.0011\n    80        0.3206             nan     0.1000    0.0008\n   100        0.2909             nan     0.1000    0.0003\n   120        0.2691             nan     0.1000    0.0002\n   140        0.2514             nan     0.1000   -0.0001\n   160        0.2366             nan     0.1000    0.0004\n   180        0.2259             nan     0.1000    0.0000\n   200        0.2173             nan     0.1000    0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8078             nan     0.1000    0.0386\n     2        0.7587             nan     0.1000    0.0243\n     3        0.7202             nan     0.1000    0.0190\n     4        0.6891             nan     0.1000    0.0157\n     5        0.6656             nan     0.1000    0.0113\n     6        0.6426             nan     0.1000    0.0109\n     7        0.6193             nan     0.1000    0.0112\n     8        0.6000             nan     0.1000    0.0079\n     9        0.5838             nan     0.1000    0.0075\n    10        0.5675             nan     0.1000    0.0083\n    20        0.4644             nan     0.1000    0.0039\n    40        0.3479             nan     0.1000    0.0029\n    60        0.2944             nan     0.1000    0.0003\n    80        0.2586             nan     0.1000    0.0006\n   100        0.2361             nan     0.1000    0.0001\n   120        0.2199             nan     0.1000    0.0000\n   140        0.2079             nan     0.1000   -0.0001\n   160        0.1971             nan     0.1000   -0.0002\n   180        0.1892             nan     0.1000   -0.0000\n   200        0.1824             nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8009             nan     0.1000    0.0420\n     2        0.7486             nan     0.1000    0.0245\n     3        0.7013             nan     0.1000    0.0211\n     4        0.6671             nan     0.1000    0.0153\n     5        0.6368             nan     0.1000    0.0142\n     6        0.6118             nan     0.1000    0.0114\n     7        0.5879             nan     0.1000    0.0107\n     8        0.5655             nan     0.1000    0.0095\n     9        0.5489             nan     0.1000    0.0077\n    10        0.5320             nan     0.1000    0.0079\n    20        0.4171             nan     0.1000    0.0041\n    40        0.3101             nan     0.1000    0.0013\n    60        0.2567             nan     0.1000    0.0004\n    80        0.2321             nan     0.1000    0.0001\n   100        0.2105             nan     0.1000   -0.0002\n   120        0.1953             nan     0.1000   -0.0001\n   140        0.1846             nan     0.1000   -0.0001\n   160        0.1768             nan     0.1000   -0.0002\n   180        0.1695             nan     0.1000   -0.0002\n   200        0.1623             nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8439             nan     0.1000    0.0177\n     2        0.8182             nan     0.1000    0.0135\n     3        0.7932             nan     0.1000    0.0122\n     4        0.7715             nan     0.1000    0.0106\n     5        0.7537             nan     0.1000    0.0086\n     6        0.7357             nan     0.1000    0.0087\n     7        0.7206             nan     0.1000    0.0067\n     8        0.7095             nan     0.1000    0.0053\n     9        0.6963             nan     0.1000    0.0069\n    10        0.6851             nan     0.1000    0.0055\n    20        0.6165             nan     0.1000    0.0034\n    40        0.5446             nan     0.1000    0.0012\n    60        0.4991             nan     0.1000    0.0007\n    80        0.4644             nan     0.1000    0.0007\n   100        0.4339             nan     0.1000    0.0005\n   120        0.4091             nan     0.1000    0.0003\n   140        0.3870             nan     0.1000    0.0009\n   160        0.3694             nan     0.1000    0.0007\n   180        0.3536             nan     0.1000    0.0002\n   200        0.3389             nan     0.1000    0.0005\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8149             nan     0.1000    0.0349\n     2        0.7703             nan     0.1000    0.0222\n     3        0.7315             nan     0.1000    0.0192\n     4        0.7033             nan     0.1000    0.0141\n     5        0.6797             nan     0.1000    0.0115\n     6        0.6612             nan     0.1000    0.0086\n     7        0.6409             nan     0.1000    0.0096\n     8        0.6282             nan     0.1000    0.0060\n     9        0.6167             nan     0.1000    0.0054\n    10        0.6003             nan     0.1000    0.0074\n    20        0.5183             nan     0.1000    0.0022\n    40        0.4026             nan     0.1000    0.0017\n    60        0.3488             nan     0.1000    0.0009\n    80        0.3055             nan     0.1000    0.0006\n   100        0.2768             nan     0.1000    0.0006\n   120        0.2567             nan     0.1000    0.0001\n   140        0.2402             nan     0.1000    0.0002\n   160        0.2288             nan     0.1000    0.0001\n   180        0.2195             nan     0.1000   -0.0002\n   200        0.2094             nan     0.1000   -0.0001\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.7992             nan     0.1000    0.0398\n     2        0.7478             nan     0.1000    0.0247\n     3        0.7071             nan     0.1000    0.0197\n     4        0.6771             nan     0.1000    0.0149\n     5        0.6480             nan     0.1000    0.0136\n     6        0.6238             nan     0.1000    0.0118\n     7        0.6019             nan     0.1000    0.0101\n     8        0.5829             nan     0.1000    0.0084\n     9        0.5673             nan     0.1000    0.0068\n    10        0.5550             nan     0.1000    0.0053\n    20        0.4559             nan     0.1000    0.0057\n    40        0.3398             nan     0.1000    0.0033\n    60        0.2847             nan     0.1000    0.0011\n    80        0.2527             nan     0.1000    0.0004\n   100        0.2297             nan     0.1000    0.0002\n   120        0.2117             nan     0.1000    0.0003\n   140        0.1983             nan     0.1000   -0.0000\n   160        0.1870             nan     0.1000    0.0000\n   180        0.1791             nan     0.1000   -0.0001\n   200        0.1706             nan     0.1000   -0.0000\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.7974             nan     0.1000    0.0407\n     2        0.7413             nan     0.1000    0.0272\n     3        0.6988             nan     0.1000    0.0214\n     4        0.6671             nan     0.1000    0.0153\n     5        0.6362             nan     0.1000    0.0146\n     6        0.6138             nan     0.1000    0.0106\n     7        0.5930             nan     0.1000    0.0105\n     8        0.5684             nan     0.1000    0.0111\n     9        0.5491             nan     0.1000    0.0089\n    10        0.5314             nan     0.1000    0.0088\n    20        0.4239             nan     0.1000    0.0036\n    40        0.3073             nan     0.1000    0.0011\n    60        0.2541             nan     0.1000    0.0005\n    80        0.2262             nan     0.1000    0.0003\n   100        0.2058             nan     0.1000    0.0000\n   120        0.1888             nan     0.1000    0.0000\n   140        0.1787             nan     0.1000   -0.0001\n   160        0.1687             nan     0.1000    0.0002\n   180        0.1615             nan     0.1000   -0.0001\n   200        0.1540             nan     0.1000   -0.0002\n\nIter   TrainDeviance   ValidDeviance   StepSize   Improve\n     1        0.8034             nan     0.1000    0.0404\n     2        0.7489             nan     0.1000    0.0272\n     3        0.7089             nan     0.1000    0.0205\n     4        0.6773             nan     0.1000    0.0156\n     5        0.6500             nan     0.1000    0.0131\n     6        0.6251             nan     0.1000    0.0120\n     7        0.6002             nan     0.1000    0.0123\n     8        0.5800             nan     0.1000    0.0098\n     9        0.5616             nan     0.1000    0.0080\n    10        0.5435             nan     0.1000    0.0082\n    20        0.4320             nan     0.1000    0.0030\n    40        0.3155             nan     0.1000    0.0011\n    60        0.2638             nan     0.1000    0.0013\n    80        0.2342             nan     0.1000    0.0001\n   100        0.2144             nan     0.1000    0.0000\n   120        0.2008             nan     0.1000   -0.0002\n   140        0.1906             nan     0.1000    0.0000\n   160        0.1836             nan     0.1000   -0.0000\n   180        0.1762             nan     0.1000    0.0000\n   200        0.1711             nan     0.1000   -0.0001\n\nconfusionMatrix(predict(fit_gbm2, test2),factor(test2$churn))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   no  yes\n       no  1673   56\n       yes   27  269\n                                          \n               Accuracy : 0.959           \n                 95% CI : (0.9494, 0.9672)\n    No Information Rate : 0.8395          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.8422          \n                                          \n Mcnemar's Test P-Value : 0.002116        \n                                          \n            Sensitivity : 0.9841          \n            Specificity : 0.8277          \n         Pos Pred Value : 0.9676          \n         Neg Pred Value : 0.9088          \n             Prevalence : 0.8395          \n         Detection Rate : 0.8262          \n   Detection Prevalence : 0.8538          \n      Balanced Accuracy : 0.9059          \n                                          \n       'Positive' Class : no              \n                                          \n\nmyRoc <- roc(test2$churn, predict(fit_gbm2, test2, type=\"prob\")[,2])\n\nplot(myRoc)\n\n\n\n## auc(myRoc)\n#kappa = .85, AUC .99\n\nSurprisingly, model with 5 non-PCA features performed better than the addition of 2 PCA features.\n\n# Here are a few lines to inspect your best model. Add some comments about optimal hyperparameters.\nprint(fit_gbm2)\n\nStochastic Gradient Boosting \n\n8102 samples\n   5 predictor\n   2 classes: 'no', 'yes' \n\nPre-processing: centered (5), scaled (5) \nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 5401, 5402, 5401 \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  ROC        Sens       Spec     \n  1                   50      0.9177438  0.9889703  0.4854071\n  1                  100      0.9456916  0.9872055  0.5453149\n  1                  150      0.9602890  0.9864703  0.6159754\n  1                  200      0.9686083  0.9848528  0.6574501\n  2                   50      0.9660284  0.9872052  0.6013825\n  2                  100      0.9775630  0.9861762  0.7288786\n  2                  150      0.9815884  0.9845587  0.7818740\n  2                  200      0.9832890  0.9824999  0.8033794\n  3                   50      0.9733137  0.9861761  0.6927803\n  3                  100      0.9817069  0.9850002  0.7826421\n  3                  150      0.9847457  0.9826473  0.8095238\n  3                  200      0.9856972  0.9820593  0.8210445\n  4                   50      0.9771594  0.9847054  0.7319508\n  4                  100      0.9844912  0.9827942  0.8056836\n  4                  150      0.9859642  0.9816178  0.8310292\n  4                  200      0.9862191  0.9810296  0.8333333\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 200, interaction.depth =\n 4, shrinkage = 0.1 and n.minobsinnode = 10.\n\nprint(fit_gbm2$bestTune)\n\n   n.trees interaction.depth shrinkage n.minobsinnode\n16     200                 4       0.1             10"
  },
  {
    "objectID": "projects/Business/index.html",
    "href": "projects/Business/index.html",
    "title": "Portfolio",
    "section": "",
    "text": "Market Segmentation Analysis\n\n\n\n\n\n\n\nR\n\n\nUnsupervised Learning\n\n\nCluster Analysis\n\n\n\n\n\n\n\n\n\n\n\nJune 30, 2023\n\n\nOrozco Karol M.\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nThe Sky’s the Limit: Unveiling the Untold Stories of US Airline Woes on Twitter!\n\n\n\n\n\n\n\nR\n\n\nSentiment Analysis\n\n\nBrand\n\n\n\n\n\n\n\n\n\n\n\nApril 20, 2023\n\n\nOrozco Karol M.\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nCustomer Churn Prediction\n\n\n\n\n\n\n\nR\n\n\nMachine Learning\n\n\nPrediction\n\n\n\n\n\n\n\n\n\n\n\nMarch 11, 2023\n\n\nOrozco Karol M.\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/Business/MarketSegmentation/segmentation.html",
    "href": "projects/Business/MarketSegmentation/segmentation.html",
    "title": "Market Segmentation Analysis",
    "section": "",
    "text": "The purpose of this business report is to provide an overview of a project that analyzed a customer database from a mall. The project aimed to uncover underlying patterns in the customer base and group customers into segments, known as market segmentation. By understanding these segments, the marketing team can adopt a more targeted approach to reach consumers, and the mall can make informed strategic decisions to increase profits."
  },
  {
    "objectID": "projects/Business/MarketSegmentation/segmentation.html#dataset-and-methods-used",
    "href": "projects/Business/MarketSegmentation/segmentation.html#dataset-and-methods-used",
    "title": "Market Segmentation Analysis",
    "section": "Dataset and Methods Used",
    "text": "Dataset and Methods Used\nThe dataset used in this analysis is a customer database from a mall, containing 200 observations with basic information such as age, gender, annual income, and spending score. The analysis was performed using the R programming language. Exploratory data analysis, descriptive statistics, data visualization, and unsupervised machine learning techniques (K-Means Clustering) were employed to achieve the project objectives."
  },
  {
    "objectID": "projects/Business/MarketSegmentation/segmentation.html#project-highlights",
    "href": "projects/Business/MarketSegmentation/segmentation.html#project-highlights",
    "title": "Market Segmentation Analysis",
    "section": "Project Highlights",
    "text": "Project Highlights\nThe project yielded several important highlights and insights:\nAge Distribution: The analysis of the age distribution revealed that the majority of customers in the database were adults below 40 years old. There were fewer middle-aged and older adults represented in the customer base.\nAnnual Income Distribution: The distribution of annual income showed that a significant number of customers had an income range of approximately $60,000 to $80,000 per year. However, there were fewer customers with higher annual incomes.\nSpending Score Distribution: The distribution of spending scores exhibited a more balanced pattern, with a concentration around the mean value. However, there were still notable proportions of customers with both low and high spending scores.\nMarket Segmentation: Through the application of K-means clustering, the analysis identified five distinct customer segments based on their demographic and spending characteristics.\n\n“Young and Moderate Income” Cluster: This segment consisted of young individuals with a moderate income level and average spending behavior.\n“Middle-Aged and Moderate Income” Cluster: This segment represented middle-aged individuals with moderate incomes and average spending habits.\n“Young and High Spending” Cluster: Despite having lower incomes, individuals in this cluster exhibited a tendency for high spending.\n“Middle-Aged and Conservative Spending” Cluster: Customers in this cluster had higher incomes but conservative spending habits, emphasizing financial stability and savings.\n“Young and High Income” Cluster: This segment consisted of young individuals with high incomes and high spending scores, representing an affluent customer group.\n\n\nLoad the required packages and dataset\n\nlibrary(ggplot2)\nlibrary(factoextra)\nlibrary(dplyr)\nlibrary(ClusterR)\nlibrary(eclust)\n\n\ncustomer_data <- read.csv(\"https://raw.githubusercontent.com/karolo89/WEBKOE/main/projects/Business/MarketSegmentation/Mall_Customers.csv\")\n\n# Current column names\ncurrent_col_names <- colnames(customer_data)\n\n# New column names\nnew_col_names <- c(\"CustomerID\", \"Gender\", \"Age\", \"AnnualIncome\", \"SpendingScore\")\n\n# Assign the new column names to the data frame\ncolnames(customer_data) <- new_col_names\n\ncustomer_data <- customer_data%>%\n  select(-CustomerID)\nsummary(customer_data)\n\n    Gender               Age         AnnualIncome    SpendingScore  \n Length:200         Min.   :18.00   Min.   : 15.00   Min.   : 1.00  \n Class :character   1st Qu.:28.75   1st Qu.: 41.50   1st Qu.:34.75  \n Mode  :character   Median :36.00   Median : 61.50   Median :50.00  \n                    Mean   :38.85   Mean   : 60.56   Mean   :50.20  \n                    3rd Qu.:49.00   3rd Qu.: 78.00   3rd Qu.:73.00  \n                    Max.   :70.00   Max.   :137.00   Max.   :99.00  \n\n\n\n# Calculate gender percentages\ngender_counts <- table(customer_data$Gender)\ngender_pct <- round(gender_counts / sum(gender_counts) * 100)\ngender_labels <- paste(names(gender_counts), gender_pct, \"%\", sep = \" \")\n\n# Set color palette\ncolors <- c(\"#336699\", \"#99CCFF\")\n\n# Create a data frame for the chart\ngender_data <- data.frame(gender = names(gender_counts), count = gender_counts) %>%\n  mutate(label = gender_labels)\n\n# Create a bar plot to visualize the gender variable\ngender_plot <- ggplot(gender_data, aes(x = gender, y = gender_counts, fill = gender)) +\n  geom_bar(stat = \"identity\", width = 0.6, color = \"white\") +\n  theme_minimal() +\n  scale_fill_manual(values = colors) +\n  labs(x = \"\", y = \"Count\", fill = \"\") +\n  geom_text(aes(label = label), position = position_stack(vjust = 0.5)) +\n  ggtitle(\"Gender Distribution\") +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        axis.text = element_text(size = 12),\n        axis.title = element_text(size = 14, face = \"bold\"),\n        legend.position = \"none\")\n\n# Plot age distribution with modified binwidth\nage_plot <- ggplot(customer_data, aes(x = Age)) +\n  geom_histogram(binwidth = 5, fill = \"#336699\", color = \"white\") +\n  labs(x = \"Age\", y = \"Frequency\") +\n  ggtitle(\"Age Distribution\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        axis.text = element_text(size = 12),\n        axis.title = element_text(size = 14, face = \"bold\"))\n\n# Plot annual income distribution with modified binwidth\nincome_plot <- ggplot(customer_data, aes(x = AnnualIncome)) +\n  geom_histogram(binwidth = 10, fill = \"#336699\", color = \"white\") +\n  labs(x = \"Annual Income (k)\", y = \"Frequency\") +\n  ggtitle(\"Annual Income Distribution\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        axis.text = element_text(size = 12),\n        axis.title = element_text(size = 14, face = \"bold\"))\n\n# Plot spending score distribution with modified binwidth\nscore_plot <- ggplot(customer_data, aes(x = SpendingScore)) +\n  geom_histogram(binwidth = 5, fill = \"#336699\", color = \"white\") +\n  labs(x = \"Spending Score\", y = \"Frequency\") +\n  ggtitle(\"Spending Score Distribution\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        axis.text = element_text(size = 12),\n        axis.title = element_text(size = 14, face = \"bold\"))\n\n# Display the plots\nprint(gender_plot)\n\n\n\nprint(age_plot)\n\n\n\nprint(income_plot)\n\n\n\nprint(score_plot)"
  },
  {
    "objectID": "projects/Business/MarketSegmentation/segmentation.html#k-means-clustering",
    "href": "projects/Business/MarketSegmentation/segmentation.html#k-means-clustering",
    "title": "Market Segmentation Analysis",
    "section": "K-means Clustering",
    "text": "K-means Clustering\n\nSilhoutte\n\nOptimal_Clusters_KMeans(customer_data %>% \n               select(\"Age\", \"AnnualIncome\", \"SpendingScore\"),\n               max_clusters = 10, plot_clusters = T, criterion = 'silhouette')\n\n\n\n\n [1] 0.0000000 0.2931661 0.3839350 0.3923693 0.4269573 0.4523444 0.4132930\n [8] 0.3969484 0.3791567 0.3747368\n\n\n\n\nWCSSE\n\nOptimal_Clusters_KMeans <- function(data, max_clusters, plot_clusters = TRUE, criterion = 'WCSSE') {\n  set.seed(123)\n  \n  # Function to calculate the total within-cluster sum of squares error (WCSSE)\n  calculate_wcsse <- function(k) {\n    kmeans_result <- kmeans(data, k)\n    return(kmeans_result$tot.withinss)\n  }\n  \n  # Calculate the WCSSE for each number of clusters\n  wcsse_values <- sapply(1:max_clusters, calculate_wcsse)\n  \n  if (plot_clusters) {\n    # Plot the WCSSE values\n    plot(1:max_clusters, wcsse_values,\n         type = \"b\", pch = 19, frame = FALSE, \n         xlab = \"Number of clusters (K)\",\n         ylab = \"Total within-cluster sum of squares error (WCSSE)\")\n  }\n  \n  # Determine the optimal number of clusters based on the criterion\n  if (criterion == 'WCSSE') {\n    optimal_clusters <- which.min(wcsse_values)\n    message(\"The optimal number of clusters based on WCSSE is \", optimal_clusters)\n  } else {\n    # Add additional criteria here\n    message(\"Invalid criterion specified.\")\n    return(NULL)\n  }\n  \n  return(optimal_clusters)\n}\n\n# Usage example\noptimal_clusters <- Optimal_Clusters_KMeans(customer_data %>% \n                                             select(\"Age\", \"AnnualIncome\", \"SpendingScore\"),\n                                           max_clusters = 10, plot_clusters = TRUE, criterion = 'WCSSE')\n\n\n\n\n\ncust_kmean <- eclust(customer_data %>% \n               select(AnnualIncome, SpendingScore, Age),\n               stand = TRUE,FUNcluster = \"kmeans\",k=5,graph = F)\n\naggregate(customer_data %>% \n               select(AnnualIncome, SpendingScore, Age),\n          by =list(gerombol=cust_kmean$cluster), FUN = mean)\n\n  gerombol AnnualIncome SpendingScore      Age\n1        1     50.10638      43.61702 27.51064\n2        2     48.48276      41.77586 55.55172\n3        3     25.72727      79.36364 25.27273\n4        4     88.93939      16.96970 41.93939\n5        5     86.10000      81.52500 32.87500\n\n\n\n# Define a custom color palette\nmy_colors <- c(\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\")\n\n# Visualize the clustering results with improved theme and custom colors\nfviz_cluster(cust_kmean, geom = \"point\", pointsize = 2) +\ntheme_minimal() +\n  theme(plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n        axis.text = element_text(size = 12),\n        axis.title = element_text(size = 14, face = \"bold\"))+  scale_fill_manual(values = my_colors)"
  },
  {
    "objectID": "projects/Business/MarketSegmentation/segmentation.html#cluster-analysis",
    "href": "projects/Business/MarketSegmentation/segmentation.html#cluster-analysis",
    "title": "Market Segmentation Analysis",
    "section": "Cluster Analysis",
    "text": "Cluster Analysis\n“Young and Moderate Income” Cluster: This cluster consists of young individuals with a moderate income level and average spending behavior. To target this segment effectively, the marketing manager can focus on offering affordable products or services that align with the interests and preferences of this younger demographic. Promotional campaigns that highlight value for money, convenience, and trendy features could resonate well with this cluster.\n“Middle-Aged and Moderate Income” Cluster: This cluster represents middle-aged individuals with moderate incomes and average spending habits. The marketing manager can target this segment by emphasizing quality, durability, and reliability in their products or services. Highlighting the long-term value and practicality of the offerings can be effective in capturing the attention of this cluster.\n“Young and High Spending” Cluster: Despite their lower incomes, individuals in this cluster exhibit a tendency for high spending. The marketing manager can capitalize on this by offering attractive financing options, discounts, or loyalty programs that reward their spending. Promoting exclusive or unique experiences can also appeal to this segment, as they may prioritize indulgence and personal enjoyment.\n“Middle-Aged and Conservative Spending” Cluster: With higher incomes but conservative spending habits, this cluster values financial stability and savings. The marketing manager can position products or services as long-term investments or solutions that offer cost savings in the long run. Emphasizing features such as durability, reliability, and cost-effectiveness can be effective in attracting this cluster.\n“Young and High Income” Cluster: This cluster represents young individuals with high incomes and high spending scores. The marketing manager can focus on luxury, prestige, and personalized experiences to appeal to this affluent segment. Tailored marketing campaigns, exclusive offers, and high-end branding can effectively capture the attention and loyalty of this cluster.\nIt’s important to further analyze the clusters and their characteristics in order to refine these recommendations and tailor them to the specific goals and objectives of the marketing manager and the business. Additionally, conducting market research and gathering customer insights will provide a more comprehensive understanding of each cluster’s preferences and behaviors, enabling the marketing manager to create targeted and impactful marketing strategies."
  },
  {
    "objectID": "projects/Data_viz_others/ChildMortality/mortality.html",
    "href": "projects/Data_viz_others/ChildMortality/mortality.html",
    "title": "Child Mortality Rates Continue to Decline: Ensuring the Survival of Our Future Generations",
    "section": "",
    "text": "Introduction\nThe global rate of child mortality has reached its lowest point in history, marking a remarkable achievement in the realm of public health. Over the past three decades, child mortality has more than halved, dropping from 12.5 million in 1990 to 5.2 million in 2019. This significant progress deserves recognition and highlights the collective efforts made by nations worldwide. In this article, we delve into the topic of child mortality, explore the current statistics, and discuss the importance of ensuring the survival of our future generations.\n\n\n\nUnderstanding the Sustainable Development Goals (SDGs)\nAs part of the Sustainable Development Goals (SDGs), all countries have committed to achieving a child mortality rate of at least 2.5% by 2030. This means that irrespective of their birthplace, more than 97.5% of children should survive beyond their fifth birthday. However, the reality is that 3.9% of children globally still face the tragic fate of dying before reaching this milestone. This equates to an average of 15,000 child deaths every day, highlighting the urgent need for further action.\n\n\nAnalyzing the Data\nTo gain insights into child mortality rates, we analyzed data from various years, including 1800, 1900, and 2020. The dataset reveals the percentage of children who do not survive beyond their fifth birthday. By visualizing the data, we can better comprehend the progress made and identify areas that require additional attention.\n\n\nData Wrangling\nTo conduct our analysis, we employed various data wrangling techniques using R programming language. We utilized libraries such as tidyverse, showtext, and emojifont to preprocess and visualize the data effectively.\n\n\nPlotting the Data\nThrough the use of ggplot2, a popular data visualization package in R, we created a visually appealing plot that showcases the child mortality rates over time. The plot employs a grid layout, with each grid representing a specific year. Within each grid, we display icons of individuals, where each icon represents a child. The icons are color-coded to differentiate between children who survive and those who perish before the age of five. The plot provides a striking visual representation of the progress made and the work that lies ahead.\n\n\nConclusion\nThe decline in child mortality rates is a testament to the collective efforts of governments, organizations, and individuals working towards the well-being of children worldwide. However, the battle is far from over. We must continue our relentless pursuit of ensuring that every child has the opportunity to thrive and contribute to our society. By investing in healthcare, education, and social support systems, we can strive to achieve the Sustainable Development Goals and secure a brighter future for all children.\nAs we move forward, let us remember that behind each statistic lies a precious life, a potential future leader, and a source of boundless possibilities. Together, we can make a difference and create a world where every child lives beyond their fifth birthday, paving the way for a healthier, happier, and more prosperous future.\n\nLoad the required packages and dataset\n\nlibrary(emojifont)\nlibrary(tidyverse)\nlibrary(usefunc)\nlibrary(showtext)\n\n\n\n\nCode\n\n# Load fonts\nfont_add_google(\"Roboto Slab\", \"slab\")\nfont_add_google(\"Roboto\", \"roboto\")\nshowtext_auto()\n\n\n# Read data\ndf <- tibble(year = c(1800, 1900, 2020),\n             percent_die = c(43.3, 36.2, 3.9))\n\n# Prepare data\ndf_data <- df %>%\n  mutate(per_hundred = round(percent_die),\n         survive = (100 - per_hundred),\n         facet_label = paste0(year, \" (\", survive, \"%)\")) %>%\n  pivot_longer(cols = 3:4, values_to = \"perc\", names_to = \"type\") %>%\n  select(-percent_die)\n\nplot_data_grid <- rep_df(expand.grid(x = rep(1:10), y = rep(1:10)), length(unique(df_data$facet_label))) %>%\n  mutate(year = rep(unique(df_data$facet_label), each = 100),\n         label = fontawesome('fa-user'),\n         type = rep(df_data$type, times = df_data$perc))\n\n# Create plot\np <- ggplot() +\n  geom_text(data = plot_data_grid,\n            mapping = aes(x = x,\n                          y = y,\n                          label = label,\n                          colour = type),\n            family = 'fontawesome-webfont', size = 12) +\n  facet_wrap(~year) +\n  scale_colour_manual(values = c(\"lightgray\",\"#3D5A80\"),\n                      labels = c(\"Survive\", \"Perish\"),\n                      name = \"Outcome\") +\n  labs(title = \"Child Mortality Rates Continue to Fall\",\n       subtitle = str_wrap_break(\"What Percentage of Children Live Beyond Their Fifth Birthday?\\n\\n\", 70),\n       caption = \"Karol O. | Data: ourworldindata.org\",\n       x = \"\",\n       y = \"\") +\n  theme_minimal() +\n  theme(panel.spacing = unit(2, \"lines\"),\n        plot.background = element_rect(fill = \"white\", colour = \"white\"),\n        panel.background = element_rect(fill = \"white\", colour = \"white\"),\n        strip.background = element_rect(fill = \"white\", colour = \"white\"),\n        strip.text = element_text(colour = '#3D5A80', family = \"slab\", size = 16),\n        plot.title = element_text(colour = \"#3D5A80\", size = 24, hjust = 0.5, family = \"slab\"),\n        plot.subtitle = element_text(colour = \"#404040\", size = 16, hjust = 0.5, family = \"slab\"),\n        plot.caption = element_text(colour = \"#404040\", size = 14, hjust = 0.01, family = \"slab\"),\n        plot.margin = unit(c(1, 1, 1, 1), \"cm\"),\n        legend.position = \"none\",\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.text = element_blank()) +\n   \n  guides(colour = guide_legend(override.aes = list(size = 6)))\n\n# Adjust figure size and save as PNG\nggsave(\"child_mortality.png\", p, width = 10, height = 6, dpi = 300)"
  },
  {
    "objectID": "projects/Data_viz_others/index.html",
    "href": "projects/Data_viz_others/index.html",
    "title": "Portfolio",
    "section": "",
    "text": "Crunching Numbers with Style: Building a Sleek Calculator in HTML\n\n\n\n\n\n\n\nhtml\n\n\nJavaScript\n\n\n\n\n\n\n\n\n\n\n\nJuly 4, 2023\n\n\nOrozco Karol M.\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nChild Mortality Rates Continue to Decline: Ensuring the Survival of Our Future Generations\n\n\n\n\n\n\n\nR\n\n\nData Viz\n\n\nData For Good\n\n\n\n\n\n\n\n\n\n\n\nJune 30, 2023\n\n\nOrozco Karol M.\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/portfolio.html",
    "href": "projects/portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Crunching Numbers with Style: Building a Sleek Calculator in HTML\n\n\n\n\n\n\n\n\n\n\n\n\nJuly 4, 2023\n\n\nOrozco Karol M.\n\n\n\n\n\n\n  \n\n\n\n\nMarket Segmentation Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nJune 30, 2023\n\n\nOrozco Karol M.\n\n\n\n\n\n\n  \n\n\n\n\nChild Mortality Rates Continue to Decline: Ensuring the Survival of Our Future Generations\n\n\n\n\n\n\n\n\n\n\n\n\nJune 30, 2023\n\n\nOrozco Karol M.\n\n\n\n\n\n\n  \n\n\n\n\nThe Sky’s the Limit: Unveiling the Untold Stories of US Airline Woes on Twitter!\n\n\n\n\n\n\n\n\n\n\n\n\nApril 20, 2023\n\n\nOrozco Karol M.\n\n\n\n\n\n\n  \n\n\n\n\nCustomer Churn Prediction\n\n\n\n\n\n\n\n\n\n\n\n\nMarch 11, 2023\n\n\nOrozco Karol M.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "projects/Data_viz_others/Calculator/Calculator.html",
    "href": "projects/Data_viz_others/Calculator/Calculator.html",
    "title": "Crunching Numbers with Style: Building a Sleek Calculator in HTML",
    "section": "",
    "text": "Building a Calculator in HTML\n\n\n\n\nIn this article, we will explore how to create a simple business calculator using HTML. The calculator will provide basic arithmetic operations such as addition, subtraction, multiplication, division, and modulus.\n\nHTML Structure\nFirst, let’s take a look at the HTML structure of our business calculator:\n\n<!DOCTYPE html>\n<html>\n<head>\n  <title>Calculator</title>\n \nWe have a basic HTML structure with a\n\nsection for the title and CSS styles, and a\n\nsection that contains the calculator’s content and JavaScript logic.\n\n\nCalculator Layout and Styles\nTo create an appealing calculator interface, we’ll define CSS styles that will be applied to the calculator elements. Here’s the CSS code:\n\n  <style>\n    body {\n      font-family: Arial, sans-serif;\n      text-align: center;\n      background-color: #f7f7f7;\n    }\n\n    .calculator {\n      width: 300px;\n      margin: 0 auto;\n      border: 1px solid #ccc;\n      border-radius: 5px;\n      background-color: #fff;\n      padding: 20px;\n      box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n    }\n\n    h1 {\n      color: #333;\n      font-size: 24px;\n      margin-bottom: 20px;\n    }\n\n    input[type=\"text\"] {\n      width: 100%;\n      margin-bottom: 20px;\n      padding: 10px;\n      font-size: 20px;\n    }\n\n    .btn-row {\n      display: flex;\n      justify-content: space-between;\n    }\n\n    .btn {\n      width: 23%;\n      padding: 10px;\n      margin-bottom: 10px;\n      font-size: 18px;\n      border-radius: 5px;\n      background-color: #eee;\n      cursor: pointer;\n      transition: background-color 0.3s ease;\n    }\n\n    .btn:hover {\n      background-color: #ddd;\n    }\n\n    .btn.operation {\n      background-color: #337ab7;\n      color: #fff;\n    }\n\n    .btn.operation:hover {\n      background-color: #23527c;\n    }\n\n    .btn.equal {\n      background-color: #5cb85c;\n      color: #fff;\n    }\n\n    .btn.equal:hover {\n      background-color: #449d44;\n    }\n\n    .btn.clear {\n      background-color: #d9534f;\n      color: #fff;\n    }\n\n    .btn.clear:hover {\n      background-color: #c9302c;\n    }\n  </style>\nThe CSS styles include formatting for the calculator’s body, calculator container, heading, input field, buttons, and their different states (e.g., hover, operation, equal, and clear).\n\n\nCalculator Content\nInside the calculator\n\n, we’ll include the calculator’s content, which consists of the heading, input field for displaying the result, and buttons for user interaction. Here’s the code:\n\n  <div class=\"calculator\">\n    <h1>Business Calculator</h1>\n    <input type=\"text\" id=\"result\" readonly>\n    <div class=\"btn-row\">\n      <input type=\"button\" class=\"btn clear\" value=\"Clear\" onclick=\"clearResult()\">\n      <input type=\"button\" class=\"btn operation\" value=\"/\" onclick=\"appendToResult('/')\">\n      <input type=\"button\" class=\"btn operation\" value=\"*\" onclick=\"appendToResult('*')\">\n      <input type=\"button\" class=\"btn operation\" value=\"-\" onclick=\"appendToResult('-')\">\n    </div>\n    <div class=\"btn-row\">\n      <input type=\"button\" class=\"btn\" value=\"7\" onclick=\"appendToResult('7')\">\n      <input type=\"button\" class=\"btn\" value=\"8\" onclick=\"appendToResult('8')\">\n      <input type=\"button\" class=\"btn\" value=\"9\" onclick=\"appendToResult('9')\">\n      <input type=\"button\" class=\"btn operation\" value=\"+\" onclick=\"appendToResult('+')\">\n    </div>\n    <div class=\"btn-row\">\n      <input type=\"button\" class=\"btn\" value=\"4\" onclick=\"appendToResult('4')\">\n      <input type=\"button\" class=\"btn\" value=\"5\" onclick=\"appendToResult('5')\">\n      <input type=\"button\" class=\"btn\" value=\"6\" onclick=\"appendToResult('6')\">\n      <input type=\"button\" class=\"btn operation\" value=\"%\" onclick=\"appendToResult('%')\">\n    </div>\n    <div class=\"btn-row\">\n      <input type=\"button\" class=\"btn\" value=\"1\" onclick=\"appendToResult('1')\">\n      <input type=\"button\" class=\"btn\" value=\"2\" onclick=\"appendToResult('2')\">\n      <input type=\"button\" class=\"btn\" value=\"3\" onclick=\"appendToResult('3')\">\n      <input type=\"button\" class=\"btn equal\" value=\"=\" onclick=\"calculateResult()\">\n    </div>\n    <div class=\"btn-row\">\n      <input type=\"button\" class=\"btn\" value=\"0\" onclick=\"appendToResult('0')\">\n      <input type=\"button\" class=\"btn\" value=\".\" onclick=\"appendToResult('.')\">\n    </div>\n  </div>\nThe calculator content includes the heading “Business Calculator,” an input field for displaying the result with the ID “result,” and several\n\nsections for organizing the rows of buttons. Each button has an assigned value and an onclick attribute that triggers JavaScript functions to perform specific actions.\n\n\nJavaScript Logic\nTo enable the calculator’s functionality, we need to define JavaScript functions that handle button clicks and perform calculations. Here’s the JavaScript code:\n\n  <script>\n    function appendToResult(value) {\n      document.getElementById('result').value += value;\n    }\n\n    function calculateResult() {\n      const result = eval(document.getElementById('result').value);\n      document.getElementById('result').value = result;\n    }\n\n    function clearResult() {\n      document.getElementById('result').value = '';\n    }\n  </script>\nThe JavaScript functions include appendToResult(value) to append values to the result input field, calculateResult() to evaluate the mathematical expression and update the result, and clearResult() to clear the result input field.\n\n\nPutting It All Together\nBy combining the HTML structure, CSS styles, calculator content, and JavaScript logic, we create a fully functional business calculator. Users can input numbers and perform basic arithmetic operations by clicking the corresponding buttons.\nThe complete code for the business calculator can be found below:\n<!DOCTYPE html>\n<html>\n<head>\n  <title>Business Calculator</title>\n  <style>\n    body {\n      font-family: Arial, sans-serif;\n      text-align: center;\n      background-color: #f7f7f7;\n    }\n\n    .calculator {\n      width: 300px;\n      margin: 0 auto;\n      border: 1px solid #ccc;\n      border-radius: 5px;\n      background-color: #fff;\n      padding: 20px;\n      box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n    }\n\n    h1 {\n      color: #333;\n      font-size: 24px;\n      margin-bottom: 20px;\n    }\n\n    input[type=\"text\"] {\n      width: 100%;\n      margin-bottom: 20px;\n      padding: 10px;\n      font-size: 20px;\n    }\n\n    .btn-row {\n      display: flex;\n      justify-content: space-between;\n    }\n\n    .btn {\n      width: 23%;\n      padding: 10px;\n      margin-bottom: 10px;\n      font-size: 18px;\n      border-radius: 5px;\n      background-color: #eee;\n      cursor: pointer;\n      transition: background-color 0.3s ease;\n    }\n\n    .btn:hover {\n      background-color: #ddd;\n    }\n\n    .btn.operation {\n      background-color: #337ab7;\n      color: #fff;\n    }\n\n    .btn.operation:hover {\n      background-color: #23527c;\n    }\n\n    .btn.equal {\n      background-color: #5cb85c;\n      color: #fff;\n    }\n\n    .btn.equal:hover {\n      background-color: #449d44;\n    }\n\n    .btn.clear {\n      background-color: #d9534f;\n      color: #fff;\n    }\n\n    .btn.clear:hover {\n      background-color: #c9302c;\n    }\n  </style>\n</head>\n<body>\n  <div class=\"calculator\">\n    <h1>Business Calculator</h1>\n    <input type=\"text\" id=\"result\" readonly>\n    <div class=\"btn-row\">\n      <input type=\"button\" class=\"btn clear\" value=\"Clear\" onclick=\"clearResult()\">\n      <input type=\"button\" class=\"btn operation\" value=\"/\" onclick=\"appendToResult('/')\">\n      <input type=\"button\" class=\"btn operation\" value=\"*\" onclick=\"appendToResult('*')\">\n      <input type=\"button\" class=\"btn operation\" value=\"-\" onclick=\"appendToResult('-')\">\n    </div>\n    <div class=\"btn-row\">\n      <input type=\"button\" class=\"btn\" value=\"7\" onclick=\"appendToResult('7')\">\n      <input type=\"button\" class=\"btn\" value=\"8\" onclick=\"appendToResult('8')\">\n      <input type=\"button\" class=\"btn\" value=\"9\" onclick=\"appendToResult('9')\">\n      <input type=\"button\" class=\"btn operation\" value=\"+\" onclick=\"appendToResult('+')\">\n    </div>\n    <div class=\"btn-row\">\n      <input type=\"button\" class=\"btn\" value=\"4\" onclick=\"appendToResult('4')\">\n      <input type=\"button\" class=\"btn\" value=\"5\" onclick=\"appendToResult('5')\">\n      <input type=\"button\" class=\"btn\" value=\"6\" onclick=\"appendToResult('6')\">\n      <input type=\"button\" class=\"btn operation\" value=\"%\" onclick=\"appendToResult('%')\">\n    </div>\n    <div class=\"btn-row\">\n      <input type=\"button\" class=\"btn\" value=\"1\" onclick=\"appendToResult('1')\">\n      <input type=\"button\" class=\"btn\" value=\"2\" onclick=\"appendToResult('2')\">\n      <input type=\"button\" class=\"btn\" value=\"3\" onclick=\"appendToResult('3')\">\n      <input type=\"button\" class=\"btn equal\" value=\"=\" onclick=\"calculateResult()\">\n    </div>\n    <div class=\"btn-row\">\n      <input type=\"button\" class=\"btn\" value=\"0\" onclick=\"appendToResult('0')\">\n      <input type=\"button\" class=\"btn\" value=\".\" onclick=\"appendToResult('.')\">\n    </div>\n  </div>\n\n  <script>\n    function appendToResult(value) {\n      document.getElementById('result').value += value;\n    }\n\n    function calculateResult() {\n      const result = eval(document.getElementById('result').value);\n      document.getElementById('result').value = result;\n    }\n\n    function clearResult() {\n      document.getElementById('result').value = '';\n    }\n  </script>\n</body>\n</html>"
  }
]