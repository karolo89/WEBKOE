---
title: "Customer Churn Prediction"
author: "Orozco Karol M."
image: churn.jpg
date: "3-11-2023"
format:
  html:
    toc: true
    toc-location: right
    html-math-method: katex
    page-layout: full
execute: 
  warning: false
  message: false
categories: [R, Machine Learning, Prediction]
---

## Introduction:

This report presents an analysis of bank customer churn using a dataset containing various attributes of bank customers. The goal is to predict whether a customer is likely to churn or not. The dataset consists of 10,127 rows and 20 columns, including both numeric and character variables. The objective is to create a model that predicts customer churn using only five features.

## Approach and Methodology

The methodology employed in predicting bank churners consisted of several steps. Initially, the necessary R packages were loaded, including tidyverse, caret, pROC, MLmetrics, fastDummies, and skimr, to facilitate the analysis. The bank dataset, obtained from an external source, was imported and assigned to the "bank" variable.

An Exploratory Data Analysis (EDA) was conducted using the skim function, which provided summary statistics and distributions of the data. Visualizations, including histograms and scatter plots, were created to explore the relationships between variables and the churn outcome.

Principal Component Analysis (PCA) was employed using the prcomp function after preprocessing the data through scaling and centering. The summary function offered insights into the results of PCA, and the screeplot visually displayed the variance explained by each principal component. The top five principal components were extracted, and their loadings were examined.

To determine variable importance, a Random Forest model was trained on the entire dataset using the train function from the caret package. The varImp function was used to calculate the importance of variables, and a plot was generated to visualize their significance.

The top two principal components, "young_spenders" and "old_spenders," were selected, along with the top five variables from the dataset. These features were combined to create a new dataset called "banksy."

A K-Nearest Neighbors (KNN) model was trained on the banksy dataset using the train function, with the tuning parameter "k" determined through cross-validation. The model's performance was evaluated using a confusion matrix and the receiver operating characteristic (ROC) curve.

To address the class imbalance between churn and non-churn instances, downsampling was performed on the training dataset. The downSample function from the caret package was utilized for this purpose.

Another Random Forest model was then trained on the downsampled training dataset, and its performance was evaluated using a confusion matrix and the ROC curve.

Two gradient boosted models were trained: one using the original dataset with PCA variables, and the other using only the top five variables. Confusion matrices and ROC curves were utilized to evaluate the performance of both models.

Finally, the best-performing model, fit_gbm2, was selected based on evaluation metrics and its optimal hyperparameters were printed.

In conclusion, the methodology involved various stages, including data preprocessing, exploratory analysis, dimensionality reduction using PCA, variable importance analysis with Random Forest, training and evaluating models such as KNN, Random Forest, and Gradient Boosted models, and ultimately selecting the best-performing model based on evaluation metrics.

#### Load the required packages

```{r}
library(tidyverse)
library(caret)
library(pROC)
library(MLmetrics)
library(fastDummies)
library(skimr)
```


## Data Summary:

The dataset contains six character variables and 14 numeric variables. The character variables include gender, education level, marital status, income category, card category, and churn (the target variable). The numeric variables include customer age, dependent count, months on book, total relationship count, months inactive in the last 12 months, contacts count in the last 12 months, credit limit, total revolving balance, average open-to-buy, total amount change from Q4 to Q1, total transaction amount, total transaction count, total count change from Q4 to Q1, and average utilization ratio.


## Data Exploration:

The numeric variables have been analyzed using summary statistics. The mean age of bank customers is 46.33 years, with a standard deviation of 8.02. The age ranges from 26 to 73 years. The dependent count ranges from 0 to 5, with an average of 2.35. The months on book range from 13 to 56, with an average of 35.93. The total relationship count ranges from 1 to 6, with an average of 3.81. The months inactive in the last 12 months range from 0 to 6, with an average of 2.34. The contacts count in the last 12 months ranges from 0 to 6, with an average of 2.46. The credit limit ranges from 1,438.3 to 34,516, with an average of 8,631.95. The total revolving balance ranges from 0 to 2,517, with an average of 1,162.81. The average open-to-buy ranges from 3 to 34,516, with an average of 7,469.14. The total amount change from Q4 to Q1 ranges from 0 to 3.4, with an average of 0.76. The total transaction amount ranges from 510 to 18,484, with an average of 4,404.09. The total transaction count ranges from 10 to 139, with an average of 64.86. The total count change from Q4 to Q1 ranges from 0 to 3.71, with an average of 0.71. The average utilization ratio ranges from 0 to 1, with an average of 0.27.

```{r}
bank <-readRDS(gzcon(url("https://github.com/karolo89/Raw_Data/raw/main/BankChurners.rds")))  
bank = bank %>% rename_all(funs(tolower(.))) 

skim(bank)

bank = bank %>% mutate(churn = as.factor(churn))

variables = bank %>% select(-churn) %>% colnames() 

```
## Principal Component Analysis (PCA):

Principal Component Analysis (PCA) was performed on the dataset to identify the most influential components. The scree plot revealed that the first [number] principal components explain a significant portion of the variance in the data. Based on the component loadings, the "young spenders" and "old spenders" components appear to be the most predictive of churn.

```{r}
bank = bank %>% mutate(churn = as.factor(churn))
bank2 = bank %>% select(-churn) %>% dummy_cols(remove_selected_columns = T)

bank3 = cbind(bank2, select(bank,churn))

pr_bank = prcomp(x = select(bank3,-churn), scale = T, center = T)
summary(pr_bank)

screeplot(pr_bank, type = "lines")

head(pr_bank$rotation)
```

```{r}
rownames_to_column(as.data.frame(pr_bank$rotation)) %>% 
  select(1:5) %>% 
    filter(abs(PC1) >= 0.3 | abs(PC2) >= 0.3 | abs(PC3) >= 0.3 | abs(PC4) >= 0.3)

prc = bind_cols(select(bank3, churn), as.data.frame(pr_bank$x)) %>%
  select(1:5) %>%
    rename("rich_men" = PC1, "cheap_men" = PC2, "young_spenders" = PC3, "old_spenders"= PC4)

#based on the graph below, "young spenders" and "old spenders" seem to be the most predictive of whether the customer will churn. 

# Pivot the data to long format
df_long <- prc %>%
  pivot_longer(cols = -churn, names_to = "component", values_to = "loading")

# Convert churn to a factor variable
df_long$churn <- as.factor(df_long$churn)

# Define custom colors
custom_colors <- c("#E69F00", "#56B4E9") # Replace with your preferred colors

# Plot the density distribution with improved theme
ggplot(df_long, aes(loading, fill = churn)) +
  geom_density(alpha = 0.5) +
  facet_grid(. ~ component) +
  theme_minimal() +  # Set the overall theme to minimal
  labs(title = "Density Distribution of Loading",
       x = "Loading", y = "Density") +  # Modify axis labels and title
  scale_fill_manual(values = custom_colors) +  # Use custom colors
  theme(plot.title = element_text(size = 16, face = "bold"),  # Adjust title appearance
        axis.text = element_text(size = 12),  # Adjust axis text size
        legend.title = element_blank(),  # Hide legend title
        legend.position = "bottom")  # Position legend at the bottom


```


## Random Forest Model 

```{r}
ctrl <- trainControl(method = "cv", number = 3, classProbs=TRUE, summaryFunction = twoClassSummary)

bank_index <- createDataPartition(bank$churn, p = 0.80, list = FALSE)
train <- bank[ bank_index, ]
test <- bank[-bank_index, ]

big_model =train(churn ~ .,
             data = train, 
             method = "rf",
             tunelength = 4,
             metric = "ROC",
             trControl = ctrl)

# Compute variable importance
importance <- varImp(big_model)

# Plot variable importance with customized theme
ggplot(importance, aes(x = reorder(Variables, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "#1f77b4") +
  labs(x = "Variables", y = "Importance") +
  ggtitle("Variable Importance") +
  theme_minimal()

#most important variables are total_trans_ct, total_trans_amt, total_revolving_bal, total_ct_chng_q4_41, total_relationship_count 
```

## Combining PRC variables with top columns 

```{r}
#choosing "old_spenders" and "young_spenders" to be 2 of the 5 total features in the model: 
prc2 = prc%>% select(young_spenders,old_spenders) 

#combining these features with rest of bank ds, then grabbing best variables: 
banksy = cbind(prc2, bank3) %>% 
            select(young_spenders, old_spenders,total_trans_ct,total_trans_amt,total_revolving_bal, churn)
```

## KNN Model 

```{r}
# specify the model to be used (i.e. KNN, Naive Bayes, decision tree, random forest, bagged trees) and the tuning parameters used



set.seed(504) 

bank_index <- createDataPartition(banksy$churn, p = 0.80, list = FALSE)
train <- banksy[ bank_index, ]
test <- banksy[-bank_index, ]

# example spec for rf
fit <- train(churn ~ .,
             data = train, 
             method = "knn",
             preProcess = c("center","scale"),
             tuneGrid = expand.grid(k = seq(31,41,2)), # best K between 31 and 41 
             metric = "ROC",
             trControl = ctrl)

fit

confusionMatrix(predict(fit, test),factor(test$churn))

myRoc <- roc(test$churn, predict(fit, test, type="prob")[,2])

plot(myRoc)
auc(myRoc)
#.95 AUC 
```

## Downsampling bank data to remove imbalance of yes/no churn: 

```{r}
traindown = downSample(x = train[,-6], y= train$churn) %>% mutate(churn = Class) %>% select(-Class)
traindown %>% group_by(churn) %>% count()
```

## Random Forest Model with downsampling

```{r}
fit <- train(churn ~ .,
             data = traindown, 
             method = "rf",
             tuneLength = 4, 
             metric = "ROC",
             trControl = ctrl)

confusionMatrix(predict(fit, test),factor(test$churn))

myRoc <- roc(test$churn, predict(fit, test, type="prob")[,2])

plot(myRoc)
auc(myRoc) 
# AUC .97
```

## Gradient boosted model with PCA vs only top 5 variables: 

```{r}
#with PCAs "young spenders" and "old spenders" 

fit_gbm1 <- train(churn ~ .,
             data = train, 
             method = "gbm",
             tuneLength = 4, 
             preProcess = c("center","scale"),
             metric = "ROC",
             trControl = ctrl)


confusionMatrix(predict(fit_gbm1, test),factor(test$churn))

myRoc <- roc(test$churn, predict(fit_gbm1, test, type="prob")[,2])

plot(myRoc)
## auc(myRoc)
#kappa = .76, AUC  = .97


#with only top 5 variables 

banksy2 = bank %>% select(total_amt_chng_q4_q1, total_trans_ct, total_trans_amt,total_revolving_bal, total_relationship_count,churn)

bank_index2 <- createDataPartition(banksy2$churn, p = 0.80, list = FALSE)
train2 <- banksy2[ bank_index2, ]
test2 <- banksy2[-bank_index2, ]

fit_gbm2 <- train(churn ~ .,
             data = train2, 
             method = "gbm",
             tuneLength = 4, 
             preProcess = c("center","scale"),
             metric = "ROC",
             trControl = ctrl)

confusionMatrix(predict(fit_gbm2, test2),factor(test2$churn))

myRoc <- roc(test2$churn, predict(fit_gbm2, test2, type="prob")[,2])

plot(myRoc)
## auc(myRoc)
#kappa = .85, AUC .99
```

**Surprisingly, model with 5 non-PCA features performed better than the addition of 2 PCA features.** 

```{r}
# Here are a few lines to inspect your best model. Add some comments about optimal hyperparameters.
print(fit_gbm2)
print(fit_gbm2$bestTune)
```


