# Print the minimum value
print(min_tweet_created)
View(tweets.df)
# Convert tweet_created column to a proper date-time format
tweets.df <- tweets.df %>%
mutate(tweet_created = mdy_hm(tweet_created))
# Remove the "+0000" from tweet_created column
tweets.df <- tweets.df %>%
mutate(tweet_created = str_remove_all(tweet_created, "\\s\\+\\d{4}"))
# Convert tweet_created column to POSIXct format
tweets.df <- tweets.df %>%
mutate(tweet_created = as.POSIXct(tweet_created, format = "%m/%d/%Y %H:%M"))
# Extract the minimum value of tweet_created column
min_tweet_created <- min(tweets.df$tweet_created)
# Print the minimum value
print(min_tweet_created)
# Convert tweet_created column to a proper date-time format
tweets.df <- tweets.df %>%
mutate(tweet_created = mdy_hm(tweet_created))%>%
# Remove zeros.
str_remove_all(pattern = '\\+0000') %>%
# Parse date.
parse_date_time(orders = '%y-%m-%d %H%M%S')
)
# Convert tweet_created column to a proper date-time format
tweets.df <- tweets.df %>%
mutate(tweet_created = mdy_hm(tweet_created))%>%
# Remove zeros.
str_remove_all(pattern = '\\+0000') %>%
# Parse date.
parse_date_time(orders = '%y-%m-%d %H%M%S')
library(hms)
library(twitteR)
library(lubridate)
library(tidytext)
library(tm)
library(wordcloud)
library(igraph)
library(glue)
library(networkD3)
library(rtweet)
library(plyr)
library(stringr)
library(ggplot2)
library(ggeasy)
library(plotly)
library(dplyr)
library(hms)
library(lubridate)
library(magrittr)
library(tidyverse)
library(janeaustenr)
library(widyr)
tweets <- read.csv("https://raw.githubusercontent.com/karolo89/WEBKOE/main/SatisfactionAnalysis/Tweets.csv")
head(tweets)
# extracting 4000 tweets related to global warming topic
n.tweet <- length(tweets)
# convert tweets to a data frame
tweets.df <- as.data.frame(tweets)
tweets.txt <- tweets$getText
# Ignore graphical Parameters to avoid input errors
tweets.txt <- str_replace_all(tweets.txt,"[^[:graph:]]", " ")
## pre-processing text:
clean.text = function(x)
{
# convert to lower case
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
# some other cleaning text
x = gsub('https://','',x)
x = gsub('http://','',x)
x = gsub('[^[:graph:]]', ' ',x)
x = gsub('[[:punct:]]', '', x)
x = gsub('[[:cntrl:]]', '', x)
x = gsub('\\d+', '', x)
x = str_replace_all(x,"[^[:graph:]]", " ")
return(x)
}
cleanText <- clean.text(tweets.txt)
# remove empty results (if any)
idx <- which(cleanText == " ")
cleanText <- cleanText[cleanText != " "]
glimpse(tweets.df)
# Convert tweet_created column to a proper date-time format
tweets.df <- tweets.df %>%
mutate(tweet_created = mdy_hm(tweet_created))%>%
# Remove zeros.
str_remove_all(pattern = '\\+0000') %>%
# Parse date.
parse_date_time(orders = '%y-%m-%d %H%M%S')
tweets.df %<>%
mutate(Created_At_Round = created%>% round(units = 'hours') %>% as.POSIXct())
# Convert tweet_created column to a proper date-time format
tweets.df <- tweets.df %>%
mutate(tweet_created = mdy_hm(tweet_created))%>%
# Remove zeros.
str_remove_all(pattern = '\\+0000') %>%
# Parse date.
parse_date_time(orders = '%y-%m-%d %H%M%S')
library(hms)
library(twitteR)
library(lubridate)
library(tidytext)
library(tm)
library(wordcloud)
library(igraph)
library(glue)
library(networkD3)
library(rtweet)
library(plyr)
library(stringr)
library(ggplot2)
library(ggeasy)
library(plotly)
library(dplyr)
library(hms)
library(lubridate)
library(magrittr)
library(tidyverse)
library(janeaustenr)
library(widyr)
tweets <- read.csv("https://raw.githubusercontent.com/karolo89/WEBKOE/main/SatisfactionAnalysis/Tweets.csv")
head(tweets)
# extracting 4000 tweets related to global warming topic
n.tweet <- length(tweets)
# convert tweets to a data frame
tweets.df <- as.data.frame(tweets)
tweets.txt <- tweets$getText
# Ignore graphical Parameters to avoid input errors
tweets.txt <- str_replace_all(tweets.txt,"[^[:graph:]]", " ")
## pre-processing text:
clean.text = function(x)
{
# convert to lower case
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
# some other cleaning text
x = gsub('https://','',x)
x = gsub('http://','',x)
x = gsub('[^[:graph:]]', ' ',x)
x = gsub('[[:punct:]]', '', x)
x = gsub('[[:cntrl:]]', '', x)
x = gsub('\\d+', '', x)
x = str_replace_all(x,"[^[:graph:]]", " ")
return(x)
}
cleanText <- clean.text(tweets.txt)
# remove empty results (if any)
idx <- which(cleanText == " ")
cleanText <- cleanText[cleanText != " "]
glimpse(tweets.df)
# Convert tweet_created column to a proper date-time format
tweets.df <- tweets.df %>%
mutate(tweet_created = mdy_hm(tweet_created))%>%
# Remove zeros.
str_remove_all(pattern = '\\+0000') %>%
# Parse date.
parse_date_time(orders = '%y-%m-%d %H%M%S')
tweets.df %<>%
mutate(Created_At_Round = created%>% round(units = 'hours') %>% as.POSIXct())
library(hms)
library(twitteR)
library(lubridate)
library(tidytext)
library(tm)
library(wordcloud)
library(igraph)
library(glue)
library(networkD3)
library(rtweet)
library(plyr)
library(stringr)
library(ggplot2)
library(ggeasy)
library(plotly)
library(dplyr)
library(hms)
library(lubridate)
library(magrittr)
library(tidyverse)
library(janeaustenr)
library(widyr)
tweets <- read.csv("https://raw.githubusercontent.com/karolo89/WEBKOE/main/SatisfactionAnalysis/Tweets.csv")
head(tweets)
# extracting 4000 tweets related to global warming topic
n.tweet <- length(tweets)
# convert tweets to a data frame
tweets.df <- as.data.frame(tweets)
tweets.txt <- tweets$getText
# Ignore graphical Parameters to avoid input errors
tweets.txt <- str_replace_all(tweets.txt,"[^[:graph:]]", " ")
## pre-processing text:
clean.text = function(x)
{
# convert to lower case
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
# some other cleaning text
x = gsub('https://','',x)
x = gsub('http://','',x)
x = gsub('[^[:graph:]]', ' ',x)
x = gsub('[[:punct:]]', '', x)
x = gsub('[[:cntrl:]]', '', x)
x = gsub('\\d+', '', x)
x = str_replace_all(x,"[^[:graph:]]", " ")
return(x)
}
cleanText <- clean.text(tweets.txt)
# remove empty results (if any)
idx <- which(cleanText == " ")
cleanText <- cleanText[cleanText != " "]
glimpse(tweets.df)
# extracting 4000 tweets related to global warming topic
n.tweet <- length(tweets)
# convert tweets to a data frame
tweets.df <- as.data.frame(tweets)
tweets.txt <- tweets$getText
# Ignore graphical Parameters to avoid input errors
tweets.txt <- str_replace_all(tweets.txt,"[^[:graph:]]", " ")
## pre-processing text:
clean.text = function(x)
{
# convert to lower case
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
# some other cleaning text
x = gsub('https://','',x)
x = gsub('http://','',x)
x = gsub('[^[:graph:]]', ' ',x)
x = gsub('[[:punct:]]', '', x)
x = gsub('[[:cntrl:]]', '', x)
x = gsub('\\d+', '', x)
x = str_replace_all(x,"[^[:graph:]]", " ")
return(x)
}
cleanText <- clean.text(tweets.txt)
# remove empty results (if any)
idx <- which(cleanText == " ")
cleanText <- cleanText[cleanText != " "]
glimpse(tweets.df)
sum(is.na(tweets))
apply(tweets, 2, function(x) sum(is.na(x)))
tweets <- read.csv("https://raw.githubusercontent.com/karolo89/WEBKOE/main/SatisfactionAnalysis/Tweets.csv")%>%
select(-negativereason_confidence)
head(tweets)
# extracting 4000 tweets related to global warming topic
n.tweet <- length(tweets)
# convert tweets to a data frame
tweets.df <- as.data.frame(tweets)
tweets.txt <- tweets$getText
# Ignore graphical Parameters to avoid input errors
tweets.txt <- str_replace_all(tweets.txt,"[^[:graph:]]", " ")
## pre-processing text:
clean.text = function(x)
{
# convert to lower case
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
# some other cleaning text
x = gsub('https://','',x)
x = gsub('http://','',x)
x = gsub('[^[:graph:]]', ' ',x)
x = gsub('[[:punct:]]', '', x)
x = gsub('[[:cntrl:]]', '', x)
x = gsub('\\d+', '', x)
x = str_replace_all(x,"[^[:graph:]]", " ")
return(x)
}
cleanText <- clean.text(tweets.txt)
# remove empty results (if any)
idx <- which(cleanText == " ")
cleanText <- cleanText[cleanText != " "]
glimpse(tweets.df)
apply(tweets, 2, function(x) sum(is.na(x)))
summary(tweets.df)
tweets.df %<>%
mutate(
created = created %>%
# Remove zeros.
str_remove_all(pattern = '\\+0000') %>%
# Parse date.
parse_date_time(orders = '%y-%m-%d %H%M%S')
)
# Apply the mutate function to update 'created' column
tweets.df <- tweets.df %>%
mutate(
created = tweet_created %>%
# Remove zeros.
str_remove_all(pattern = '\\+0000') %>%
# Parse date.
parse_date_time(orders = '%m/%d/%Y %H:%M')
)
# Create a new column 'Created_At_Round' by rounding 'created' to hours
tweets.df <- tweets.df %>%
mutate(Created_At_Round = round_date(created, unit = "hours") %>% as.POSIXct())
# Extract the minimum value of 'created' column
min_created <- min(tweets.df$created)
# Print the minimum value
print(min_created)
# Print the minimum value
print(min_created)
tweets.df %>% pull(created) %>% max()
plt <- tweets.df %>%
dplyr::count(Created_At_Round) %>%
ggplot(mapping = aes(x = Created_At_Round, y = n)) +
theme_light() +
geom_line() +
xlab(label = 'Date') +
ylab(label = NULL) +
ggtitle(label = 'Number of Tweets per Hour')
plt %>% ggplotly()
## Loading sentiment word lists
positive = scan('resources/twitter_sentiment_analysis/positive-words.txt', what = 'character', comment.char = ';')
## Loading sentiment word lists
install.packages("textdata")
## Loading sentiment word lists
library("textdata")
positive = scan('resources/twitter_sentiment_analysis/positive-words.txt', what = 'character', comment.char = ';')
library("textdata")
# Retrieve the positive and negative word lists from the "textdata" package
positive <- textdata::positive_words()
library(textdata)
# Extract the positive word list
positive <- positive_words()
library(textdata)
# Extract the positive word list
positive <- positive_words(c('upgrade', 'Congrats', 'prizes', 'prize', 'thanks', 'thnx', 'Grt', 'gr8', 'plz', 'trending', 'recovering', 'brainstorm', 'leader'))
library(textdata)
library(OpinionLexicon)
library(textdata)
# Extract the positive word list
positive <- lexicon_bing(c('upgrade', 'Congrats', 'prizes', 'prize', 'thanks', 'thnx', 'Grt', 'gr8', 'plz', 'trending', 'recovering', 'brainstorm', 'leader'))
# Extract the negative word list
negative <- lexicon_bing((c('wtf', 'wait', 'waiting', 'epicfail', 'Fight', 'fighting', 'arrest', 'no', 'not'))
View(positive)
library(textdata)
# Extract the positive word list
positive <- lexicon_bing(c('upgrade', 'Congrats', 'prizes', 'prize', 'thanks', 'thnx', 'Grt', 'gr8', 'plz', 'trending', 'recovering', 'brainstorm', 'leader','wtf', 'wait', 'waiting', 'epicfail', 'Fight', 'fighting', 'arrest', 'no', 'not'))
library(hms)
library(twitteR)
library(lubridate)
library(tidytext)
library(tm)
library(wordcloud)
library(igraph)
library(glue)
library(networkD3)
library(rtweet)
library(plyr)
library(stringr)
library(ggplot2)
library(ggeasy)
library(plotly)
library(dplyr)
library(hms)
library(lubridate)
library(magrittr)
library(tidyverse)
library(janeaustenr)
library(widyr)
tweets <- read.csv("https://raw.githubusercontent.com/karolo89/WEBKOE/main/SatisfactionAnalysis/Tweets.csv")%>%
select(-negativereason_confidence)
head(tweets)
# extracting 4000 tweets related to global warming topic
n.tweet <- length(tweets)
# convert tweets to a data frame
tweets.df <- as.data.frame(tweets)
tweets.txt <- tweets$getText
# Ignore graphical Parameters to avoid input errors
tweets.txt <- str_replace_all(tweets.txt,"[^[:graph:]]", " ")
## pre-processing text:
clean.text = function(x)
{
# convert to lower case
x = tolower(x)
# remove rt
x = gsub("rt", "", x)
# remove at
x = gsub("@\\w+", "", x)
# remove punctuation
x = gsub("[[:punct:]]", "", x)
# remove numbers
x = gsub("[[:digit:]]", "", x)
# remove links http
x = gsub("http\\w+", "", x)
# remove tabs
x = gsub("[ |\t]{2,}", "", x)
# remove blank spaces at the beginning
x = gsub("^ ", "", x)
# remove blank spaces at the end
x = gsub(" $", "", x)
# some other cleaning text
x = gsub('https://','',x)
x = gsub('http://','',x)
x = gsub('[^[:graph:]]', ' ',x)
x = gsub('[[:punct:]]', '', x)
x = gsub('[[:cntrl:]]', '', x)
x = gsub('\\d+', '', x)
x = str_replace_all(x,"[^[:graph:]]", " ")
return(x)
}
cleanText <- clean.text(tweets.txt)
# remove empty results (if any)
idx <- which(cleanText == " ")
cleanText <- cleanText[cleanText != " "]
summary(tweets.df)
# Apply the mutate function to update 'created' column
tweets.df <- tweets.df %>%
mutate(
created = tweet_created %>%
# Remove zeros.
str_remove_all(pattern = '\\+0000') %>%
# Parse date.
parse_date_time(orders = '%m/%d/%Y %H:%M')
)
# Create a new column 'Created_At_Round' by rounding 'created' to hours
tweets.df <- tweets.df %>%
mutate(Created_At_Round = round_date(created, unit = "hours") %>% as.POSIXct())
# Extract the minimum value of 'created' column
min_created <- min(tweets.df$created)
# Print the minimum value
print(min_created)
tweets.df %>% pull(created) %>% max()
plt <- tweets.df %>%
dplyr::count(Created_At_Round) %>%
ggplot(mapping = aes(x = Created_At_Round, y = n)) +
theme_light() +
geom_line() +
xlab(label = 'Date') +
ylab(label = NULL) +
ggtitle(label = 'Number of Tweets per Hour')
plt %>% ggplotly()
library(textdata)
# Extract the positive word list
positive <- lexicon_bing()
# Retrieve the positive and negative word lists from the "textdata" package
positive <- textdata::positive_words()
