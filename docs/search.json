[
  {
    "objectID": "blog/facebook/facebook.html",
    "href": "blog/facebook/facebook.html",
    "title": "Facebook’s Algorithmic Pandora’s Box: Unmasking the Amplification of Fake News and Political Polarization",
    "section": "",
    "text": "As we integrate social media into our daily routines, we must consider the potential impact on the spread of information. Facebook’s algorithm is intended to personalize users’ News Feeds using engagement, relevance, relationships, post type, and recency, but this approach may unintentionally create filter bubbles that propagate false narratives and reinforce confirmation bias. This article delves into the impact of Facebook’s algorithm on the amplification of incorrect information and political polarization and explores the mechanisms behind this concerning phenomenon.\n\nUnderstanding Facebook’s Algorithm\nFacebook’s algorithm heavily relies on user engagement to determine the popularity and relevance of a post. All forms of engagement, including likes, comments, shares, and reactions, are considered to ensure that the content reaches a broader audience, prompting users to interact with posts of interest and increasing the visibility of similar content in the future. The algorithm aims to deliver content that aligns with users’ interests by analyzing past interactions, preferences, and relationships prioritizing engaging formats such as videos or visually captivating posts in the user’s News Feeds. Moreover, the algorithm considers advertising and promoted content, targeting users based on their demographics, interests, and behaviors to present them with relevant ads.\n\n\nThe Evolution of the Algorithm\nFacebook, established by Mark Zuckerberg in 2004 from a Harvard dormitory, quickly gained popularity and became a global sensation with its intuitive design, innovative features, and emphasis on digital connectivity. Throughout its history, Facebook’s algorithm has evolved significantly to meet its user base’s changing needs and challenges. From the introduction of the News Feed in 2006 to the ongoing refinements, the algorithm has aimed to balance personalization, user engagement, and responsible content curation. However, the platform has faced ethical and societal challenges, including privacy and transparency concerns, data security issues, and misinformation.\nThe News Feed’s introduction replaced users’ need to browse friends’ profiles manually. This infinite scroll of content tailored to each user’s interests and connections revolutionized how people consumed information on the platform. The algorithm gained further power in 2009 with the introduction of the Like button, allowing users to express approval or support for posts and photos.\nIn 2011, Facebook shifted its algorithm to prioritize content based on user engagement metrics like likes, comments, and shares. This update aimed to deliver more relevant and engaging content to users, enhancing their overall experience on the platform. By leveraging these metrics, the algorithm became a powerful tool for advertisers to target their desired audience precisely.\nThe algorithm continued to evolve in 2013 with the introduction of Graph Search. This feature enabled users to search for specific content, posts, photos, and people based on their connections and interests. This update improved the search experience and further personalized content recommendations, making it easier for users to discover relevant information.\nIn 2015, Facebook announced an algorithm update that prioritized posts from friends and family over content from brands, publishers, and pages. With growing concerns about the spread of clickbait headlines and fake news, Facebook took action to reduce the visibility of fake news, updating the system to favor posts from trustworthy sources and promoting more accurate and reliable information. This move aimed to address challenges associated with misinformation and enhance the platform’s content quality.\nIn 2018, the company made significant changes to de-emphasize posts created by publishers and brands and prioritize content shared by friends and families. In 2019, the company introduced the “Why am I seeing this post?” feature, allowing users to understand the factors influencing their News Feed content and promoting transparency. Users gained more control over the types of content they see through improved customization options, empowering them to tailor their Facebook experience.\nIn 2020, the algorithm was adjusted to combat false information dissemination, especially during significant events like elections. The platform prioritized authoritative sources, implemented measures to prevent the amplification of misinformation, and worked with third-party fact-checkers to minimize the impact on elections and provide users with reliable and more accurate information.\n\n\nSpread of Misinformation and Political Polarization on Facebook\nThe prevalence of fake news on Facebook is a significant concern encompassing a wide range of problematic content, including imposter news, conspiracy theories, hyperpartisan websites, and “junk news.” The insidious use of clickbait to attract readers and the dissemination of dubious information through bots and trolls, known as “computational propaganda,” only exacerbates the issue. This problem extends beyond news, surrounding cultural commentary and satire, and raises crucial questions about how user engagement and algorithmic prioritization contribute to the spread of misinformation.\nThe influence of user engagement on Facebook’s dissemination of fake news cannot be disregarded. Those who unintentionally like, comment, or share false information only increase their visibility and reach. Facebook’s algorithm can magnify fake news with high engagement, making it visible to a broader audience. Additionally, personalized content based on user preferences and behavior can create echo chambers and filter bubbles, exposing users to misinformation that aligns with their beliefs and interests. Acknowledging that maximizing engagement can lead to polarization is crucial, particularly within networks of like-minded users.\nWhile Facebook has tried to combat the spread of fake news, the issue remains complex. The platform has partnered with third-party organizations for fact-checking, flagging disputed content, and reducing false information’s visibility. However, balancing freedom of expression, user engagement, and the responsibility to combat misinformation is an ongoing challenge.\nRegarding political polarization, experts challenge Facebook CEO Mark Zuckerberg’s denial that the platform fuels divisiveness. Zuckerberg’s assertions have been disputed despite testifying before a U.S. House of Representatives subcommittee in March 2021 and attributing division to external factors, such as the political and media environment. Facebook’s VP for global affairs and communication, Nick Clegg, has similarly claimed there is no evidence to support the notion that social media is a clear driver of polarization. However, the scholarly consensus is that social media platforms like Facebook and Twitter intensify political sectarianism, contrary to the company’s claims. In articles published in Science in October 2020 and Trends in Cognitive Sciences in August 2021, researchers have concluded that social media is a significant facilitator of polarization.\nFacebook’s algorithm limits exposure to cross-cutting links, causing users to connect with others who share the same political beliefs and reducing the likelihood of encountering diverse opinions. While this algorithm aims to enhance user experience, it inadvertently reduces politically diverse content by around 5% for conservatives and 8% for liberals. Typically, individuals have five politically like-minded friends on Facebook for every friend from the opposing side.\nAlthough Silicon Valley is not solely responsible for these issues, a study conducted in March 2020 showed that taking a break from Facebook for a month significantly reduced polarization of views on policy issues among participants. The study published by the American Economic Review suggested that exposure to political content on social media tends to provoke heightened emotions, anger toward the opposing side, and more robust views on specific issues, which contradicts Facebook’s narrative.\n\n\nAlgorithmic Failure’s High Costs\nThe spread of false information through social media is a grave issue that can trigger severe short, and long-lasting consequences for individuals, society, and organizations. Acknowledging that misinformation can lead to social division, offer misleading medical advice, and promote fraudulent schemes that erode trust and pose serious health risks is paramount. The fact that fake news purporting that pure alcohol (methanol) could cure COVID-19 led to approximately 800 deaths in Iran, with an additional 5,876 people hospitalized due to methanol poisoning, is unacceptable. This tragic example underscores the precarious nature of misinformation for individuals and society.\nMoreover, the Cambridge Analytica and Facebook case explicitly demonstrates the cost of false information for businesses. Cambridge Analytica was accused of manipulating political outcomes through Facebook data, with some executives proposing unethical practices such as bribery and fake news. These actions ultimately led to the closure of the company in 2018 and Facebook paying a record $5 billion settlement to the Federal Trade Commission (FTC) and another $100 million to settle allegations of misuse of user data with the U.S. Securities and Exchange Commission.\nSuch actions have far-reaching consequences beyond these companies, as non-profit organizations and advocacy groups may struggle to communicate their messages effectively when fake news dominates the conversation. This effect in non-profits was demonstrated in 2014 when anti-abortion activists secretly recorded meetings and staff lunches at Planned Parenthood, a non-profit organization that provides reproductive healthcare options. The activists then edited and disseminated the footage through social media, creating a viral sensation. The hashtag #defundPP was shared over 1.3 million times within a few months, subjecting Planned Parenthood to intense scrutiny and calls for defunding.\nAnother example of the algorithm failure cost is the aftermath of the January 6th attack on the U.S. Capitol, which has been financially and politically disastrous. It’s been widely reported that Facebook groups played a significant role in fueling political polarization and spreading false narratives that contributed to the siege. These groups saw a massive surge in posts aimed at undermining the legitimacy of Joe Biden’s victory, totaling over 650,000 between Election Day and the attack. The costs of the damage caused by this event have already exceeded $30 million and are expected to continue rising. These expenses include implementing enhanced security measures, repairing damaged infrastructure, and deploying additional law enforcement resources. However, the financial implications are only a tiny part of the more significant problem. The attack on the Capitol represents a severe assault on American democratic principles and substantially threatens the peaceful transfer of power.\n\n\nData Ethics and Facebook’s Responsibility\nFacebook’s handling of user data has sparked significant concerns, mainly due to the infamous Cambridge Analytica scandal in 2018. The scandal highlighted Facebook’s unethical practices, including weak consent mechanisms, inadequate data protection, user profiling, targeted advertising, and a lack of transparency. The scandal revealed that personal data from around 87 million Facebook users had been collected and exploited through a third-party app called “This Is Your Digital Life.” This breach violated users’ privacy rights and exposed their data to misuse. Cambridge Analytica used this unauthorized data to create psychographic profiles and targeted political advertising during the 2016 U.S. presidential election and other campaigns worldwide.\nAs of today, misleading, and extremist content disguised as journalism and facts continue to dominate the most shared posts on the platform, making Facebook a prominent source of misinformation. Likewise, this criticism is further accentuated by the legal and regulatory landscape, allowing the company to prioritize profit maximization and shareholder value over proactive ethical considerations.\nRegarding algorithm responsibility, Article 230 of the Communications Decency Act of 1996 is a significant factor. Originally intended to safeguard freedom of expression online, this legislation has shielded platforms like Facebook from active duty in regulating their hosted content. However, it has been exploited by social media startups and big tech companies, enabling harmful content to thrive. The article stipulates that interactive computer service providers or users should not be held accountable for the information provided by content creators.\nFacebook promotes a “shared responsibility” approach and emphasizes community cooperation. The company argues that it cannot be the sole arbiter of truth and instead believes in empowering individuals to have a voice. They encourage users to report false posts, avoid sharing or posting misleading articles, and flag spammy content, placing some responsibility on the user community.\nIn the face of these concerns, it is evident that Facebook needs to address its data ethics practices more effectively and take substantial steps to combat misinformation while prioritizing transparency, user privacy, and responsible decision-making.\n\n\nNext Steps: Collaborative Solutions, Section 230 Reform, Proactive Measures by Social Media Companies, and Digital Citizenship\nSeveral next steps are crucial to address the challenges in the online environment: Section 230 Reform: Reforming Section 230 of the Communications Decency Act of 1996 is necessary to encourage platforms to exercise their duty of care responsibly without unnecessary regulatory burdens. This reform could motivate media outlets to address potential risks proactively and create a safer and more responsible online environment. The protections outlined in Section 230 were crafted over 25 years ago, a time of limited technological capabilities and naive technological optimism. In light of the significant changes since then, these protections are outdated and need reconsideration and updating by the legislators.\nCollaboration for Comprehensive Solutions: Governments, tech companies, civil society organizations, and individuals must join forces to develop comprehensive solutions. Collaborative efforts can include sharing expertise, resources, and best practices to effectively address the spread of fake news. This collective approach ensures a more holistic and impactful response.\nStrengthening Media Literacy and Promoting Digital Citizenship: Investing in media literacy programs is crucial to empower individuals with the critical skills to navigate the digital landscape. Education initiatives should focus on fostering media literacy, promoting digital citizenship, and cultivating necessary thinking skills. By equipping people with the tools to discern reliable information from misinformation, they become active participants in combating the spread of fake news.\nLong-term Algorithmic Adjustments: Social media companies, such as Facebook, must acknowledge their contribution to political polarization and take proactive measures to reduce the prevalence of divisive content. Making lasting changes to their algorithms is a practical step in curbing heated debates and reducing the amplification of polarizing narratives. Continuously refining automated systems and moderation policies is necessary to balance fostering free expression and mitigating the unintended suppression of legitimate political discourse.\nEmbrace transparency: Social media platforms must prioritize transparency by disclosing how their algorithms work, such as how content is rated, suggested, and removed. In addition, these platforms should continuously enhance their data protection measures, participate in independent audits, and foster a responsible environment for sharing information.\n\n\nConclusion\nThe potential contribution of Facebook’s algorithm to spreading fake news and political polarization is a significant concern. Despite its original purpose to personalize users’ News Feeds, the algorithm could create filter bubbles reinforcing false narratives and confirmation bias. Imposter news, conspiracy theories, hyperpartisan websites, and “junk news” are just a few examples of inappropriate content in the fake news sphere on Facebook. It’s worth noting that user engagement is a significant factor in disseminating phony information, and personalized content based on user preferences and behavior can create echo chambers and filter bubbles that expose users to misinformation that aligns with their beliefs and interests.\nIt’s essential to recognize that maximizing engagement can lead to polarization, particularly within networks of like-minded users. Facebook has tried to counter the spread of fake news, but balancing freedom of expression, user engagement, and the responsibility to combat misinformation is challenging. To develop comprehensive solutions, we must collaborate, promote media literacy, adjust algorithms, and embrace transparency.\n\n\nReferences\nAllcott, H., Braghieri, L., Eichmeyer, S., & Gentzkow, M. (2020). [The Welfare Effects of Social Media](https://doi.org/10.1257/aer.20190468). American Economic Review, 110(3), 629-676.\nBarrett, P., et al. (2021, September 27). [How Tech Platforms Fuel U.S. Political Polarization and What Government Can Do about It](https://www.brookings.edu/blog/techtank/2021/09/27/how-tech-platforms-fuel-u-s-political-polarization-and-what-government-can-do-about-it/). Brookings.\nBleiberg, J., & West, D. M. (2015, May 13). [Political Polarization on Facebook](https://www.brookings.edu/blog/techtank/2015/05/13/political-polarization-on-facebook/). Brookings.\nCochrane, E., & Broadwater, L. (2021, February 24). [Capitol Riot Costs Will Exceed $30 Million, Official Tells Congress](https://www.nytimes.com/2021/02/24/us/politics/capitol-riot-damage.html). The New York Times.\nDepartment of Justice. (2020, June 3). [Department of Justice’s Review of Section 230 of the Communications Decency Act of 1996](https://www.justice.gov/archives/ag/department-justice-s-review-section-230-communications-decency-act-1996).\nFinkel, E. J., et al. (2020). [Political Sectarianism in America](https://doi.org/10.1126/science.abe1715). Science, 370(6516), 533-536.\nHarvard Kennedy School. (n.d.). [Research Note: The Scale of Facebook’s Problem Depends upon How ‘Fake News’ Is Classified](https://misinforeview.hks.harvard.edu/article/research-note-the-scale-of-facebooks-problem-depends-upon-how-fake-news-is-classified/).\nHarvard Law Today. (n.d.). [The Algorithm Has Primacy over Media … over Each of Us, and It Controls What We Do](https://hls.harvard.edu/today/the-algorithm-has-primacy-over-media-over-each-of-us-and-it-controls-what-we-do/).\nMadrigal, A. C. (2017, October 12). [What Facebook Did to American Democracy](https://www.theatlantic.com/technology/archive/2017/10/what-facebook-did/542502/). The Atlantic.\nMIT Technology Review. (2021, March 11). [How Facebook Got Addicted to Spreading Misinformation](https://www.technologyreview.com/2021/03/11/1020600/facebook-responsible-ai-misinformation/).\nMIT Technology Review. (2021, November 20). [How Facebook and Google Fund Global Misinformation](https://www.technologyreview.com/2021/11/20/1039076/facebook-google-disinformation-clickbait/).\nMosseri, A. (2017, April 7). [Working to Stop Misinformation and False News](https://www.facebook.com/formedia/blog/working-to-stop-misinformation-and-false-news). Facebook Media.\nRocha, Y. M., et al. (2021). [The Impact of Fake News on Social Media and Its Influence on Health during the COVID-19 Pandemic: A Systematic Review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8502082/). Journal of Public Health, 1(10).\nSilverman, C. (2016, November 16). [This Analysis Shows How Viral Fake Election News Stories Outperformed Real News on Facebook](https://www.buzzfeednews.com/article/craigsilverman/viral-fake-election-news-outperformed-real-news-on-facebook). BuzzFeed News.\nSmith, M. D., & Van Alstyne, M. (2021, August 12). [It’s Time to Update Section 230](https://hbr.org/2021/08/its-time-to-update-section-230). Harvard Business Review.\nStars, S., Stortstrom, R., Lagrace, L., & King, N. (2018). [Overview of Fake News: For Non-Profit Organizations](https://stars.library.ucf.edu/cgi/viewcontent.cgi?article=1001&context=publicsectormedialiteracy).\nThe Guardian. (2016, November 10). [Facebook’s Failure: Did Fake News and Polarized Politics Get Trump Elected?](https://www.theguardian.com/technology/2016/nov/10/facebook-fake-news-election-conspiracy-theories).\nThe New York Times. (2015, May 8). [Facebook Use Polarizing? Site Begs to Differ](https://www.nytimes.com/2015/05/08/technology/facebook-study-disputes-theory-of-political-polarization-among-users.html).\nThe New York Times. (2020, May 28). [Facebook and Its Secret Policies](https://www.nytimes.com/2020/05/28/technology/facebook-polarization.html).\nVan Bavel, J. J., et al. (2021). [How Social Media Shapes Polarization](https://doi.org/10.1016/j.tics.2021.07.013). Trends in Cognitive Sciences, 25(11).\nFacebook AI. (n.d.). [How We’re Using Fairness Flow to Help Build AI That Works Better for Everyone](https://ai.facebook.com/blog/how-were-using-fairness-flow-to-help-build-ai-that-works-better-for-everyone)."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Facebook’s Algorithmic Pandora’s Box: Unmasking the Amplification of Fake News and Political Polarization\n\n\n\n\n\n\n\nData Journalism\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOrozco Karol M.\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\nCase Analysis: Theranos: The Unicorn That Wasn’t, By Joseph B. Fuller and John Masko\n\n\n\n\n\n\n\nData Journalism\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOrozco Karol M.\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello! It’s nice to meet you",
    "section": "",
    "text": "About Me\nI help to create and sustain communities, programs, and solutions that enhance the use of responsible data science and business practices for equitable social good.\nMy name is Karol Orozco; I live in Portland, OR, with my husband, two kids, and my dog Lucy. I am originally from Barranquilla, Colombia, a city located in the northern region of Colombia on the Caribbean coast. It is the fourth-largest city in the country and an important economic and cultural center in the Caribbean region. Barranquilla is known for its vibrant culture and festive atmosphere, exemplified by its famous Carnival of Barranquilla.\n\n\nBehind the Scenes\nlibrary(leaflet)\nleaflet() %>%\n  addTiles() %>%  # Add default OpenStreetMap map tiles\n  addMarkers(lng=-74.77028, lat=10.98611, popup=\"The birthplace of R\")\n\n\n\n\n\n\nAs a graduated engineer, I have acquired extensive experience and problem–solving expertise in data wrangling, statistical analysis, and model development. Since effective communication of data is of paramount importance, I have subsequently developed strong skills in data visualization and business knowledge. I hold an MBA from Willamette University and currently pursuing a second master’s in Data Science.\nI have experience working in the public and private sectors helping organizations to embrace Shared Value by positively impacting society and the environment: The sweet spot where profit meets purpose."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "blog/Theranos/theranos.html",
    "href": "blog/Theranos/theranos.html",
    "title": "Case Analysis: Theranos: The Unicorn That Wasn’t, By Joseph B. Fuller and John Masko",
    "section": "",
    "text": "Situation Analysis\nTheranos was a privately-owned biotech start-up based in Palo Alto, California, United States. It was founded in 2005 by Elizabeth Holmes, a 19-year-old chemical engineering student who dropped out of Stanford University with aspirations of revolutionizing the healthcare industry through technological innovations in blood testing and becoming a billionaire. At its peak, the company was valued at $9 billion, but it eventually dissolved in September 2018 with a value below zero.\nIn 2013, Theranos established its first blood collection sites in Palo Alto and two locations in Phoenix, Arizona. The company employed approximately 700-900 staff, including Sunny Balwani as the president and COO. Its board comprised high-profile individuals with strong connections to political figures in the United States. Quest Diagnostics and LabCorp, which held significant market share, posed as major competitors for Theranos. Nevertheless, the start-up managed to form powerful alliances with renowned pharmaceutical entities such as Walgreens, CVS, Safeway, and the U.S. Department of Defense (DOD).\nTheranos aimed to develop a prototype that would revolutionize blood testing by eliminating the need for needles, utilizing smaller blood samples, providing rapid results, and enabling testing at home. However, the company encountered various setbacks in realizing this vision. The initial design, a medical patch for diagnosing and treating medical conditions, proved unsuccessful, leading to the development of other failed inventions such as the cartridge-and-reader blood analyzer called the Edison, and later, the MiniLab.\nTheranos operated within an industry where most companies exhibited characteristics of traditional organizations and heavily relied on government support, technological advancements, and socio-economic conditions. According to the World Health Organization (WHO), the healthcare industry in the U.S. is one of the largest and most complex. In 2015, the country spent over $3.2 trillion on healthcare, primarily funded through public funds and various insurance coverage options, including individual and private employer-based plans.\nMoreover, the healthcare industry faced stringent regulations, making it the most highly regulated sector in the U.S. The Department of Health and Human Services (DHHS) and agencies like the Centers for Medicare & Medicaid Services and the Food and Drug Administration (FDA) played crucial roles in monitoring and regulating the industry. Failure to comply with government regulations and agency requirements could result in civil and criminal cases, financial penalties, exclusion from healthcare programs, and potential prohibitions or restrictions on services, all of which would significantly harm the company’s reputation and potentially lead to its dissolution.\nThe healthcare industry was subject to technological advancements and the introduction of new products. Strong competition existed, making intellectual property protection and partnerships crucial for the success of the innovation business model. The ability to develop, acquire, or license new and improved technologies would be key determinants of a company’s success. In the absence of such advancements or in the event of adverse effects, the company’s testing methods could become obsolete. Therefore, validation of the scientific base, expertise in clinical testing, manufacturing, risk reduction, and acquisition of necessary resources and approvals were essential for developing and advancing their science.\nThese organizations were also affected by socio-economic factors that influenced health disparities and people’s spending behavior. Factors such as inflation, unemployment, interest rates, salary inequality, age, marital status, food and housing security, and poverty rates played significant roles. Individuals facing unemployment were less likely to afford medical costs, including emergency visits or routine lab tests. Consequently, the price of services or products often served as a critical factor when selecting a healthcare provider.\n\n\nANALYSIS\nTo analyze Theranos, I will employ the Nadler-Tushman Congruence Model, which offers a comprehensive framework for evaluating the organization’s performance. This model considers essential elements including strategy, critical tasks, structure and processes, people, and culture. By examining these interconnected aspects, we can gain a holistic understanding of the company’s operations and their alignment.\n\nStrategy:\nThe article does not explicitly mention Theranos’ operational objectives, strategy, values, and mission. However, it can be inferred that the organization aimed to prioritize innovation and maximize shareholder value. Theranos Inc utilized the blue ocean strategy to explore untapped market spaces, where competition was limited or non-existent. Instead of engaging in head-to-head battles in existing competitive markets, the company aimed to create new demand and establish itself as a pioneer in innovative healthcare solutions.\n\n\nStructure and Processes:\nAccording to the Fuller and Maski article, Theranos had a hierarchical structure with departments organized based on their respective functions, resulting in high work specialization. The organization comprised a Board of Directors, a CEO, a COO, a human resources department, a laboratory, and an engineering unit. These units operated as silos, and in some cases, there was a sense of rivalry among them. The article lacks information regarding the size of each department and the span of control.\nThe authors also suggest the existence of weak linking mechanisms within the organization due to Holmes’ obsession with maintaining tight control over information flow and the company’s narrative. Decision-making was centralized and top-down, with Holmes being the primary decision-maker. Communication channels like instant messaging were prohibited, and emails and computer programs were compulsively monitored, leading to reports containing misleading information.\nRegarding alignment mechanisms at Theranos, the article does not provide details about the systems the company used to measure performance, manage talent, or provide rewards and incentives at the individual and group levels. However, based on the article, there seemed to be a policy of dismissal or marginalization if employees failed to meet Holmes’ expectations, which included following orders, working long hours, and demonstrating critical abilities and discretion. Additionally, the selection process seemed to rely on the leader’s social and family connections, the reputation of employees, and their susceptibility to manipulation.\n\n\nCulture and Leadership:\nThe culture at Theranos was characterized by secrecy, fear, and control. Psychological safety was absent, and employees tended to work in isolation, avoiding eye contact and distrusting others. They feared retaliation from Holmes and Balwani, who were also involved in a secret romantic relationship. The leadership styles of both individuals were authoritarian, abusive, and micromanaging. Holmes used a baritone voice and maintained an unwavering gaze that conveyed a sense of intensity.\nEmployees faced humiliation and threats of legal action if they disobeyed orders or expressed disagreement with the company’s philosophy. The culture also exhibited hypersensitivity towards intellectual property and a paranoia regarding information leakage, necessitating the signing of confidentiality and nondisclosure agreements.\nIt is worth mentioning that Theranos was situated in Silicon Valley, a place known for slogans like “Fake it until you make it” and “Fail Fast.” The culture at Theranos was influenced by Apple and its CEO, with Holmes being particularly obsessed with Steve Jobs and his illusory effect.\n\n\nCritical Task:\nWhile the article does not explicitly outline Theranos’ workflow, critical tasks within the organization were identified in the engineering and laboratory departments. The engineering department was responsible for developing a compact product and ensuring the interaction of all elements as expected. The laboratory department focused on creating the necessary chemical reactions for analyzing blood samples. These tasks were complex, and each unit operated independently.\n\n\nPeople:\nThe leaders and members of the Board of Directors lacked formal scientific training and had no prior experience in biotechnology or healthcare, with the exception of two physician Board members. Most Board members were older men with backgrounds in politics and the military, and they had close connections to the CEO. The workforce consisted of renowned scientists and engineers, interns, recent graduates, and immigrants holding H1-B visas.\nAfter analyzing the elements of the Congruence model, it can be concluded that there was a lack of congruence among them. If Theranos’ products had worked and been certified, the strategy could have aligned with a blue ocean approach, revolutionizing the market through innovation.\n\n\n\nRecommendations\nTerminate Elizabeth Holmes and Sunny Balwani: The Board of Directors should promptly develop a comprehensive termination plan that includes clear justifications for this decision, performance evaluations, and the necessary skills and knowledge required in new leadership. A communication plan should be established to inform all stakeholders impacted by this change, ensuring clarity and authoritative communication. The Board should also identify an interim CEO for the transition period and define the legal framework for the dismissal process. Following this transition, a robust hiring process should be implemented to select a new leader, with clearly communicated expectations and a formal performance review process.\nStrengthen Linking Mechanisms: The new leadership team should prioritize the establishment of effective linking mechanisms within the organization. This can be achieved by creating liaison roles and forming cross-functional teams consisting of at least two members from each department. Regular meetings of these teams should be conducted to address specific problems and generate innovative ideas for new technologies. Utilizing technology such as enterprise resource planning (ERP), chat platforms, and other collaboration tools will facilitate information sharing and improve interdepartmental visibility.\nFoster a Culture of Transparency and Inclusion: Leaders must actively work to earn the trust of employees and create a culture of transparency and inclusion. This can be accomplished by providing consistent and honest communication, delegating responsibilities, and promoting a participatory and inclusive decision-making process. Creating spaces for listening and understanding within the organization is essential. Initiatives such as retreats, focus groups, Q&A sessions, employee surveys, and incentivizing creativity and teamwork can contribute to fostering an environment of trust and open communication.\nHalt Product Commercialization: Theranos should immediately cease the marketing of its products, recall those already on the market, and develop a comprehensive marketing plan. Establishing a dedicated control design unit with cross-functional teams will ensure strict oversight of facilities, equipment, design, and manufacturing processes. Prior to relaunching the product, obtaining approvals from relevant regulatory entities is crucial. A soft launch strategy should be adopted, accompanied by continuous monitoring to ensure compliance with industry requirements. Analytical and assessment tools, such as The Congruence Model, Process Analytical Technology (PAT), Design of Experiments, Risk Assessments, and Corrective and Preventive Actions (CAPA), should be utilized to enhance product quality and regulatory compliance.\nBy implementing these recommendations, Theranos can begin to address its performance and reputation challenges, foster a more transparent and inclusive culture, and establish a solid foundation for future growth and compliance with industry standards."
  }
]