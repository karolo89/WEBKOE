---
title: "The Sky's the Limit: Unveiling the Untold Stories of US Airline Woes on Twitter!"
author: "Orozco Karol M."
image: airplane.jpg
date: "4-20-2023"
format:
  html:
    toc: true
    toc-location: right
    html-math-method: katex
    page-layout: full
execute: 
  warning: false
  message: false
categories: [R, Sentiment Analysis, Brand]
---

## Introduction

Welcome to this sentiment analysis project in R, where I delve into the realm of major U.S. airlines and uncover the sentiments expressed by Twitter users regarding their experiences. By analyzing data scraped from February 2015, I aim to gain insights into the problems faced by these airlines through the eyes of their customers.

The dataset used in this analysis originated from Crowdflower's Data for Everyone library. It is available in both CSV file and SQLite database formats, providing flexibility for your preferred data exploration methods. The transformation code used to format the data can be found on GitHub, ensuring transparency and reproducibility.

Within this dataset, you will find a comprehensive collection of tweets categorized based on their sentiment: positive, negative, or neutral. Additionally, negative tweets have been further classified based on the reasons provided, ranging from "late flight" to "rude service." This granular categorization enables us to pinpoint specific problem areas faced by these airlines.

Join me on this journey as we uncover the untold stories behind each major U.S. airline, unraveling the sentiments expressed by Twitter users and shedding light on the challenges faced by the industry.

## Approach and Methodology

#### Load the required packages and data

```{r, warning=FALSE, message=FALSE}

library(hms)
library(twitteR)
library(lubridate) 
library(tidytext)
library(tm)
library(wordcloud)
library(igraph)
library(glue)
library(networkD3)
library(rtweet)
library(plyr)
library(stringr)
library(ggplot2)
library(ggeasy)
library(plotly)
library(dplyr)  
library(hms)
library(lubridate) 
library(magrittr)
library(tidyverse)
library(janeaustenr)
library(widyr)
library(RColorBrewer)

```

```{r}
tweets <- read.csv("https://raw.githubusercontent.com/karolo89/WEBKOE/main/SatisfactionAnalysis/Tweets.csv")%>%
  select(-negativereason_confidence)

head(tweets)

str(tweets)
```

## Data Analysis

### Extracting Tweets

```{r}
library(stringr)

tweets.df$text <- str_replace_all(tweets.df$text, "[^[:graph:]]", " ") %>%
  gsub("https://", "", .) %>% 
  gsub("rt", "", .)%>%
  gsub("@\\w+", "", .)%>%
  gsub("http://", "", .) %>%
  gsub("[[:digit:]]", "", .)%>%
  gsub("[[:punct:]]", "", .)%>%
  gsub("[ |\t]{2,}", "", .)%>%
  gsub("[^[:graph:]]", " ", .) %>%
  gsub("[[:punct:]]", "", .) %>%
  gsub("[[:cntrl:]]", "", .) %>%
  gsub(" $", "", .)%>%
  gsub("\\d+", "", .) %>%
  str_replace_all(., "[^[:graph:]]", " ") %>%
  tolower()

cleanText <- tweets.df$text
# remove empty results (if any)
cleanText <- cleanText[cleanText != " "]

head(tweets.df)



```

### Frequency of Tweets

```{r}
# Apply the mutate function to update 'created' column
tweets.df <- tweets.df %>% 
  mutate(
    created = tweet_created %>% 
      # Remove zeros.
      str_remove_all(pattern = '\\+0000') %>%
      # Parse date.
      parse_date_time(orders = '%m/%d/%Y %H:%M')
  )

# Create a new column 'Created_At_Round' by rounding 'created' to hours
tweets.df <- tweets.df %>% 
  mutate(Created_At_Round = round_date(created, unit = "hours") %>% as.POSIXct())

# Extract the minimum value of 'created' column
min_created <- min(tweets.df$created)

# Print the minimum value
print(min_created)


tweets.df %>% pull(created) %>% max()

```

```{r}
plt <- tweets.df %>% 
  dplyr::count(Created_At_Round) %>% 
  ggplot(mapping = aes(x = Created_At_Round, y = n)) +
  geom_line(color = "blue") +
  labs(x = "Date", y = "Number of Tweets", title = "Number of Tweets per Hour") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

ggplotly(plt)
```

### Loading sentiment word lists

```{r}
library(textdata)

# Extract the positive word list
words <- lexicon_bing()

positive =words %>% filter(sentiment == "positive")

negative = words %>% filter(sentiment == "negative")

pos.words = c(lexicon_bing,'upgrade','Congrats','prizes','prize','thanks','thnx',         'Grt','gr8','plz','trending','recovering','brainstorm','leader')
neg.words = c(negative,'wtf','wait','waiting','epicfail','Fight','fighting','arrest','no','not')

```

### Sentiment scoring function

```{r}
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
  require(plyr)
  require(stringr)
  
  # we are giving vector of sentences as input. 
  # plyr will handle a list or a vector as an "l" for us
  # we want a simple array of scores back, so we use "l" + "a" + "ply" = laply:
  scores = laply(sentences, function(sentence, pos.words, neg.words) {
    
    # clean up sentences with R's regex-driven global substitute, gsub() function:
    sentence = gsub('https://','',sentence)
    sentence = gsub('http://','',sentence)
    sentence = gsub('[^[:graph:]]', ' ',sentence)
    sentence = gsub('[[:punct:]]', '', sentence)
    sentence = gsub('[[:cntrl:]]', '', sentence)
    sentence = gsub('\\d+', '', sentence)
    sentence = str_replace_all(sentence,"[^[:graph:]]", " ")
    # and convert to lower case:
    sentence = tolower(sentence)
    
    # split into words. str_split is in the stringr package
    word.list = str_split(sentence, '\\s+')
    # sometimes a list() is one level of hierarchy too much
    words = unlist(word.list)
    
    # compare our words to the dictionaries of positive & negative terms
    pos.matches = match(words, pos.words)
    neg.matches = match(words, neg.words)
    
    # match() returns the position of the matched term or NA
    # we just want a TRUE/FALSE:
    pos.matches = !is.na(pos.matches)
    neg.matches = !is.na(neg.matches)
    
    # TRUE/FALSE will be treated as 1/0 by sum():
    score = sum(pos.matches) - sum(neg.matches)
    
    return(score)
  }, pos.words, neg.words, .progress=.progress )
  
  scores.df = data.frame(score=scores, text=sentences)
  return(scores.df)
}
```

### Calculating the sentiment score

```{r}
analysis <- score.sentiment(cleanText, pos.words, neg.words)
# Sentiment score frequency table
score_table <- as.data.frame(table(analysis$score))
colnames(score_table) <- c("Sentiment Score", "Frequency")

score_table
```

### Histogram of sentiment scores

```{r}
analysis %>%
  ggplot(aes(x = score)) +
 geom_histogram(binwidth = 1, fill = "#2171b5") +
  labs(x = "Sentiment Score", y = "Frequency") +
  ggtitle("Distribution of Sentiment Scores of the Tweets") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank()) +
  easy_center_title()

ggsave("plot.png", width = 8, height = 6, dpi = 300)  # Save the plot as an image file
```

### Barplot of sentiment type

```{r}
neutral <- length(which(analysis$score == 0))
positive <- length(which(analysis$score > 0))
negative <- length(which(analysis$score < 0))

Sentiment <- c("Positive", "Neutral", "Negative")
Count <- c(positive, neutral, negative)
output <- data.frame(Sentiment, Count)
output$Sentiment <- factor(output$Sentiment, levels = Sentiment)

# Define custom color palette for business report
business_colors <- c("#0072B2", "#009E73", "#D55E00")

ggplot(output, aes(x = Sentiment, y = Count, fill = Sentiment)) +
  geom_bar(stat = "identity") +
  labs(x = "Sentiment", y = "Count", fill = "Sentiment") +
  ggtitle("Distribution of Sentiment") +
  scale_fill_manual(values = business_colors) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "none",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank()
  )

```

### Wordcloud

```{r}
library(RColorBrewer)

# Create a text corpus
text_corpus <- Corpus(VectorSource(cleanText))
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
text_corpus <- tm_map(text_corpus, removeWords, stopwords("english"))
text_corpus <- tm_map(text_corpus, removeWords, c("global", "globalwarming"))

# Create a term-document matrix
tdm <- TermDocumentMatrix(text_corpus)
tdm <- as.matrix(tdm)
tdm <- sort(rowSums(tdm), decreasing = TRUE)
tdm <- data.frame(word = names(tdm), freq = tdm)

set.seed(123)
```

### Word Frequency plot

```{r}

ggplot(tdm[1:20,], aes(x = reorder(word, freq), y = freq, fill = freq)) +
  geom_bar(stat = "identity") +
  xlab("Terms") +
  ylab("Count") +
  coord_flip() +
  theme(axis.text = element_text(size = 7)) +
  ggtitle("Most Common Word Frequency Plot") +
  easy_center_title() +
  theme_minimal() +
  scale_fill_gradient(low = "#E5F5E0", high = "#31A354")

```

## Network Analysis

### Bigram analysis and Network definition

Bigram counts pairwise occurrences of words which appear together in the text.

```{r}
#bigram
bi.gram.words <- tweets.df %>% 
  unnest_tokens(
    input = text, 
    output = bigram, 
    token = 'ngrams', 
    n = 2
  ) %>% 
  filter(! is.na(bigram))

bi.gram.words %>% 
  select(bigram) %>% 
  head(10)
```

```{r}
extra.stop.words <- c('https')
stopwords.df <- tibble(
  word = c(stopwords(kind = 'es'),
           stopwords(kind = 'en'),
           extra.stop.words)
)
```

Next, we filter for stop words and remove white spaces.

```{r}
bi.gram.words %<>% 
  separate(col = bigram, into = c('word1', 'word2'), sep = ' ') %>% 
  filter(! word1 %in% stopwords.df$word) %>% 
  filter(! word2 %in% stopwords.df$word) %>% 
  filter(! is.na(word1)) %>% 
  filter(! is.na(word2)) 
```

Finally, we group and count by bigram.

```{r}
bi.gram.count <- bi.gram.words %>% 
  dplyr::count(word1, word2, sort = TRUE) %>% 
  dplyr::rename(weight = n)

bi.gram.count %>% head()
```

Let us plot the distribution of the weightvalues:

```{r}
## Note that it is very skewed, for visualization purposes it might be a good idea to perform a transformation, eg log transform:

bi.gram.count %>% 
  mutate(weight = log(weight + 1)) %>% 
  ggplot(mapping = aes(x = weight)) +
  theme_light() +
  geom_histogram(fill = "#2171b5") +
  labs(title = "Bigram log-Weight Distribution")+
  theme_minimal()
```

### Network visualization

```{r}
threshold <- 50

# For visualization purposes we scale by a global factor. 
ScaleWeight <- function(x, lambda) {
  x / lambda
}

network <-  bi.gram.count %>%
  filter(weight > threshold) %>%
  mutate(weight = ScaleWeight(x = weight, lambda = 2E3)) %>% 
  graph_from_data_frame(directed = FALSE)

plot(
  network, 
  vertex.size = 1,
  vertex.label.color = 'black', 
  vertex.label.cex = 0.7, 
  vertex.label.dist = 1,
  edge.color = 'gray', 
  main = 'Bigram Count Network', 
  sub = glue('Weight Threshold: {threshold}'), 
  alpha = 50
)
```

We can go a step further and make our visualization more dynamic using the networkD3 library.

```{r}
threshold <- 50

network <-  bi.gram.count %>%
  filter(weight > threshold) %>%
  graph_from_data_frame(directed = FALSE)

# Store the degree.
V(network)$degree <- strength(graph = network)
# Compute the weight shares.
E(network)$width <- E(network)$weight/max(E(network)$weight)

# Create networkD3 object.
network.D3 <- igraph_to_networkD3(g = network)
# Define node size.
network.D3$nodes %<>% mutate(Degree = (1E-2)*V(network)$degree)
# Define color group
network.D3$nodes %<>% mutate(Group = 1)
# Define edges width. 
network.D3$links$Width <- 10*E(network)$width

forceNetwork(
  Links = network.D3$links, 
  Nodes = network.D3$nodes, 
  Source = 'source', 
  Target = 'target',
  NodeID = 'name',
  Group = 'Group', 
  opacity = 0.9,
  Value = 'Width',
  Nodesize = 'Degree', 
  # We input a JavaScript function.
  linkWidth = JS("function(d) { return Math.sqrt(d.value); }"), 
  fontSize = 12,
  zoom = TRUE, 
  opacityNoHover = 1
)
```

### References:

Bing Liu, Minqing Hu and Junsheng Cheng. "Opinion Observer: Analyzing and Comparing Opinions on the Web." Proceedings of the 14th International World Wide Web conference (WWW-2005), May 10-14, 2005, Chiba, Japan.
