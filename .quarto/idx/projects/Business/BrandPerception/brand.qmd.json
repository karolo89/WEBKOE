{"title":"The Sky's the Limit: Unveiling the Untold Stories of US Airline Woes on Twitter!","markdown":{"yaml":{"title":"The Sky's the Limit: Unveiling the Untold Stories of US Airline Woes on Twitter!","author":"Orozco Karol M.","image":"airplane.jpg","date":"4-20-2023","format":{"html":{"toc":true,"toc-location":"right","html-math-method":"katex","page-layout":"full"}},"execute":{"warning":false,"message":false},"categories":["R","Sentiment Analysis","Brand"]},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nWelcome to this sentiment analysis project in R, where I delve into the realm of major U.S. airlines and uncover the sentiments expressed by Twitter users regarding their experiences. By analyzing data scraped from February 2015, I aim to gain insights into the problems faced by these airlines through the eyes of their customers.\n\nThe dataset used in this analysis originated from Crowdflower's Data for Everyone library. It is available in both CSV file and SQLite database formats, providing flexibility for your preferred data exploration methods. The transformation code used to format the data can be found on GitHub, ensuring transparency and reproducibility.\n\nWithin this dataset, you will find a comprehensive collection of tweets categorized based on their sentiment: positive, negative, or neutral. Additionally, negative tweets have been further classified based on the reasons provided, ranging from \"late flight\" to \"rude service.\" This granular categorization enables us to pinpoint specific problem areas faced by these airlines.\n\nJoin me on this journey as we uncover the untold stories behind each major U.S. airline, unraveling the sentiments expressed by Twitter users and shedding light on the challenges faced by the industry.\n\n## Approach and Methodology\n\nThe first step is to gather relevant Twitter data. This typically involves utilizing the Twitter API or third-party tools to retrieve tweets based on specific keywords, hashtags, or user profiles. The collected data forms the basis for further analysis.\n\nOnce the data is collected, it undergoes preprocessing to clean and prepare it for sentiment analysis. This involves removing irrelevant information such as URLs, usernames, and special characters, as well as standardizing the text by converting it to lowercase and removing stopwords.\n\n#### Load the required packages and data\n\n```{r, warning=FALSE, message=FALSE}\n\nlibrary(hms)\nlibrary(twitteR)\nlibrary(lubridate) \nlibrary(tidytext)\nlibrary(tm)\nlibrary(wordcloud)\nlibrary(igraph)\nlibrary(glue)\nlibrary(networkD3)\nlibrary(rtweet)\nlibrary(plyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggeasy)\nlibrary(plotly)\nlibrary(dplyr)  \nlibrary(hms)\nlibrary(lubridate) \nlibrary(magrittr)\nlibrary(tidyverse)\nlibrary(janeaustenr)\nlibrary(widyr)\nlibrary(RColorBrewer)\nlibrary(ggeasy)\n\n\n```\n\n```{r}\ntweets <- read.csv(\"https://raw.githubusercontent.com/karolo89/WEBKOE/main/projects/Business/BrandPerception/Tweets.csv\")%>%\n  select(-negativereason_confidence)\n\nhead(tweets)\n\nstr(tweets)\n```\n\n## Data Analysis\n\n### Data Overview\n\n```{r}\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(dplyr)\n\ntweets <- tweets %>%\n  mutate(airline = factor(airline, levels = names(sort(table(airline), decreasing = TRUE))))\n\nggplot(tweets) +\n  aes(airline, fill = airline) +\n  geom_bar() +\n  geom_text(stat = 'count', aes(label = paste0(round((..count..)/sum(..count..) * 100), \"%\")),   vjust = 1.6, color = \"white\") +\n  labs(title = 'Number of Tweets by Airlines', x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.border = element_blank(),\n    panel.background = element_blank(),\n    legend.position = \"none\",\n    legend.title = element_blank(),\n    legend.text = element_text(size = 10),\n    axis.text.y = element_text(size = 10),\n    legend.key.size = unit(0.5, \"cm\"),\n    legend.key.height = unit(0.5, \"cm\"),\n    legend.key.width = unit(0.5, \"cm\")\n  ) +\n  scale_fill_tableau()\n\n\n```\n\n```{r}\n\ntableau_colors <- c(\"#4E79A7\", \"#F28E2B\", \"#E15759\", \"#76B7B2\", \"#59A14F\", \"#EDC948\")\n\ntweets %>%\n  group_by(airline, airline_sentiment) %>%\n  summarise(count = n()) %>%\n  ggplot(aes(airline, count, fill = airline)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ airline_sentiment, ncol = 1) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 9)) +\n  labs(x = \"Airlines\", y = \"Count of Tweet Sentiments\") +\n  theme_minimal() +\n  theme(\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.border = element_blank(),\n    panel.background = element_blank(),\n    legend.position = \"none\",\n    legend.title = element_blank(),\n    legend.text = element_text(size = 10),\n    axis.text.y = element_text(size = 10),\n    legend.key.size = unit(0.5, \"cm\"),\n    legend.key.height = unit(0.5, \"cm\"),\n    legend.key.width = unit(0.5, \"cm\")\n  ) +\n  scale_fill_manual(values = tableau_colors)\n\n\n```\n\n```{r}\n\ntweets %>%\n  group_by(negativereason) %>%\n  summarise(count = n(), na.rm = TRUE) %>%\n  arrange(desc(count)) %>%\n  ggplot(aes(reorder(negativereason, count), count)) +\n  geom_bar(fill = \"#4E79A7\", color = \"black\", stat = \"identity\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 9),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.border = element_blank(),\n    panel.background = element_blank(),\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0.5, size = 16),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  labs(x = \"\", y = \"Count of Negative Reasons\") +\n  scale_fill_manual(values = c(\"#4E79A7\"))  # Using Tableau palette color\n\n\n\n```\n\n### Extracting Tweets\n\n```{r}\ntweets.df <- tweets %>%\n  mutate(text = str_replace_all(text, \"[^[:graph:]]\", \" \") %>%\n           gsub(\"https://\", \"\", .) %>% \n           gsub(\"rt\", \"\", .) %>%\n           gsub(\"@\\\\w+\", \"\", .) %>%\n           gsub(\"http://\", \"\", .) %>%\n           gsub(\"[[:digit:]]\", \"\", .) %>%\n           gsub(\"[[:punct:]]\", \"\", .) %>%\n           gsub(\"[ |\\t]{2,}\", \"\", .) %>%\n           gsub(\"[^[:graph:]]\", \" \", .) %>%\n           gsub(\"[[:punct:]]\", \"\", .) %>%\n           gsub(\"[[:cntrl:]]\", \"\", .) %>%\n           gsub(\" $\", \"\", .) %>%\n           gsub(\"\\\\d+\", \"\", .) %>%\n           str_replace_all(., \"[^[:graph:]]\", \" \") %>%\n           tolower())\n\ncleanText <- tweets.df$text\n# remove empty results (if any)\ncleanText <- cleanText[cleanText != \" \"]\n\nhead(tweets.df)\n\n\n\n```\n\n### Frequency of Tweets\n\n```{r}\n# Apply the mutate function to update 'created' column\ntweets.df <- tweets.df %>% \n  mutate(\n    created = tweet_created %>% \n      # Remove zeros.\n      str_remove_all(pattern = '\\\\+0000') %>%\n      # Parse date.\n      parse_date_time(orders = '%m/%d/%Y %H:%M')\n  )\n\n# Create a new column 'Created_At_Round' by rounding 'created' to hours\ntweets.df <- tweets.df %>% \n  mutate(Created_At_Round = round_date(created, unit = \"hours\") %>% as.POSIXct())\n\n# Extract the minimum value of 'created' column\nmin_created <- min(tweets.df$created)\n\n# Print the minimum value\nprint(min_created)\n\n\ntweets.df %>% pull(created) %>% max()\n```\n\n```{r}\nplt <- tweets.df %>% \n  dplyr::count(Created_At_Round) %>% \n  ggplot(mapping = aes(x = Created_At_Round, y = n)) +\n  geom_line(color = \"blue\") +\n  labs(x = \"Date\", y = \"Number of Tweets\", title = \"Number of Tweets per Hour\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\nggplotly(plt)\n```\n\n### Loading sentiment word lists\n\n```{r}\nlibrary(tidytext)\n\n# Extract the positive and negative word lists\nwords <- get_sentiments(\"bing\")\n\npositive <- words %>% filter(sentiment == \"positive\")\nnegative <- words %>% filter(sentiment == \"negative\")\n\npos.words <- c(positive$word, 'upgrade', 'Congrats', 'prizes', 'prize', 'thanks', 'thnx', 'Grt', 'gr8', 'plz', 'trending', 'recovering', 'brainstorm', 'leader')\nneg.words <- c(negative$word, 'wtf', 'wait', 'waiting', 'epicfail', 'Fight', 'fighting', 'arrest', 'no', 'not')\n\n\n```\n\n### Sentiment scoring function\n\n```{r}\nscore.sentiment = function(sentences, pos.words, neg.words, airline, .progress='none')\n{\n  require(plyr)\n  require(stringr)\n  \n  # we are giving vector of sentences as input. \n  # plyr will handle a list or a vector as an \"l\" for us\n  # we want a simple array of scores back, so we use \"l\" + \"a\" + \"ply\" = laply:\n  scores = laply(sentences, function(sentence, pos.words, neg.words) {\n    \n    # clean up sentences with R's regex-driven global substitute, gsub() function:\n    sentence = gsub('https://','',sentence)\n    sentence = gsub('http://','',sentence)\n    sentence = gsub('[^[:graph:]]', ' ',sentence)\n    sentence = gsub('[[:punct:]]', '', sentence)\n    sentence = gsub('[[:cntrl:]]', '', sentence)\n    sentence = gsub('\\\\d+', '', sentence)\n    sentence = str_replace_all(sentence,\"[^[:graph:]]\", \" \")\n    # and convert to lower case:\n    sentence = tolower(sentence)\n    \n    # split into words. str_split is in the stringr package\n    word.list = str_split(sentence, '\\\\s+')\n    # sometimes a list() is one level of hierarchy too much\n    words = unlist(word.list)\n    \n    # compare our words to the dictionaries of positive & negative terms\n    pos.matches = match(words, pos.words)\n    neg.matches = match(words, neg.words)\n    \n    # match() returns the position of the matched term or NA\n    # we just want a TRUE/FALSE:\n    pos.matches = !is.na(pos.matches)\n    neg.matches = !is.na(neg.matches)\n    \n    # TRUE/FALSE will be treated as 1/0 by sum():\n    score = sum(pos.matches) - sum(neg.matches)\n    \n    return(score)\n  }, pos.words, neg.words, .progress=.progress )\n  \n  scores.df = data.frame(score=scores, text=sentences)\n  return(scores.df)\n}\n```\n\n### Calculating the sentiment score\n\n```{r}\nanalysis <- score.sentiment(cleanText, pos.words, neg.words)\n# Sentiment score frequency table\nscore_table <- as.data.frame(table(analysis$score))\ncolnames(score_table) <- c(\"Sentiment Score\", \"Frequency\")\n\nscore_table\n```\n\n### Histogram of sentiment scores\n\n```{r}\nanalysis %>%\n  ggplot(aes(x = score)) +\n geom_histogram(binwidth = 1, fill = \"#2171b5\") +\n  labs(x = \"Sentiment Score\", y = \"Frequency\") +\n  ggtitle(\"Distribution of Sentiment Scores of the Tweets\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n        axis.title = element_text(size = 12),\n        axis.text = element_text(size = 10),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.border = element_blank(),\n        panel.background = element_blank()) +\n  easy_center_title()\n\nggsave(\"plot.png\", width = 8, height = 6, dpi = 300)  # Save the plot as an image file\n```\n\n### Wordcloud\n\n```{r}\nlibrary(RColorBrewer)\n\n# Create a text corpus\ntext_corpus <- Corpus(VectorSource(cleanText))\ntext_corpus <- tm_map(text_corpus, content_transformer(tolower))\ntext_corpus <- tm_map(text_corpus, removeWords, stopwords(\"english\"))\ntext_corpus <- tm_map(text_corpus, removeWords, c(\"global\", \"globalwarming\"))\n\n# Create a term-document matrix\ntdm <- TermDocumentMatrix(text_corpus)\ntdm <- as.matrix(tdm)\ntdm <- sort(rowSums(tdm), decreasing = TRUE)\ntdm <- data.frame(word = names(tdm), freq = tdm)\n\n\nset.seed(123)\n\nwordcloud(text_corpus, min.freq = 1, max.words = 100, scale = c(2.2,1),\n          colors=brewer.pal(8, \"Dark2\"), random.color = T, random.order = F)\n```\n\n### Word Frequency plot\n\n```{r}\n\nggplot(tdm[1:20,], aes(x = reorder(word, freq), y = freq, fill = freq)) +\n  geom_bar(stat = \"identity\") +\n  xlab(\"Terms\") +\n  ylab(\"Count\") +\n  coord_flip() +\n  theme(axis.text = element_text(size = 7)) +\n  ggtitle(\"Most Common Word Frequency Plot\") +\n  easy_center_title() +\n  theme_minimal() +\n  scale_fill_gradient(low = \"#E5F5E0\", high = \"#31A354\")\n\n```\n\n## Network Analysis\n\n### Bigram analysis and Network definition\n\nBigram counts pairwise occurrences of words which appear together in the text.\n\n```{r}\n#bigram\nbi.gram.words <- tweets.df %>% \n  unnest_tokens(\n    input = text, \n    output = bigram, \n    token = 'ngrams', \n    n = 2\n  ) %>% \n  filter(! is.na(bigram))\n\nbi.gram.words %>% \n  select(bigram) %>% \n  head(10)\n```\n\n```{r}\nextra.stop.words <- c('https')\nstopwords.df <- tibble(\n  word = c(stopwords(kind = 'es'),\n           stopwords(kind = 'en'),\n           extra.stop.words)\n)\n```\n\nNext, we filter for stop words and remove white spaces.\n\n```{r}\nbi.gram.words %<>% \n  separate(col = bigram, into = c('word1', 'word2'), sep = ' ') %>% \n  filter(! word1 %in% stopwords.df$word) %>% \n  filter(! word2 %in% stopwords.df$word) %>% \n  filter(! is.na(word1)) %>% \n  filter(! is.na(word2)) \n```\n\nFinally, we group and count by bigram.\n\n```{r}\nbi.gram.count <- bi.gram.words %>% \n  dplyr::count(word1, word2, sort = TRUE) %>% \n  dplyr::rename(weight = n)\n\nbi.gram.count %>% head()\n```\n\nLet us plot the distribution of the weightvalues:\n\n```{r}\n## Note that it is very skewed, for visualization purposes it might be a good idea to perform a transformation, eg log transform:\n\nbi.gram.count %>% \n  mutate(weight = log(weight + 1)) %>% \n  ggplot(mapping = aes(x = weight)) +\n  theme_light() +\n  geom_histogram(fill = \"#2171b5\") +\n  labs(title = \"Bigram log-Weight Distribution\")+\n  theme_minimal()\n```\n\n### Network visualization\n\n```{r}\nthreshold <- 50\n\n# For visualization purposes we scale by a global factor. \nScaleWeight <- function(x, lambda) {\n  x / lambda\n}\n\nnetwork <-  bi.gram.count %>%\n  filter(weight > threshold) %>%\n  mutate(weight = ScaleWeight(x = weight, lambda = 2E3)) %>% \n  graph_from_data_frame(directed = FALSE)\n\nplot(\n  network, \n  vertex.size = 1,\n  vertex.label.color = 'black', \n  vertex.label.cex = 0.7, \n  vertex.label.dist = 1,\n  edge.color = 'gray', \n  main = 'Bigram Count Network', \n  sub = glue('Weight Threshold: {threshold}'), \n  alpha = 50\n)\n```\n\nWe can go a step further and make our visualization more dynamic using the networkD3 library.\n\n```{r}\nthreshold <- 50\n\nnetwork <-  bi.gram.count %>%\n  filter(weight > threshold) %>%\n  graph_from_data_frame(directed = FALSE)\n\n# Store the degree.\nV(network)$degree <- strength(graph = network)\n# Compute the weight shares.\nE(network)$width <- E(network)$weight/max(E(network)$weight)\n\n# Create networkD3 object.\nnetwork.D3 <- igraph_to_networkD3(g = network)\n# Define node size.\nnetwork.D3$nodes %<>% mutate(Degree = (1E-2)*V(network)$degree)\n# Define color group\nnetwork.D3$nodes %<>% mutate(Group = 1)\n# Define edges width. \nnetwork.D3$links$Width <- 10*E(network)$width\n\nforceNetwork(\n  Links = network.D3$links, \n  Nodes = network.D3$nodes, \n  Source = 'source', \n  Target = 'target',\n  NodeID = 'name',\n  Group = 'Group', \n  opacity = 0.9,\n  Value = 'Width',\n  Nodesize = 'Degree', \n  # We input a JavaScript function.\n  linkWidth = JS(\"function(d) { return Math.sqrt(d.value); }\"), \n  fontSize = 12,\n  zoom = TRUE, \n  opacityNoHover = 1\n)\n```\n\n### References:\n\nBing Liu, Minqing Hu and Junsheng Cheng. \"Opinion Observer: Analyzing and Comparing Opinions on the Web.\" Proceedings of the 14th International World Wide Web conference (WWW-2005), May 10-14, 2005, Chiba, Japan.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"html-math-method":"katex","output-file":"brand.html"},"language":{"code-summary":"Behind the Scenes"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.313","editor":"visual","citations-hover":true,"footnotes-hover":true,"theme":{"light":"../../../LightTheme.css","dark":"DarkTheme.scss"},"code-copy":true,"title-block-banner":true,"title":"The Sky's the Limit: Unveiling the Untold Stories of US Airline Woes on Twitter!","author":"Orozco Karol M.","image":"airplane.jpg","date":"4-20-2023","categories":["R","Sentiment Analysis","Brand"],"toc-location":"right","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}}}