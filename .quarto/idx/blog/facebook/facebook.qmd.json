{"title":"Facebook's Algorithmic Pandora's Box: Unmasking the Amplification of Fake News and Political Polarization","markdown":{"yaml":{"title":"Facebook's Algorithmic Pandora's Box: Unmasking the Amplification of Fake News and Political Polarization","author":"Orozco Karol M.","categories":["Data Journalism","R"],"image":"fb_icon"},"headingText":"Understanding Facebook's Algorithm","containsRefs":false,"markdown":"\n\nAs we integrate social media into our daily routines, we must consider the potential impact on the spread of information. Facebook's algorithm is intended to personalize users' News Feeds using engagement, relevance, relationships, post type, and recency, but this approach may unintentionally create filter bubbles that propagate false narratives and reinforce confirmation bias. This article delves into the impact of Facebook's algorithm on the amplification of incorrect information and political polarization and explores the mechanisms behind this concerning phenomenon.\n\n\nFacebook's algorithm heavily relies on user engagement to determine the popularity and relevance of a post. All forms of engagement, including likes, comments, shares, and reactions, are considered to ensure that the content reaches a broader audience, prompting users to interact with posts of interest and increasing the visibility of similar content in the future. The algorithm aims to deliver content that aligns with users' interests by analyzing past interactions, preferences, and relationships prioritizing engaging formats such as videos or visually captivating posts in the user's News Feeds. Moreover, the algorithm considers advertising and promoted content, targeting users based on their demographics, interests, and behaviors to present them with relevant ads.\n\n#### The Evolution of the Algorithm\n\nFacebook, established by Mark Zuckerberg in 2004 from a Harvard dormitory, quickly gained popularity and became a global sensation with its intuitive design, innovative features, and emphasis on digital connectivity. Throughout its history, Facebook's algorithm has evolved significantly to meet its user base's changing needs and challenges. From the introduction of the News Feed in 2006 to the ongoing refinements, the algorithm has aimed to balance personalization, user engagement, and responsible content curation. However, the platform has faced ethical and societal challenges, including privacy and transparency concerns, data security issues, and misinformation.\n\nThe News Feed's introduction replaced users' need to browse friends' profiles manually. This infinite scroll of content tailored to each user's interests and connections revolutionized how people consumed information on the platform. The algorithm gained further power in 2009 with the introduction of the Like button, allowing users to express approval or support for posts and photos.\n\nIn 2011, Facebook shifted its algorithm to prioritize content based on user engagement metrics like likes, comments, and shares. This update aimed to deliver more relevant and engaging content to users, enhancing their overall experience on the platform. By leveraging these metrics, the algorithm became a powerful tool for advertisers to target their desired audience precisely.\n\nThe algorithm continued to evolve in 2013 with the introduction of Graph Search. This feature enabled users to search for specific content, posts, photos, and people based on their connections and interests. This update improved the search experience and further personalized content recommendations, making it easier for users to discover relevant information.\n\nIn 2015, Facebook announced an algorithm update that prioritized posts from friends and family over content from brands, publishers, and pages. With growing concerns about the spread of clickbait headlines and fake news, Facebook took action to reduce the visibility of fake news, updating the system to favor posts from trustworthy sources and promoting more accurate and reliable information. This move aimed to address challenges associated with misinformation and enhance the platform's content quality.\n\nIn 2018, the company made significant changes to de-emphasize posts created by publishers and brands and prioritize content shared by friends and families. In 2019, the company introduced the \"Why am I seeing this post?\" feature, allowing users to understand the factors influencing their News Feed content and promoting transparency. Users gained more control over the types of content they see through improved customization options, empowering them to tailor their Facebook experience.\n\nIn 2020, the algorithm was adjusted to combat false information dissemination, especially during significant events like elections. The platform prioritized authoritative sources, implemented measures to prevent the amplification of misinformation, and worked with third-party fact-checkers to minimize the impact on elections and provide users with reliable and more accurate information.\n\n#### Spread of Misinformation and Political Polarization on Facebook\n\nThe prevalence of fake news on Facebook is a significant concern encompassing a wide range of problematic content, including imposter news, conspiracy theories, hyperpartisan websites, and \"junk news.\" The insidious use of clickbait to attract readers and the dissemination of dubious information through bots and trolls, known as \"computational propaganda,\" only exacerbates the issue. This problem extends beyond news, surrounding cultural commentary and satire, and raises crucial questions about how user engagement and algorithmic prioritization contribute to the spread of misinformation.\n\nThe influence of user engagement on Facebook's dissemination of fake news cannot be disregarded. Those who unintentionally like, comment, or share false information only increase their visibility and reach. Facebook's algorithm can magnify fake news with high engagement, making it visible to a broader audience. Additionally, personalized content based on user preferences and behavior can create echo chambers and filter bubbles, exposing users to misinformation that aligns with their beliefs and interests. Acknowledging that maximizing engagement can lead to polarization is crucial, particularly within networks of like-minded users.\n\nWhile Facebook has tried to combat the spread of fake news, the issue remains complex. The platform has partnered with third-party organizations for fact-checking, flagging disputed content, and reducing false information's visibility. However, balancing freedom of expression, user engagement, and the responsibility to combat misinformation is an ongoing challenge.\n\nRegarding political polarization, experts challenge Facebook CEO Mark Zuckerberg's denial that the platform fuels divisiveness. Zuckerberg's assertions have been disputed despite testifying before a U.S. House of Representatives subcommittee in March 2021 and attributing division to external factors, such as the political and media environment. Facebook's VP for global affairs and communication, Nick Clegg, has similarly claimed there is no evidence to support the notion that social media is a clear driver of polarization. However, the scholarly consensus is that social media platforms like Facebook and Twitter intensify political sectarianism, contrary to the company's claims. In articles published in Science in October 2020 and Trends in Cognitive Sciences in August 2021, researchers have concluded that social media is a significant facilitator of polarization.\n\nFacebook's algorithm limits exposure to cross-cutting links, causing users to connect with others who share the same political beliefs and reducing the likelihood of encountering diverse opinions. While this algorithm aims to enhance user experience, it inadvertently reduces politically diverse content by around 5% for conservatives and 8% for liberals. Typically, individuals have five politically like-minded friends on Facebook for every friend from the opposing side.\n\nAlthough Silicon Valley is not solely responsible for these issues, a study conducted in March 2020 showed that taking a break from Facebook for a month significantly reduced polarization of views on policy issues among participants. The study published by the American Economic Review suggested that exposure to political content on social media tends to provoke heightened emotions, anger toward the opposing side, and more robust views on specific issues, which contradicts Facebook's narrative.\n\n#### Algorithmic Failure's High Costs\n\nThe spread of false information through social media is a grave issue that can trigger severe short, and long-lasting consequences for individuals, society, and organizations. Acknowledging that misinformation can lead to social division, offer misleading medical advice, and promote fraudulent schemes that erode trust and pose serious health risks is paramount. The fact that fake news purporting that pure alcohol (methanol) could cure COVID-19 led to approximately 800 deaths in Iran, with an additional 5,876 people hospitalized due to methanol poisoning, is unacceptable. This tragic example underscores the precarious nature of misinformation for individuals and society.\n\nMoreover, the Cambridge Analytica and Facebook case explicitly demonstrates the cost of false information for businesses. Cambridge Analytica was accused of manipulating political outcomes through Facebook data, with some executives proposing unethical practices such as bribery and fake news. These actions ultimately led to the closure of the company in 2018 and Facebook paying a record \\$5 billion settlement to the Federal Trade Commission (FTC) and another \\$100 million to settle allegations of misuse of user data with the U.S. Securities and Exchange Commission.\n\nSuch actions have far-reaching consequences beyond these companies, as non-profit organizations and advocacy groups may struggle to communicate their messages effectively when fake news dominates the conversation. This effect in non-profits was demonstrated in 2014 when anti-abortion activists secretly recorded meetings and staff lunches at Planned Parenthood, a non-profit organization that provides reproductive healthcare options. The activists then edited and disseminated the footage through social media, creating a viral sensation. The hashtag #defundPP was shared over 1.3 million times within a few months, subjecting Planned Parenthood to intense scrutiny and calls for defunding.\n\nAnother example of the algorithm failure cost is the aftermath of the January 6th attack on the U.S. Capitol, which has been financially and politically disastrous. It's been widely reported that Facebook groups played a significant role in fueling political polarization and spreading false narratives that contributed to the siege. These groups saw a massive surge in posts aimed at undermining the legitimacy of Joe Biden's victory, totaling over 650,000 between Election Day and the attack. The costs of the damage caused by this event have already exceeded \\$30 million and are expected to continue rising. These expenses include implementing enhanced security measures, repairing damaged infrastructure, and deploying additional law enforcement resources. However, the financial implications are only a tiny part of the more significant problem. The attack on the Capitol represents a severe assault on American democratic principles and substantially threatens the peaceful transfer of power.\n\n#### Data Ethics and Facebook's Responsibility\n\nFacebook's handling of user data has sparked significant concerns, mainly due to the infamous Cambridge Analytica scandal in 2018. The scandal highlighted Facebook's unethical practices, including weak consent mechanisms, inadequate data protection, user profiling, targeted advertising, and a lack of transparency. The scandal revealed that personal data from around 87 million Facebook users had been collected and exploited through a third-party app called \"This Is Your Digital Life.\" This breach violated users' privacy rights and exposed their data to misuse. Cambridge Analytica used this unauthorized data to create psychographic profiles and targeted political advertising during the 2016 U.S. presidential election and other campaigns worldwide.\n\nAs of today, misleading, and extremist content disguised as journalism and facts continue to dominate the most shared posts on the platform, making Facebook a prominent source of misinformation. Likewise, this criticism is further accentuated by the legal and regulatory landscape, allowing the company to prioritize profit maximization and shareholder value over proactive ethical considerations.\n\nRegarding algorithm responsibility, Article 230 of the Communications Decency Act of 1996 is a significant factor. Originally intended to safeguard freedom of expression online, this legislation has shielded platforms like Facebook from active duty in regulating their hosted content. However, it has been exploited by social media startups and big tech companies, enabling harmful content to thrive. The article stipulates that interactive computer service providers or users should not be held accountable for the information provided by content creators.\n\nFacebook promotes a \"shared responsibility\" approach and emphasizes community cooperation. The company argues that it cannot be the sole arbiter of truth and instead believes in empowering individuals to have a voice. They encourage users to report false posts, avoid sharing or posting misleading articles, and flag spammy content, placing some responsibility on the user community.\n\nIn the face of these concerns, it is evident that Facebook needs to address its data ethics practices more effectively and take substantial steps to combat misinformation while prioritizing transparency, user privacy, and responsible decision-making.\n\n#### Next Steps: Collaborative Solutions, Section 230 Reform, Proactive Measures by Social Media Companies, and Digital Citizenship\n\nSeveral next steps are crucial to address the challenges in the online environment: Section 230 Reform: Reforming Section 230 of the Communications Decency Act of 1996 is necessary to encourage platforms to exercise their duty of care responsibly without unnecessary regulatory burdens. This reform could motivate media outlets to address potential risks proactively and create a safer and more responsible online environment. The protections outlined in Section 230 were crafted over 25 years ago, a time of limited technological capabilities and naive technological optimism. In light of the significant changes since then, these protections are outdated and need reconsideration and updating by the legislators.\n\nCollaboration for Comprehensive Solutions: Governments, tech companies, civil society organizations, and individuals must join forces to develop comprehensive solutions. Collaborative efforts can include sharing expertise, resources, and best practices to effectively address the spread of fake news. This collective approach ensures a more holistic and impactful response.\n\nStrengthening Media Literacy and Promoting Digital Citizenship: Investing in media literacy programs is crucial to empower individuals with the critical skills to navigate the digital landscape. Education initiatives should focus on fostering media literacy, promoting digital citizenship, and cultivating necessary thinking skills. By equipping people with the tools to discern reliable information from misinformation, they become active participants in combating the spread of fake news.\n\nLong-term Algorithmic Adjustments: Social media companies, such as Facebook, must acknowledge their contribution to political polarization and take proactive measures to reduce the prevalence of divisive content. Making lasting changes to their algorithms is a practical step in curbing heated debates and reducing the amplification of polarizing narratives. Continuously refining automated systems and moderation policies is necessary to balance fostering free expression and mitigating the unintended suppression of legitimate political discourse.\n\nEmbrace transparency: Social media platforms must prioritize transparency by disclosing how their algorithms work, such as how content is rated, suggested, and removed. In addition, these platforms should continuously enhance their data protection measures, participate in independent audits, and foster a responsible environment for sharing information.\n\n#### Conclusion\n\nThe potential contribution of Facebook's algorithm to spreading fake news and political polarization is a significant concern. Despite its original purpose to personalize users' News Feeds, the algorithm could create filter bubbles reinforcing false narratives and confirmation bias. Imposter news, conspiracy theories, hyperpartisan websites, and \"junk news\" are just a few examples of inappropriate content in the fake news sphere on Facebook. It's worth noting that user engagement is a significant factor in disseminating phony information, and personalized content based on user preferences and behavior can create echo chambers and filter bubbles that expose users to misinformation that aligns with their beliefs and interests.\n\nIt's essential to recognize that maximizing engagement can lead to polarization, particularly within networks of like-minded users. Facebook has tried to counter the spread of fake news, but balancing freedom of expression, user engagement, and the responsibility to combat misinformation is challenging. To develop comprehensive solutions, we must collaborate, promote media literacy, adjust algorithms, and embrace transparency.\n\n#### References\n\nAllcott, H., Braghieri, L., Eichmeyer, S., & Gentzkow, M. (2020). \\[The Welfare Effects of Social Media\\](https://doi.org/10.1257/aer.20190468). American Economic Review, 110(3), 629-676.\n\nBarrett, P., et al. (2021, September 27). \\[How Tech Platforms Fuel U.S. Political Polarization and What Government Can Do about It\\](https://www.brookings.edu/blog/techtank/2021/09/27/how-tech-platforms-fuel-u-s-political-polarization-and-what-government-can-do-about-it/). Brookings.\n\nBleiberg, J., & West, D. M. (2015, May 13). \\[Political Polarization on Facebook\\](https://www.brookings.edu/blog/techtank/2015/05/13/political-polarization-on-facebook/). Brookings.\n\nCochrane, E., & Broadwater, L. (2021, February 24). \\[Capitol Riot Costs Will Exceed \\$30 Million, Official Tells Congress\\](https://www.nytimes.com/2021/02/24/us/politics/capitol-riot-damage.html). The New York Times.\n\nDepartment of Justice. (2020, June 3). \\[Department of Justice's Review of Section 230 of the Communications Decency Act of 1996\\](https://www.justice.gov/archives/ag/department-justice-s-review-section-230-communications-decency-act-1996).\n\nFinkel, E. J., et al. (2020). \\[Political Sectarianism in America\\](https://doi.org/10.1126/science.abe1715). Science, 370(6516), 533-536.\n\nHarvard Kennedy School. (n.d.). \\[Research Note: The Scale of Facebook's Problem Depends upon How 'Fake News' Is Classified\\](https://misinforeview.hks.harvard.edu/article/research-note-the-scale-of-facebooks-problem-depends-upon-how-fake-news-is-classified/).\n\nHarvard Law Today. (n.d.). \\[The Algorithm Has Primacy over Media ... over Each of Us, and It Controls What We Do\\](https://hls.harvard.edu/today/the-algorithm-has-primacy-over-media-over-each-of-us-and-it-controls-what-we-do/).\n\nMadrigal, A. C. (2017, October 12). \\[What Facebook Did to American Democracy\\](https://www.theatlantic.com/technology/archive/2017/10/what-facebook-did/542502/). The Atlantic.\n\nMIT Technology Review. (2021, March 11). \\[How Facebook Got Addicted to Spreading Misinformation\\](https://www.technologyreview.com/2021/03/11/1020600/facebook-responsible-ai-misinformation/).\n\nMIT Technology Review. (2021, November 20). \\[How Facebook and Google Fund Global Misinformation\\](https://www.technologyreview.com/2021/11/20/1039076/facebook-google-disinformation-clickbait/).\n\nMosseri, A. (2017, April 7). \\[Working to Stop Misinformation and False News\\](https://www.facebook.com/formedia/blog/working-to-stop-misinformation-and-false-news). Facebook Media.\n\nRocha, Y. M., et al. (2021). \\[The Impact of Fake News on Social Media and Its Influence on Health during the COVID-19 Pandemic: A Systematic Review\\](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8502082/). Journal of Public Health, 1(10).\n\nSilverman, C. (2016, November 16). \\[This Analysis Shows How Viral Fake Election News Stories Outperformed Real News on Facebook\\](https://www.buzzfeednews.com/article/craigsilverman/viral-fake-election-news-outperformed-real-news-on-facebook). BuzzFeed News.\n\nSmith, M. D., & Van Alstyne, M. (2021, August 12). \\[It's Time to Update Section 230\\](https://hbr.org/2021/08/its-time-to-update-section-230). Harvard Business Review.\n\nStars, S., Stortstrom, R., Lagrace, L., & King, N. (2018). \\[Overview of Fake News: For Non-Profit Organizations\\](https://stars.library.ucf.edu/cgi/viewcontent.cgi?article=1001&context=publicsectormedialiteracy).\n\nThe Guardian. (2016, November 10). \\[Facebook's Failure: Did Fake News and Polarized Politics Get Trump Elected?\\](https://www.theguardian.com/technology/2016/nov/10/facebook-fake-news-election-conspiracy-theories).\n\nThe New York Times. (2015, May 8). \\[Facebook Use Polarizing? Site Begs to Differ\\](https://www.nytimes.com/2015/05/08/technology/facebook-study-disputes-theory-of-political-polarization-among-users.html).\n\nThe New York Times. (2020, May 28). \\[Facebook and Its Secret Policies\\](https://www.nytimes.com/2020/05/28/technology/facebook-polarization.html).\n\nVan Bavel, J. J., et al. (2021). \\[How Social Media Shapes Polarization\\](https://doi.org/10.1016/j.tics.2021.07.013). Trends in Cognitive Sciences, 25(11).\n\nFacebook AI. (n.d.). \\[How We're Using Fairness Flow to Help Build AI That Works Better for Everyone\\](https://ai.facebook.com/blog/how-were-using-fairness-flow-to-help-build-ai-that-works-better-for-everyone).\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"facebook.html"},"language":{"code-summary":"Behind the Scenes"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.313","editor":"visual","citations-hover":true,"footnotes-hover":true,"theme":{"light":"../../LightTheme.css","dark":"DarkTheme.scss"},"code-copy":true,"title-block-banner":true,"title":"Facebook's Algorithmic Pandora's Box: Unmasking the Amplification of Fake News and Political Polarization","author":"Orozco Karol M.","categories":["Data Journalism","R"],"image":"fb_icon"},"extensions":{"book":{"multiFile":true}}}}}